#
# COPYRIGHT Ericsson 2024
#
#
#
# The copyright to the computer program(s) herein is the property of
#
# Ericsson Inc. The programs may be used and/or copied only with written
#
# permission from Ericsson Inc. or in accordance with the terms and
#
# conditions stipulated in the agreement/contract under which the
#
# program(s) have been supplied.
#

modelVersion: '3.0'
dependencies:
- ID: absl-py+2.1.0
  additional_info:
    fossa-attribution:
      Description: '# Abseil Python Common LibrariesThis repository is a collection
        of Python library code for building Pythonapplications. The code is collected
        from Google''s own Python code base, and hasbeen extensively tested and used
        in production.## Features* Simple application startup* Distributed commandline
        flags system* Custom logging module with additional features* Testing utilities##
        Getting Started### InstallationTo install the package, simply run:```bashpip
        install absl-py```Or install from source:```bashpython setup.py install```###
        Running TestsTo run Abseil tests, you can clone the git repo and run[bazel](https://bazel.build/):```bashgit
        clone https://github.com/abseil/abseil-py.gitcd abseil-pybazel test absl/...```###
        Example CodePlease refer to[smoke_tests/sample_app.py](https://github.com/abseil/abseil-py/blob/main/smoke_tests/sample_app.py)as
        an example to get started.## DocumentationSee the [Abseil Python Developer
        Guide](https://abseil.io/docs/python/).## Future ReleasesThe current repository
        includes an initial set of libraries for early adoption.More components and
        interoperability with Abseil C++ Common Librarieswill come in future releases.##
        LicenseThe Abseil Python library is licensed under the terms of the Apachelicense.
        See [LICENSE](LICENSE) for more information.'
      Package: absl-py
      Source: pip
      Version: '2.1.0'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: absl-py
      DownloadURL: https://files.pythonhosted.org/packages/7a/8f/fc001b92ecc467cc32ab38398bd0bfb45df46e7523bf33c2ad22a505f06e/absl-py-2.1.0.tar.gz
  bazaar:
    register: 'no'
    prim: 10/CTX1022520
    community_link: https://pypi.org/project/absl-py/
    community_name: https://pypi.org/project/absl-py/
    community_url: https://pypi.org/project/absl-py/
    component_comment: ''
    component_highlevel_description: Abseil Common Libraries (Python)
    component_name: absl-py
    component_platform: linux
    component_programing_language: Python
    component_version: '2.1.0'
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://files.pythonhosted.org/packages/7a/8f/fc001b92ecc467cc32ab38398bd0bfb45df46e7523bf33c2ad22a505f06e/absl-py-2.1.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078629&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: absl-py
    target_sw: linux
    vendor: pip
    version: '2.1.0'
    web_url: https://github.com/abseil/abseil-py
  licenses:
  - Apache-2.0
  name: absl-py
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '2.1.0'
  mimer:
    linking: Static
    product_number: CTX1022520
    product_version_label: '2.1.0'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: aiohttp+3.9.3
  additional_info:
    fossa-attribution:
      Description: '==================================Async http client/server framework==================================..
        image:: https://raw.githubusercontent.com/aio-libs/aiohttp/master/docs/_static/aiohttp-icon-128x128.png   :height:
        64px   :width: 64px   :alt: aiohttp logo|.. image:: https://travis-ci.com/aio-libs/aiohttp.svg?branch=master   :target:
        https://travis-ci.com/aio-libs/aiohttp   :align: right   :alt: Travis status
        for master branch.. image:: https://ci.appveyor.com/api/projects/status/tnddy9k6pphl8w7k/branch/master?svg=true   :target:
        https://ci.appveyor.com/project/aio-libs/aiohttp   :align: right   :alt: AppVeyor
        status for master branch.. image:: https://codecov.io/gh/aio-libs/aiohttp/branch/master/graph/badge.svg   :target:
        https://codecov.io/gh/aio-libs/aiohttp   :alt: codecov.io status for master
        branch.. image:: https://badge.fury.io/py/aiohttp.svg   :target: https://pypi.org/project/aiohttp   :alt:
        Latest PyPI package version.. image:: https://readthedocs.org/projects/aiohttp/badge/?version=latest   :target:
        https://docs.aiohttp.org/   :alt: Latest Read The Docs.. image:: https://badges.gitter.im/Join%20Chat.svg    :target:
        https://gitter.im/aio-libs/Lobby    :alt: Chat on GitterKey Features============-
        Supports both client and server side of HTTP protocol.- Supports both client
        and server Web-Sockets out-of-the-box and avoids  Callback Hell.- Provides
        Web-server with middlewares and pluggable routing.Getting started===============Client------To
        get something from the web:.. code-block:: python  import aiohttp  import
        asyncio  async def fetch(session, url):      async with session.get(url) as
        response:          return await response.text()  async def main():      async
        with aiohttp.ClientSession() as session:          html = await fetch(session,
        ''http://python.org'')          print(html)  if __name__ == ''__main__'':      loop
        = asyncio.get_event_loop()      loop.run_until_complete(main())Server------An
        example using a simple server:.. code-block:: python    # examples/server_simple.py    from
        aiohttp import web    async def handle(request):        name = request.match_info.get(''name'',
        "Anonymous")        text = "Hello, " + name        return web.Response(text=text)    async
        def wshandle(request):        ws = web.WebSocketResponse()        await ws.prepare(request)        async
        for msg in ws:            if msg.type == web.WSMsgType.text:                await
        ws.send_str("Hello, {}".format(msg.data))            elif msg.type == web.WSMsgType.binary:                await
        ws.send_bytes(msg.data)            elif msg.type == web.WSMsgType.close:                break        return
        ws    app = web.Application()    app.add_routes([web.get(''/'', handle),                    web.get(''/echo'',
        wshandle),                    web.get(''/{name}'', handle)])    if __name__
        == ''__main__'':        web.run_app(app)Documentation=============https://aiohttp.readthedocs.io/Demos=====https://github.com/aio-libs/aiohttp-demosExternal
        links==============* `Third party libraries  <http://aiohttp.readthedocs.io/en/latest/third_party.html>`_*
        `Built with aiohttp  <http://aiohttp.readthedocs.io/en/latest/built_with.html>`_*
        `Powered by aiohttp  <http://aiohttp.readthedocs.io/en/latest/powered_by.html>`_Feel
        free to make a Pull Request for adding your link to these pages!Communication
        channels======================*aio-libs* google group: https://groups.google.com/forum/#!forum/aio-libsFeel
        free to post your questions and ideas here.*gitter chat* https://gitter.im/aio-libs/LobbyWe
        support `Stack Overflow<https://stackoverflow.com/questions/tagged/aiohttp>`_.Please
        add *aiohttp* tag to your question there.Requirements============- Python
        >= 3.5.3- async-timeout_- attrs_- chardet_- multidict_- yarl_Optionally you
        may install the cChardet_ and aiodns_ libraries (highlyrecommended for sake
        of speed)... _chardet: https://pypi.python.org/pypi/chardet.. _aiodns: https://pypi.python.org/pypi/aiodns..
        _attrs: https://github.com/python-attrs/attrs.. _multidict: https://pypi.python.org/pypi/multidict..
        _yarl: https://pypi.python.org/pypi/yarl.. _async-timeout: https://pypi.python.org/pypi/async_timeout..
        _cChardet: https://pypi.python.org/pypi/cchardetLicense=======``aiohttp``
        is offered under the Apache 2 license.Keepsafe========The aiohttp community
        would like to thank Keepsafe(https://www.getkeepsafe.com) for its support
        in the early days ofthe project.Source code===========The latest developer
        version is available in a GitHub repository:https://github.com/aio-libs/aiohttpBenchmarks==========If
        you are interested in efficiency, the AsyncIO community maintains alist of
        benchmarks on the official wiki:https://github.com/python/asyncio/wiki/Benchmarks=========Changelog=========..    You
        should *NOT* be adding new change log entries to this file, this    file is
        managed by towncrier. You *may* edit previous change logs to    fix problems
        like typo corrections or such.    To add a new change log entry, please see    https://pip.pypa.io/en/latest/development/#adding-a-news-entry    we
        named the news folder "changes".    WARNING: Don''t drop the next directive!..
        towncrier release notes start3.6.1 (2019-09-19)==================Features---------
        Compatibility with Python 3.8.  `#4056 <https://github.com/aio-libs/aiohttp/issues/4056>`_Bugfixes---------
        correct some exception string format  `#4068 <https://github.com/aio-libs/aiohttp/issues/4068>`_-
        Emit a warning when ``ssl.OP_NO_COMPRESSION`` is  unavailable because the
        runtime is built against  an outdated OpenSSL.  `#4052 <https://github.com/aio-libs/aiohttp/issues/4052>`_-
        Update multidict requirement to >= 4.5  `#4057 <https://github.com/aio-libs/aiohttp/issues/4057>`_Improved
        Documentation----------------------- Provide pytest-aiohttp namespace for
        pytest fixtures in docs.  `#3723 <https://github.com/aio-libs/aiohttp/issues/3723>`_----3.6.0
        (2019-09-06)==================Features--------- Add support for Named Pipes
        (Site and Connector) under Windows. This feature requires Proactor event loop
        to work.  `#3629 <https://github.com/aio-libs/aiohttp/issues/3629>`_- Removed
        `Transfer-Encoding: chunked` header from websocket responses to be compatible
        with more http proxy servers.  `#3798 <https://github.com/aio-libs/aiohttp/issues/3798>`_-
        Accept non-GET request for starting websocket handshake on server side.  `#3980
        <https://github.com/aio-libs/aiohttp/issues/3980>`_Bugfixes--------- Raise
        a ClientResponseError instead of an AssertionError for a blank  HTTP Reason
        Phrase.  `#3532 <https://github.com/aio-libs/aiohttp/issues/3532>`_- Fix an
        issue where cookies would sometimes not be set during a redirect.  `#3576
        <https://github.com/aio-libs/aiohttp/issues/3576>`_- Change normalize_path_middleware
        to use 308 redirect instead of 301.  This behavior should prevent clients
        from being unable to use PUT/POST  methods on endpoints that are redirected
        because of a trailing slash.  `#3579 <https://github.com/aio-libs/aiohttp/issues/3579>`_-
        Drop the processed task from ``all_tasks()`` list early. It prevents logging
        about a task with unhandled exception when the server is used in conjunction
        with ``asyncio.run()``.  `#3587 <https://github.com/aio-libs/aiohttp/issues/3587>`_-
        ``Signal`` type annotation changed from `Signal[Callable[[''TraceConfig''],
        Awaitable[None]]]` to `Signal[Callable[ClientSession, SimpleNamespace, ...]`.  `#3595
        <https://github.com/aio-libs/aiohttp/issues/3595>`_- Use sanitized URL as
        Location header in redirects  `#3614 <https://github.com/aio-libs/aiohttp/issues/3614>`_-
        Improve typing annotations for multipart.py along with changes required  by
        mypy in files that references multipart.py.  `#3621 <https://github.com/aio-libs/aiohttp/issues/3621>`_-
        Close session created inside ``aiohttp.request`` when unhandled exception
        occurs  `#3628 <https://github.com/aio-libs/aiohttp/issues/3628>`_- Cleanup
        per-chunk data in generic data read. Memory leak fixed.  `#3631 <https://github.com/aio-libs/aiohttp/issues/3631>`_-
        Use correct type for add_view and family  `#3633 <https://github.com/aio-libs/aiohttp/issues/3633>`_-
        Fix _keepalive field in __slots__ of ``RequestHandler``.  `#3644 <https://github.com/aio-libs/aiohttp/issues/3644>`_-
        Properly handle ConnectionResetError, to silence the "Cannot write to closing  transport"
        exception when clients disconnect uncleanly.  `#3648 <https://github.com/aio-libs/aiohttp/issues/3648>`_-
        Suppress pytest warnings due to ``test_utils`` classes  `#3660 <https://github.com/aio-libs/aiohttp/issues/3660>`_-
        Fix overshadowing of overlapped sub-application prefixes.  `#3701 <https://github.com/aio-libs/aiohttp/issues/3701>`_-
        Fixed return type annotation for WSMessage.json()  `#3720 <https://github.com/aio-libs/aiohttp/issues/3720>`_-
        Properly expose TooManyRedirects publicly as documented.  `#3818 <https://github.com/aio-libs/aiohttp/issues/3818>`_-
        Fix missing brackets for IPv6 in proxy CONNECT request  `#3841 <https://github.com/aio-libs/aiohttp/issues/3841>`_-
        Make the signature of `aiohttp.test_utils.TestClient.request` match `asyncio.ClientSession.request`
        according to the docs  `#3852 <https://github.com/aio-libs/aiohttp/issues/3852>`_-
        Use correct style for re-exported imports, makes mypy ``--strict`` mode happy.  `#3868
        <https://github.com/aio-libs/aiohttp/issues/3868>`_- Fixed type annotation
        for add_view method of UrlDispatcher to accept any subclass of View  `#3880
        <https://github.com/aio-libs/aiohttp/issues/3880>`_- Made cython HTTP parser
        set Reason-Phrase of the response to an empty string if it is missing.  `#3906
        <https://github.com/aio-libs/aiohttp/issues/3906>`_- Add URL to the string
        representation of ClientResponseError.  `#3959 <https://github.com/aio-libs/aiohttp/issues/3959>`_-
        Accept ``istr`` keys in ``LooseHeaders`` type hints.  `#3976 <https://github.com/aio-libs/aiohttp/issues/3976>`_-
        Fixed race conditions in _resolve_host caching and throttling when tracing
        is enabled.  `#4013 <https://github.com/aio-libs/aiohttp/issues/4013>`_- For
        URLs like "unix://localhost/..." set Host HTTP header to "localhost" instead
        of "localhost:None".  `#4039 <https://github.com/aio-libs/aiohttp/issues/4039>`_Improved
        Documentation----------------------- Modify documentation for Background Tasks
        to remove deprecated usage of event loop.  `#3526 <https://github.com/aio-libs/aiohttp/issues/3526>`_-
        use ``if __name__ == ''__main__'':`` in server examples.  `#3775 <https://github.com/aio-libs/aiohttp/issues/3775>`_-
        Update documentation reference to the default access logger.  `#3783 <https://github.com/aio-libs/aiohttp/issues/3783>`_-
        Improve documentation for ``web.BaseRequest.path`` and ``web.BaseRequest.raw_path``.  `#3791
        <https://github.com/aio-libs/aiohttp/issues/3791>`_- Removed deprecation warning
        in tracing example docs  `#3964 <https://github.com/aio-libs/aiohttp/issues/3964>`_----3.5.4
        (2019-01-12)==================Bugfixes--------- Fix stream ``.read()`` / ``.readany()``
        / ``.iter_any()`` which used to return a  partial content only in case of
        compressed content  `#3525 <https://github.com/aio-libs/aiohttp/issues/3525>`_3.5.3
        (2019-01-10)==================Bugfixes--------- Fix type stubs for ``aiohttp.web.run_app(access_log=True)``
        and fix edge case of ``access_log=True`` and the event loop being in debug
        mode.  `#3504 <https://github.com/aio-libs/aiohttp/issues/3504>`_- Fix ``aiohttp.ClientTimeout``
        type annotations to accept ``None`` for fields  `#3511 <https://github.com/aio-libs/aiohttp/issues/3511>`_-
        Send custom per-request cookies even if session jar is empty  `#3515 <https://github.com/aio-libs/aiohttp/issues/3515>`_-
        Restore Linux binary wheels publishing on PyPI----3.5.2 (2019-01-08)==================Features---------
        ``FileResponse`` from ``web_fileresponse.py`` uses a ``ThreadPoolExecutor``
        to work with files asynchronously.  I/O based payloads from ``payload.py``
        uses a ``ThreadPoolExecutor`` to work with I/O objects asynchronously.  `#3313
        <https://github.com/aio-libs/aiohttp/issues/3313>`_- Internal Server Errors
        in plain text if the browser does not support HTML.  `#3483 <https://github.com/aio-libs/aiohttp/issues/3483>`_Bugfixes---------
        Preserve MultipartWriter parts headers on write.  Refactor the way how ``Payload.headers``
        are handled. Payload instances now always  have headers and Content-Type defined.  Fix
        Payload Content-Disposition header reset after initial creation.  `#3035 <https://github.com/aio-libs/aiohttp/issues/3035>`_-
        Log suppressed exceptions in ``GunicornWebWorker``.  `#3464 <https://github.com/aio-libs/aiohttp/issues/3464>`_-
        Remove wildcard imports.  `#3468 <https://github.com/aio-libs/aiohttp/issues/3468>`_-
        Use the same task for app initialization and web server handling in gunicorn
        workers.  It allows to use Python3.7 context vars smoothly.  `#3471 <https://github.com/aio-libs/aiohttp/issues/3471>`_-
        Fix handling of chunked+gzipped response when first chunk does not give uncompressed
        data  `#3477 <https://github.com/aio-libs/aiohttp/issues/3477>`_- Replace
        ``collections.MutableMapping`` with ``collections.abc.MutableMapping`` to
        avoid a deprecation warning.  `#3480 <https://github.com/aio-libs/aiohttp/issues/3480>`_-
        ``Payload.size`` type annotation changed from `Optional[float]` to `Optional[int]`.  `#3484
        <https://github.com/aio-libs/aiohttp/issues/3484>`_- Ignore done tasks when
        cancels pending activities on ``web.run_app`` finalization.  `#3497 <https://github.com/aio-libs/aiohttp/issues/3497>`_Improved
        Documentation----------------------- Add documentation for ``aiohttp.web.HTTPException``.  `#3490
        <https://github.com/aio-libs/aiohttp/issues/3490>`_Misc----- `#3487 <https://github.com/aio-libs/aiohttp/issues/3487>`_----3.5.1
        (2018-12-24)====================- Fix a regression about ``ClientSession._requote_redirect_url``
        modification in debug  mode.3.5.0 (2018-12-22)====================Features---------
        The library type annotations are checked in strict mode now.- Add support
        for setting cookies for individual request (`#2387 <https://github.com/aio-libs/aiohttp/pull/2387>`_)-
        Application.add_domain implementation (`#2809 <https://github.com/aio-libs/aiohttp/pull/2809>`_)-
        The default ``app`` in the request returned by ``test_utils.make_mocked_request``  can
        now have objects assigned to it and retrieved using the ``[]`` operator. (`#3174
        <https://github.com/aio-libs/aiohttp/pull/3174>`_)- Make ``request.url`` accessible
        when transport is closed. (`#3177 <https://github.com/aio-libs/aiohttp/pull/3177>`_)-
        Add ``zlib_executor_size`` argument to ``Response`` constructor to allow compression
        to run in a background executor to avoid blocking the main thread and potentially
        triggering health check failures. (`#3205 <https://github.com/aio-libs/aiohttp/pull/3205>`_)-
        Enable users to set `ClientTimeout` in `aiohttp.request` (`#3213 <https://github.com/aio-libs/aiohttp/pull/3213>`_)-
        Don''t raise a warning if ``NETRC`` environment variable is not set and ``~/.netrc``
        file  doesn''t exist. (`#3267 <https://github.com/aio-libs/aiohttp/pull/3267>`_)-
        Add default logging handler to web.run_app  If the `Application.debug` flag
        is set and the default logger `aiohttp.access` is used, access logs will now
        be output using a `stderr` `StreamHandler` if no handlers are attached. Furthermore,
        if the default logger has no log level set, the log level will be set to `DEBUG`.
        (`#3324 <https://github.com/aio-libs/aiohttp/pull/3324>`_)- Add method argument
        to ``session.ws_connect()``.  Sometimes server API requires a different HTTP
        method for WebSocket connection establishment.  For example, ``Docker exec``
        needs POST. (`#3378 <https://github.com/aio-libs/aiohttp/pull/3378>`_)- Create
        a task per request handling. (`#3406 <https://github.com/aio-libs/aiohttp/pull/3406>`_)Bugfixes---------
        Enable passing `access_log_class` via `handler_args` (`#3158 <https://github.com/aio-libs/aiohttp/pull/3158>`_)-
        Return empty bytes with end-of-chunk marker in empty stream reader. (`#3186
        <https://github.com/aio-libs/aiohttp/pull/3186>`_)- Accept ``CIMultiDictProxy``
        instances for ``headers`` argument in ``web.Response``  constructor. (`#3207
        <https://github.com/aio-libs/aiohttp/pull/3207>`_)- Don''t uppercase HTTP
        method in parser (`#3233 <https://github.com/aio-libs/aiohttp/pull/3233>`_)-
        Make method match regexp RFC-7230 compliant (`#3235 <https://github.com/aio-libs/aiohttp/pull/3235>`_)-
        Add ``app.pre_frozen`` state to properly handle startup signals in sub-applications.
        (`#3237 <https://github.com/aio-libs/aiohttp/pull/3237>`_)- Enhanced parsing
        and validation of helpers.BasicAuth.decode. (`#3239 <https://github.com/aio-libs/aiohttp/pull/3239>`_)-
        Change imports from collections module in preparation for 3.8. (`#3258 <https://github.com/aio-libs/aiohttp/pull/3258>`_)-
        Ensure Host header is added first to ClientRequest to better replicate browser
        (`#3265 <https://github.com/aio-libs/aiohttp/pull/3265>`_)- Fix forward compatibility
        with Python 3.8: importing ABCs directly from the collections module will
        not be supported anymore. (`#3273 <https://github.com/aio-libs/aiohttp/pull/3273>`_)-
        Keep the query string by `normalize_path_middleware`. (`#3278 <https://github.com/aio-libs/aiohttp/pull/3278>`_)-
        Fix missing parameter ``raise_for_status`` for aiohttp.request() (`#3290 <https://github.com/aio-libs/aiohttp/pull/3290>`_)-
        Bracket IPv6 addresses in the HOST header (`#3304 <https://github.com/aio-libs/aiohttp/pull/3304>`_)-
        Fix default message for server ping and pong frames. (`#3308 <https://github.com/aio-libs/aiohttp/pull/3308>`_)-
        Fix tests/test_connector.py typo and tests/autobahn/server.py duplicate loop
        def. (`#3337 <https://github.com/aio-libs/aiohttp/pull/3337>`_)- Fix false-negative
        indicator end_of_HTTP_chunk in StreamReader.readchunk function (`#3361 <https://github.com/aio-libs/aiohttp/pull/3361>`_)-
        Release HTTP response before raising status exception (`#3364 <https://github.com/aio-libs/aiohttp/pull/3364>`_)-
        Fix task cancellation when ``sendfile()`` syscall is used by static file handling.
        (`#3383 <https://github.com/aio-libs/aiohttp/pull/3383>`_)- Fix stack trace
        for ``asyncio.TimeoutError`` which was not logged, when it is caught  in the
        handler. (`#3414 <https://github.com/aio-libs/aiohttp/pull/3414>`_)Improved
        Documentation----------------------- Improve documentation of ``Application.make_handler``
        parameters. (`#3152 <https://github.com/aio-libs/aiohttp/pull/3152>`_)- Fix
        BaseRequest.raw_headers doc. (`#3215 <https://github.com/aio-libs/aiohttp/pull/3215>`_)-
        Fix typo in TypeError exception reason in ``web.Application._handle`` (`#3229
        <https://github.com/aio-libs/aiohttp/pull/3229>`_)- Make server access log
        format placeholder %b documentation reflect  behavior and docstring. (`#3307
        <https://github.com/aio-libs/aiohttp/pull/3307>`_)Deprecations and Removals--------------------------
        Deprecate modification of ``session.requote_redirect_url`` (`#2278 <https://github.com/aio-libs/aiohttp/pull/2278>`_)-
        Deprecate ``stream.unread_data()`` (`#3260 <https://github.com/aio-libs/aiohttp/pull/3260>`_)-
        Deprecated use of boolean in ``resp.enable_compression()`` (`#3318 <https://github.com/aio-libs/aiohttp/pull/3318>`_)-
        Encourage creation of aiohttp public objects inside a coroutine (`#3331 <https://github.com/aio-libs/aiohttp/pull/3331>`_)-
        Drop dead ``Connection.detach()`` and ``Connection.writer``. Both methods
        were broken  for more than 2 years. (`#3358 <https://github.com/aio-libs/aiohttp/pull/3358>`_)-
        Deprecate ``app.loop``, ``request.loop``, ``client.loop`` and ``connector.loop``
        properties. (`#3374 <https://github.com/aio-libs/aiohttp/pull/3374>`_)- Deprecate
        explicit debug argument. Use asyncio debug mode instead. (`#3381 <https://github.com/aio-libs/aiohttp/pull/3381>`_)-
        Deprecate body parameter in HTTPException (and derived classes) constructor.
        (`#3385 <https://github.com/aio-libs/aiohttp/pull/3385>`_)- Deprecate bare
        connector close, use ``async with connector:`` and ``await connector.close()``
        instead. (`#3417 <https://github.com/aio-libs/aiohttp/pull/3417>`_)- Deprecate
        obsolete ``read_timeout`` and ``conn_timeout`` in ``ClientSession`` constructor.
        (`#3438 <https://github.com/aio-libs/aiohttp/pull/3438>`_)Misc----- #3341,
        #3351'
      Package: aiohttp
      Source: pip
      Version: '3.9.3'
      Hash: ''
      licenses:
      - Apache-2.0
      - MIT
      Title: aiohttp
      DownloadURL: https://files.pythonhosted.org/packages/18/93/1f005bbe044471a0444a82cdd7356f5120b9cf94fe2c50c0cdbf28f1258b/aiohttp-3.9.3.tar.gz
  bazaar:
    register: 'no'
    prim: 15/CTX1025933
    community_link: https://github.com/aio-libs/aiohttp
    community_name: https://github.com/aio-libs/aiohttp
    community_url: https://github.com/aio-libs/aiohttp
    component_comment: ''
    component_highlevel_description: Asynchronous HTTP client/server framework for
      asyncio and Python
    component_name: aiohttp
    component_platform: linux
    component_programing_language: Python
    component_version: V3.9.3
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/aio-libs/aiohttp/archive/v3.9.3.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1077991&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: aiohttp
    target_sw: linux
    vendor: pip
    version: '3.9.3'
    web_url: https://github.com/aio-libs/aiohttp
  licenses:
  - Apache-2.0
  - MIT
  name: aiohttp
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '3.9.3'
  mimer:
    linking: Static
    product_number: CTX1025933
    product_version_label: V3.9.3
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: aiosignal+1.3.1
  additional_info:
    fossa-attribution:
      Description: '=========aiosignal=========.. image:: https://github.com/aio-libs/aiosignal/workflows/CI/badge.svg   :target:
        https://github.com/aio-libs/aiosignal/actions?query=workflow%3ACI   :alt:
        GitHub status for master branch.. image:: https://codecov.io/gh/aio-libs/aiosignal/branch/master/graph/badge.svg   :target:
        https://codecov.io/gh/aio-libs/aiosignal   :alt: codecov.io status for master
        branch.. image:: https://badge.fury.io/py/aiosignal.svg   :target: https://pypi.org/project/aiosignal   :alt:
        Latest PyPI package version.. image:: https://readthedocs.org/projects/aiosignal/badge/?version=latest   :target:
        https://aiosignal.readthedocs.io/   :alt: Latest Read The Docs.. image:: https://img.shields.io/discourse/topics?server=https%3A%2F%2Faio-libs.discourse.group%2F   :target:
        https://aio-libs.discourse.group/   :alt: Discourse group for io-libs.. image::
        https://badges.gitter.im/Join%20Chat.svg   :target: https://gitter.im/aio-libs/Lobby   :alt:
        Chat on GitterIntroduction============A project to manage callbacks in `asyncio`
        projects.``Signal`` is a list of registered asynchronous callbacks.The signal''s
        life-cycle has two stages: after creation its contentcould be filled by using
        standard list operations: ``sig.append()``etc.After you call ``sig.freeze()``
        the signal is *frozen*: adding, removingand dropping callbacks is forbidden.The
        only available operation is calling the previously registeredcallbacks by
        using ``await sig.send(data)``.For concrete usage examples see the `Signals<https://docs.aiohttp.org/en/stable/web_advanced.html#aiohttp-web-signals>section
        of the `Web Server Advanced<https://docs.aiohttp.org/en/stable/web_advanced.html>`
        chapter of the `aiohttpdocumentation`_.Installation------------::   $ pip
        install aiosignalThe library requires Python 3.6 or newer.Documentation=============https://aiosignal.readthedocs.io/Communication
        channels======================*gitter chat* https://gitter.im/aio-libs/LobbyRequirements============-
        Python >= 3.6- frozenlist >= 1.0.0License=======``aiosignal`` is offered under
        the Apache 2 license.Source code===========The project is hosted on GitHub_Please
        file an issue in the `bug tracker<https://github.com/aio-libs/aiosignal/issues>`_
        if you have found a bugor have some suggestions to improve the library...
        _GitHub: https://github.com/aio-libs/aiosignal.. _aiohttp documentation: https://docs.aiohttp.org/'
      Package: aiosignal
      Source: pip
      Version: '1.3.1'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: aiosignal
      DownloadURL: https://files.pythonhosted.org/packages/ae/67/0952ed97a9793b4958e5736f6d2b346b414a2cd63e82d05940032f45b32f/aiosignal-1.3.1.tar.gz
  bazaar:
    register: 'no'
    prim: 2/CTX1035571
    community_link: https://github.com/aio-libs/aiosignal
    community_name: https://github.com/aio-libs/aiosignal
    community_url: https://github.com/aio-libs/aiosignal
    component_comment: ''
    component_highlevel_description: 'aiosignal: a list of registered asynchronous
      callbacks'
    component_name: aiosignal
    component_platform: linux
    component_programing_language: Python
    component_version: '1.3.1'
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/aio-libs/aiosignal/archive/v1.3.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1030036&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Ukraine
    crypto: 'NO'
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: aiosignal
    target_sw: linux
    vendor: pip
    version: '1.3.1'
    web_url: https://github.com/aio-libs/aiosignal
  licenses:
  - Apache-2.0
  name: aiosignal
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '1.3.1'
  mimer:
    linking: Static
    product_number: CTX1035571
    product_version_label: v1.3.1
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: astunparse+1.6.3
  additional_info:
    fossa-attribution:
      Description: '============AST Unparser============.. image:: https://badge.fury.io/py/astunparse.png    :target:
        http://badge.fury.io/py/astunparse.. image:: https://travis-ci.org/simonpercivall/astunparse.png?branch=master    :target:
        https://travis-ci.org/simonpercivall/astunparse.. image:: https://readthedocs.org/projects/astunparse/badge/    :target:
        https://astunparse.readthedocs.org/An AST unparser for Python.This is a factored
        out version of ``unparse`` found in the Pythonsource distribution; under Demo/parser
        in Python 2 and under Tools/parserin Python 3.Basic example::    import inspect    import
        ast    import astunparse    # get back the source code    astunparse.unparse(ast.parse(inspect.getsource(ast)))    #
        get a pretty-printed dump of the AST    astunparse.dump(ast.parse(inspect.getsource(ast)))This
        library is single-source compatible with Python 2.6 through Python 3.5. Itis
        authored by the Python core developers; I have simply merged the Python 2.7and
        the Python 3.5 source and test suites, and added a wrapper. This factoringout
        is to provide a library implementation that supports both versions.Added to
        this is a pretty-printing ``dump`` utility function.The test suite both runs
        specific tests and also roundtrips much of thestandard library.Extensions
        and Alternatives---------------------------Similar projects include:    *
        codegen_    * astor_    * astmonkey_    * astprint_None of these roundtrip
        much of the standard library and fail several of the basictests in the ``test_unparse``
        test suite.This library uses mature and core maintained code instead of trying
        to patchexisting libraries. The ``unparse`` and the ``test_unparse`` modulesare
        under the PSF license.Extensions include:    * typed-astunparse: extends astunparse
        to support type annotations.* Documentation: http://astunparse.rtfd.org.Features--------*
        unparses Python AST.* pretty-prints AST... _codegen: https://github.com/andreif/codegen..
        _astor: https://github.com/berkerpeksag/astor.. _astmonkey: https://github.com/konradhalas/astmonkey..
        _astprint: https://github.com/Manticore/astprintChangelog=========Here''s
        the recent changes to AST Unparser.1.6.3 - 2019-12-22~~~~~~~~~~~~~~~~~~* Add
        full support for Python 3.81.6.2 - 2019-01-19~~~~~~~~~~~~~~~~~~* Add support
        for the Constant node in Python 3.8* Add tests to the sdist1.6.1 - 2018-10-03~~~~~~~~~~~~~~~~~~*
        Fix the roundtripping of very complex f-strings.1.6.0 - 2018-09-30~~~~~~~~~~~~~~~~~~*
        Python 3.7 compatibility1.5.0 - 2017-02-05~~~~~~~~~~~~~~~~~~* Python 3.6 compatibility*
        bugfix: correct argparser option type1.4.0 - 2016-06-24~~~~~~~~~~~~~~~~~~*
        Support for the ``async`` keyword* Support for unparsing "Interactive" and
        "Expression" nodes1.3.0 - 2016-01-17~~~~~~~~~~~~~~~~~~* Python 3.5 compatibility1.2.0
        - 2014-04-03~~~~~~~~~~~~~~~~~~* Python 2.6 through 3.4 compatibility* A new
        function ``dump`` is added to return a pretty-printed version  of the AST.
        It''s also available when running ``python -m astunparse``  as the ``--dump``
        argument.1.1.0 - 2014-04-01~~~~~~~~~~~~~~~~~~* ``unparse`` will return the
        source code for an AST. It is pretty  feature-complete, and round-trips the
        stdlib, and is compatible with  Python 2.7 and Python 3.4.  Running ``python
        -m astunparse`` will print the round-tripped source  for any python files
        given as argument.'
      Package: astunparse
      Source: pip
      Version: '1.6.3'
      Hash: ''
      licenses:
      - BSD-2-Clause
      - BSD-3-Clause
      - Python-2.0
      Title: astunparse
      DownloadURL: https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl
  bazaar:
    register: 'no'
    prim: 1/CTX1029644
    community_link: https://github.com/simonpercivall/astunparse
    community_name: https://github.com/simonpercivall/astunparse
    community_url: https://github.com/simonpercivall/astunparse
    component_comment: ''
    component_highlevel_description: An AST unparser for Python.
    component_name: AST Unparser
    component_platform: linux
    component_programing_language: Python
    component_version: '1.6.3'
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    - FAL1159179/20 (Python Software Foundation License 2.0 (PSF-2.0))
    src_download_link: https://github.com/simonpercivall/astunparse/archive/v1.6.3.zip
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=942976&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: astunparse
    target_sw: linux
    vendor: pip
    version: '1.6.3'
    web_url: https://github.com/simonpercivall/astunparse
  licenses:
  - BSD-2-Clause
  - BSD-3-Clause
  - Python-2.0
  name: astunparse
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '1.6.3'
  mimer:
    linking: Static
    product_number: CTX1029644
    product_version_label: v1.6.3
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: async-timeout+4.0.3
  additional_info:
    fossa-attribution:
      Description: 'async-timeout=============.. image:: https://travis-ci.com/aio-libs/async-timeout.svg?branch=master    :target:
        https://travis-ci.com/aio-libs/async-timeout.. image:: https://codecov.io/gh/aio-libs/async-timeout/branch/master/graph/badge.svg    :target:
        https://codecov.io/gh/aio-libs/async-timeout.. image:: https://img.shields.io/pypi/v/async-timeout.svg    :target:
        https://pypi.python.org/pypi/async-timeout.. image:: https://badges.gitter.im/Join%20Chat.svg    :target:
        https://gitter.im/aio-libs/Lobby    :alt: Chat on Gitterasyncio-compatible
        timeout context manager.Usage example-------------The context manager is useful
        in cases when you want to apply timeoutlogic around block of code or in cases
        when ``asyncio.wait_for()`` isnot suitable. Also it''s much faster than ``asyncio.wait_for()``because
        ``timeout`` doesn''t create a new task.The ``timeout(delay, *, loop=None)``
        call returns a context managerthat cancels a block on *timeout* expiring::   from
        async_timeout import timeout   async with timeout(1.5):       await inner()1.
        If ``inner()`` is executed faster than in ``1.5`` seconds nothing   happens.2.
        Otherwise ``inner()`` is cancelled internally by sending   ``asyncio.CancelledError``
        into but ``asyncio.TimeoutError`` is   raised outside of context manager scope.*timeout*
        parameter could be ``None`` for skipping timeout functionality.Alternatively,
        ``timeout_at(when)`` can be used for schedulingat the absolute time::   loop
        = asyncio.get_event_loop()   now = loop.time()   async with timeout_at(now
        + 1.5):       await inner()Please note: it is not POSIX time but a time withundefined
        starting base, e.g. the time of the system power on.Context manager has ``.expired``
        property for check if timeout happensexactly in context manager::   async
        with timeout(1.5) as cm:       await inner()   print(cm.expired)The property
        is ``True`` if ``inner()`` execution is cancelled bytimeout context manager.If
        ``inner()`` call explicitly raises ``TimeoutError`` ``cm.expired``is ``False``.The
        scheduled deadline time is available as ``.deadline`` property::   async with
        timeout(1.5) as cm:       cm.deadlineNot finished yet timeout can be rescheduled
        by ``shift_by()``or ``shift_to()`` methods::   async with timeout(1.5) as
        cm:       cm.shift(1)  # add another second on waiting       cm.update(loop.time()
        + 5)  # reschedule to now+5 secondsRescheduling is forbidden if the timeout
        is expired or after exit from ``async with``code block.Installation------------::   $
        pip install async-timeoutThe library is Python 3 only!Authors and License-------------------The
        module is written by Andrew Svetlov.It''s *Apache 2* licensed and freely available.'
      Package: async-timeout
      Source: pip
      Version: '4.0.3'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: async-timeout
      DownloadURL: https://files.pythonhosted.org/packages/87/d6/21b30a550dafea84b1b8eee21b5e23fa16d010ae006011221f33dcd8d7f8/async-timeout-4.0.3.tar.gz
  bazaar:
    register: 'no'
    prim: 4/CTX1028739
    community_link: https://pypi.org/project/async-timeout
    community_name: https://pypi.org/project/async-timeout
    community_url: https://pypi.org/project/async-timeout
    component_comment: ''
    component_highlevel_description: asyncio-compatible timeout context manager.
    component_name: async-timeout
    component_platform: linux
    component_programing_language: Python
    component_version: '4.0.3'
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://files.pythonhosted.org/packages/87/d6/21b30a550dafea84b1b8eee21b5e23fa16d010ae006011221f33dcd8d7f8/async-timeout-4.0.3.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Low community activity.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1062789&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Ukraine
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: async-timeout
    target_sw: linux
    vendor: pip
    version: '4.0.3'
    web_url: https://github.com/aio-libs/async-timeout
  licenses:
  - Apache-2.0
  name: async-timeout
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '4.0.3'
  mimer:
    linking: Static
    product_number: CTX1028739
    product_version_label: '4.0.3'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: attrs+23.2.0
  additional_info:
    fossa-attribution:
      Description: "<p align=\"center\">  <a href=\"https://www.attrs.org/\">    <img
        src=\"https://raw.githubusercontent.com/python-attrs/attrs/main/docs/_static/attrs_logo.svg\"
        width=\"35%\" alt=\"attrs\" />  </a></p>*attrs* is the Python package that
        will bring back the **joy** of **writing classes** by relieving you from the
        drudgery of implementing object protocols (aka [dunder methods](https://www.attrs.org/en/latest/glossary.html#term-dunder-methods)).[Trusted
        by NASA](https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-profile/customizing-your-profile/personalizing-your-profile#list-of-qualifying-repositories-for-mars-2020-helicopter-contributor-achievement)
        for Mars missions since 2020!Its main goal is to help you to write **concise**
        and **correct** software without slowing down your code.## Sponsors*attrs*
        would not be possible without our [amazing sponsors](https://github.com/sponsors/hynek).Especially
        those generously supporting us at the *The Organization* tier and higher:<p
        align=\"center\">   <a href=\"https://www.variomedia.de/\"><img src=\"https://www.attrs.org/en/23.2.0/_static/sponsors/Variomedia.svg\"
        width=\"200\" height=\"60\" /></a>   <a href=\"https://tidelift.com/subscription/pkg/pypi-attrs?utm_source=pypi-attrs&utm_medium=referral&utm_campaign=enterprise&utm_term=repo\"><img
        src=\"https://www.attrs.org/en/23.2.0/_static/sponsors/Tidelift.svg\" width=\"200\"
        height=\"60\" /></a>   <a href=\"https://filepreviews.io/\"><img src=\"https://www.attrs.org/en/23.2.0/_static/sponsors/FilePreviews.svg\"
        width=\"200\" height=\"60\"/></a></p><p align=\"center\">   <strong>Please
        consider <a href=\"https://github.com/sponsors/hynek\">joining them</a> to
        help make <em>attrs</em>\u2019s maintenance more sustainable!</strong></p><!--
        teaser-end -->## Example*attrs* gives you a class decorator and a way to declaratively
        define the attributes on that class:<!-- code-begin -->```pycon>>> from attrs
        import asdict, define, make_class, Factory>>> @define... class SomeClass:...
        \    a_number: int = 42...     list_of_numbers: list[int] = Factory(list)......
        \    def hard_math(self, another_number):...         return self.a_number
        + sum(self.list_of_numbers) * another_number>>> sc = SomeClass(1, [1, 2, 3])>>>
        scSomeClass(a_number=1, list_of_numbers=[1, 2, 3])>>> sc.hard_math(3)19>>>
        sc == SomeClass(1, [1, 2, 3])True>>> sc != SomeClass(2, [3, 2, 1])True>>>
        asdict(sc){'a_number': 1, 'list_of_numbers': [1, 2, 3]}>>> SomeClass()SomeClass(a_number=42,
        list_of_numbers=[])>>> C = make_class(\"C\", [\"a\", \"b\"])>>> C(\"foo\",
        \"bar\")C(a='foo', b='bar')```After *declaring* your attributes, *attrs* gives
        you:- a concise and explicit overview of the class's attributes,- a nice human-readable
        `__repr__`,- equality-checking methods,- an initializer,- and much more,*without*
        writing dull boilerplate code again and again and *without* runtime performance
        penalties.**Hate type annotations**!?No problem!Types are entirely **optional**
        with *attrs*.Simply assign `attrs.field()` to the attributes instead of annotating
        them with types.---This example uses *attrs*'s modern APIs that have been
        introduced in version 20.1.0, and the *attrs* package import name that has
        been added in version 21.3.0.The classic APIs (`@attr.s`, `attr.ib`, plus
        their serious-business aliases) and the `attr` package import name will remain
        **indefinitely**.Please check out [*On The Core API Names*](https://www.attrs.org/en/latest/names.html)
        for a more in-depth explanation.## Data ClassesOn the tin, *attrs* might remind
        you of `dataclasses` (and indeed, `dataclasses` [are a descendant](https://hynek.me/articles/import-attrs/)
        of *attrs*).In practice it does a lot more and is more flexible.For instance
        it allows you to define [special handling of NumPy arrays for equality checks](https://www.attrs.org/en/stable/comparison.html#customization),
        allows more ways to [plug into the initialization process](https://www.attrs.org/en/stable/init.html#hooking-yourself-into-initialization),
        and allows for stepping through the generated methods using a debugger.For
        more details, please refer to our [comparison page](https://www.attrs.org/en/stable/why.html#data-classes).##
        Project Information- [**Changelog**](https://www.attrs.org/en/stable/changelog.html)-
        [**Documentation**](https://www.attrs.org/)- [**PyPI**](https://pypi.org/project/attrs/)-
        [**Source Code**](https://github.com/python-attrs/attrs)- [**Contributing**](https://github.com/python-attrs/attrs/blob/main/.github/CONTRIBUTING.md)-
        [**Third-party Extensions**](https://github.com/python-attrs/attrs/wiki/Extensions-to-attrs)-
        **Get Help**: please use the `python-attrs` tag on [StackOverflow](https://stackoverflow.com/questions/tagged/python-attrs)###
        *attrs* for EnterpriseAvailable as part of the Tidelift Subscription.The maintainers
        of *attrs* and thousands of other packages are working with Tidelift to deliver
        commercial support and maintenance for the open source packages you use to
        build your applications.Save time, reduce risk, and improve code health, while
        paying the maintainers of the exact packages you use.[Learn more.](https://tidelift.com/subscription/pkg/pypi-attrs?utm_source=pypi-attrs&utm_medium=referral&utm_campaign=enterprise&utm_term=repo)##
        Release Information### Changes- The type annotation for `attrs.resolve_types()`
        is now correct.  [#1141](https://github.com/python-attrs/attrs/issues/1141)-
        Type stubs now use `typing.dataclass_transform` to decorate dataclass-like
        decorators, instead of the non-standard `__dataclass_transform__` special
        form, which is only supported by Pyright.  [#1158](https://github.com/python-attrs/attrs/issues/1158)-
        Fixed serialization of namedtuple fields using `attrs.asdict/astuple()` with
        `retain_collection_types=True`.  [#1165](https://github.com/python-attrs/attrs/issues/1165)-
        `attrs.AttrsInstance` is now a `typing.Protocol` in both type hints and code.
        \ This allows you to subclass it along with another `Protocol`.  [#1172](https://github.com/python-attrs/attrs/issues/1172)-
        If *attrs* detects that `__attrs_pre_init__` accepts more than just `self`,
        it will call it with the same arguments as `__init__` was called.  This allows
        you to, for example, pass arguments to `super().__init__()`.  [#1187](https://github.com/python-attrs/attrs/issues/1187)-
        Slotted classes now transform `functools.cached_property` decorated methods
        to support equivalent semantics.  [#1200](https://github.com/python-attrs/attrs/issues/1200)-
        Added *class_body* argument to `attrs.make_class()` to provide additional
        attributes for newly created classes.  It is, for example, now possible to
        attach methods.  [#1203](https://github.com/python-attrs/attrs/issues/1203)---[Full
        changelog](https://www.attrs.org/en/stable/changelog.html)"
      Package: attrs
      Source: pip
      Version: '23.2.0'
      Hash: ''
      licenses:
      - MIT
      Title: attrs
      DownloadURL: https://files.pythonhosted.org/packages/e3/fc/f800d51204003fa8ae392c4e8278f256206e7a919b708eef054f5f4b650d/attrs-23.2.0.tar.gz
  bazaar:
    register: 'no'
    prim: 17/CTX1020696
    community_link: https://github.com/python-attrs/attrs
    community_name: https://github.com/python-attrs/attrs
    community_url: https://github.com/python-attrs/attrs
    component_comment: ''
    component_highlevel_description: Classes Without Boilerplate.Its main goal is
      to help you to write concise and correct software without slowing down your
      code.
    component_name: attrs
    component_platform: linux
    component_programing_language: Python
    component_version: '23.2.0'
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://github.com/python-attrs/attrs/archive/23.2.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1075921&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Germany
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: attrs
    target_sw: linux
    vendor: pip
    version: '23.2.0'
    web_url: https://pypi.org/project/attrs/23.2.0/
  licenses:
  - MIT
  name: attrs
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '23.2.0'
  mimer:
    linking: Static
    product_number: CTX1020696
    product_version_label: '23.2.0'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: cachetools+5.3.3
  additional_info:
    fossa-attribution:
      Description: 'cachetools========================================================================..
        image:: https://img.shields.io/pypi/v/cachetools   :target: https://pypi.org/project/cachetools/   :alt:
        Latest PyPI version.. image:: https://img.shields.io/github/actions/workflow/status/tkem/cachetools/ci.yml   :target:
        https://github.com/tkem/cachetools/actions/workflows/ci.yml   :alt: CI build
        status.. image:: https://img.shields.io/readthedocs/cachetools   :target:
        https://cachetools.readthedocs.io/   :alt: Documentation build status.. image::
        https://img.shields.io/codecov/c/github/tkem/cachetools/master.svg   :target:
        https://codecov.io/gh/tkem/cachetools   :alt: Test coverage.. image:: https://img.shields.io/librariesio/sourcerank/pypi/cachetools   :target:
        https://libraries.io/pypi/cachetools   :alt: Libraries.io SourceRank.. image::
        https://img.shields.io/github/license/tkem/cachetools   :target: https://raw.github.com/tkem/cachetools/master/LICENSE   :alt:
        License.. image:: https://img.shields.io/badge/code%20style-black-000000.svg   :target:
        https://github.com/psf/black   :alt: Code style: blackThis module provides
        various memoizing collections and decorators,including variants of the Python
        Standard Library''s `@lru_cache`_function decorator... code-block:: python   from
        cachetools import cached, LRUCache, TTLCache   # speed up calculating Fibonacci
        numbers with dynamic programming   @cached(cache={})   def fib(n):       return
        n if n < 2 else fib(n - 1) + fib(n - 2)   # cache least recently used Python
        Enhancement Proposals   @cached(cache=LRUCache(maxsize=32))   def get_pep(num):       url
        = ''http://www.python.org/dev/peps/pep-%04d/'' % num       with urllib.request.urlopen(url)
        as s:           return s.read()   # cache weather data for no longer than
        ten minutes   @cached(cache=TTLCache(maxsize=1024, ttl=600))   def get_weather(place):       return
        owm.weather_at_place(place).get_weather()For the purpose of this module, a
        *cache* is a mutable_ mapping_ of afixed maximum size.  When the cache is
        full, i.e. by adding anotheritem the cache would exceed its maximum size,
        the cache must choosewhich item(s) to discard based on a suitable `cache algorithm`_.This
        module provides multiple cache classes based on different cachealgorithms,
        as well as decorators for easily memoizing function andmethod calls.Installation------------------------------------------------------------------------cachetools
        is available from PyPI_ and can be installed by running::  pip install cachetoolsTyping
        stubs for this package are provided by typeshed_ and can beinstalled by running::  pip
        install types-cachetoolsProject Resources-------------------------------------------------------------------------
        `Documentation`_- `Issue tracker`_- `Source code`_- `Change log`_Related Projects-------------------------------------------------------------------------
        asyncache_: Helpers to use cachetools with async functions- cacheing_: Pure
        Python Cacheing Library- CacheToolsUtils_: Cachetools Utilities- kids.cache_:
        Kids caching library- shelved-cache_: Persistent cache for Python cachetoolsLicense------------------------------------------------------------------------Copyright
        (c) 2014-2024 Thomas Kemmer.Licensed under the `MIT License`_... _@lru_cache:
        https://docs.python.org/3/library/functools.html#functools.lru_cache.. _mutable:
        https://docs.python.org/dev/glossary.html#term-mutable.. _mapping: https://docs.python.org/dev/glossary.html#term-mapping..
        _cache algorithm: https://en.wikipedia.org/wiki/Cache_algorithms.. _PyPI:
        https://pypi.org/project/cachetools/.. _typeshed: https://github.com/python/typeshed/..
        _Documentation: https://cachetools.readthedocs.io/.. _Issue tracker: https://github.com/tkem/cachetools/issues/..
        _Source code: https://github.com/tkem/cachetools/.. _Change log: https://github.com/tkem/cachetools/blob/master/CHANGELOG.rst..
        _MIT License: https://raw.github.com/tkem/cachetools/master/LICENSE.. _asyncache:
        https://pypi.org/project/asyncache/.. _cacheing: https://github.com/breid48/cacheing..
        _CacheToolsUtils: https://pypi.org/project/CacheToolsUtils/.. _kids.cache:
        https://pypi.org/project/kids.cache/.. _shelved-cache: https://pypi.org/project/shelved-cache/'
      Package: cachetools
      Source: pip
      Version: '5.3.3'
      Hash: ''
      licenses:
      - MIT
      Title: cachetools
      DownloadURL: https://files.pythonhosted.org/packages/b3/4d/27a3e6dd09011649ad5210bdf963765bc8fa81a0827a4fc01bafd2705c5b/cachetools-5.3.3.tar.gz
  bazaar:
    register: 'no'
    prim: 20/CTX1022067
    community_link: https://pypi.org/project/cachetools
    community_name: https://pypi.org/project/cachetools
    community_url: https://pypi.org/project/cachetools
    component_comment: ''
    component_highlevel_description: Extensible memoizing collections and decorators
    component_name: cachetools
    component_platform: linux
    component_programing_language: Python
    component_version: '5.3.3'
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://files.pythonhosted.org/packages/b3/4d/27a3e6dd09011649ad5210bdf963765bc8fa81a0827a4fc01bafd2705c5b/cachetools-5.3.3.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1083763&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: cachetools
    target_sw: linux
    vendor: pip
    version: '5.3.3'
    web_url: https://github.com/tkem/cachetools/
  licenses:
  - MIT
  name: cachetools
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '5.3.3'
  mimer:
    linking: Static
    product_number: CTX1022067
    product_version_label: v5.3.3
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: certifi+2024.2.2
  additional_info:
    fossa-attribution:
      Description: 'Certifi: Python SSL Certificates================================Certifi
        provides Mozilla''s carefully curated collection of Root Certificates forvalidating
        the trustworthiness of SSL certificates while verifying the identityof TLS
        hosts. It has been extracted from the `Requests`_ project.Installation------------``certifi``
        is available on PyPI. Simply install it with ``pip``::    $ pip install certifiUsage-----To
        reference the installed certificate authority (CA) bundle, you can use thebuilt-in
        function::    >>> import certifi    >>> certifi.where()    ''/usr/local/lib/python3.7/site-packages/certifi/cacert.pem''Or
        from the command line::    $ python -m certifi    /usr/local/lib/python3.7/site-packages/certifi/cacert.pemEnjoy!..
        _`Requests`: https://requests.readthedocs.io/en/master/Addition/Removal of
        Certificates--------------------------------Certifi does not support any addition/removal
        or other modification of theCA trust store content. This project is intended
        to provide a reliable andhighly portable root of trust to python deployments.
        Look to upstream projectsfor methods to use alternate trust.'
      Package: certifi
      Source: pip
      Version: '2024.2.2'
      Hash: ''
      licenses:
      - MPL-2.0
      Title: certifi
      DownloadURL: https://files.pythonhosted.org/packages/71/da/e94e26401b62acd6d91df2b52954aceb7f561743aa5ccc32152886c76c96/certifi-2024.2.2.tar.gz
  bazaar:
    register: 'no'
    prim: 39/CTX1020628
    community_link: https://github.com/certifi/python-certifi
    community_name: https://github.com/certifi/python-certifi
    community_url: https://github.com/certifi/python-certifi
    component_comment: ''
    component_highlevel_description: (Python Distribution) A carefully curated collection of Root Certificates for validating the trustworthiness of SSL certificates while verifying the identity of TLS hosts.
    component_name: certifi
    component_platform: linux
    component_programing_language: Python
    component_version: 2024.02.02
    licenses:
    - FAL1159005/20 (Mozilla Public License 2.0 (MPL-2.0))
    src_download_link: https://files.pythonhosted.org/packages/71/da/e94e26401b62acd6d91df2b52954aceb7f561743aa5ccc32152886c76c96/certifi-2024.2.2.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1077790&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: certifi
    target_sw: linux
    vendor: pip
    version: '2024.2.2'
    web_url: https://github.com/certifi/python-certifi
  licenses:
  - MPL-2.0
  name: certifi
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '2024.2.2'
  mimer:
    linking: Static
    product_number: CTX1020628
    product_version_label: 2024.02.02
    selected_licenses:
    - MPL-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: charset-normalizer+3.3.2
  additional_info:
    fossa-attribution:
      Description: "<h1 align=\"center\">Charset Detection, for Everyone \U0001F44B</h1><p
        align=\"center\">  <sup>The Real First Universal Charset Detector</sup><br>
        \ <a href=\"https://pypi.org/project/charset-normalizer\">    <img src=\"https://img.shields.io/pypi/pyversions/charset_normalizer.svg?orange=blue\"
        />  </a>  <a href=\"https://pepy.tech/project/charset-normalizer/\">    <img
        alt=\"Download Count Total\" src=\"https://static.pepy.tech/badge/charset-normalizer/month\"
        />  </a>  <a href=\"https://bestpractices.coreinfrastructure.org/projects/7297\">
        \   <img src=\"https://bestpractices.coreinfrastructure.org/projects/7297/badge\">
        \ </a></p><p align=\"center\">  <sup><i>Featured Packages</i></sup><br>  <a
        href=\"https://github.com/jawah/niquests\">   <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Niquests-HTTP_1.1%2C%202%2C_and_3_Client-cyan\">
        \ </a>  <a href=\"https://github.com/jawah/wassima\">   <img alt=\"Static
        Badge\" src=\"https://img.shields.io/badge/Wassima-Certifi_Killer-cyan\">
        \ </a></p><p align=\"center\">  <sup><i>In other language (unofficial port
        - by the community)</i></sup><br>  <a href=\"https://github.com/nickspring/charset-normalizer-rs\">
        \  <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Rust-red\">
        \ </a></p>> A library that helps you read text from an unknown charset encoding.<br
        /> Motivated by `chardet`,> I'm trying to resolve the issue by taking a new
        approach.> All IANA character set names for which the Python core library
        provides codecs are supported.<p align=\"center\">  >>>>> <a href=\"https://charsetnormalizerweb.ousret.now.sh\"
        target=\"_blank\">\U0001F449 Try Me Online Now, Then Adopt Me \U0001F448 </a>
        <<<<<</p>This project offers you an alternative to **Universal Charset Encoding
        Detector**, also known as **Chardet**.| Feature                                          |
        [Chardet](https://github.com/chardet/chardet) |                                         Charset
        Normalizer                                         | [cChardet](https://github.com/PyYoshi/cChardet)
        ||--------------------------------------------------|:---------------------------------------------:|:--------------------------------------------------------------------------------------------------:|:-----------------------------------------------:||
        `Fast`                                           |                       \u274C
        \                      |                                                 \u2705
        \                                                 |                        \u2705
        \                       || `Universal**`                                    |
        \                      \u274C                       |                                                 \u2705
        \                                                 |                        \u274C
        \                       || `Reliable` **without** distinguishable standards
        |                       \u274C                       |                                                 \u2705
        \                                                 |                        \u2705
        \                       || `Reliable` **with** distinguishable standards    |
        \                      \u2705                       |                                                 \u2705
        \                                                 |                        \u2705
        \                       || `License`                                        |
        \          LGPL-2.1<br>_restrictive_           |                                                MIT
        \                                                |            MPL-1.1<br>_restrictive_
        \            || `Native Python`                                  |                       \u2705
        \                      |                                                 \u2705
        \                                                 |                        \u274C
        \                       || `Detect spoken language`                         |
        \                      \u274C                       |                                                 \u2705
        \                                                 |                       N/A
        \                      || `UnicodeDecodeError Safety`                      |
        \                      \u274C                       |                                                 \u2705
        \                                                 |                        \u274C
        \                       || `Whl Size (min)`                                 |
        \                  193.6 kB                    |                                               42
        kB                                                |                     ~200
        kB                     || `Supported Encoding`                             |
        \                     33                       | \U0001F389 [99](https://charset-normalizer.readthedocs.io/en/latest/user/support.html#supported-encodings)
        |                       40                        |<p align=\"center\"><img
        src=\"https://i.imgflip.com/373iay.gif\" alt=\"Reading Normalized Text\" width=\"226\"/><img
        src=\"https://media.tenor.com/images/c0180f70732a18b4965448d33adba3d0/tenor.gif\"
        alt=\"Cat Reading Text\" width=\"200\"/></p>*\\*\\* : They are clearly using
        specific code for a specific encoding even if covering most of used one*<br>
        Did you got there because of the logs? See [https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html](https://charset-normalizer.readthedocs.io/en/latest/user/miscellaneous.html)##
        \u26A1 PerformanceThis package offer better performance than its counterpart
        Chardet. Here are some numbers.| Package                                       |
        Accuracy | Mean per file (ms) | File per sec (est) ||-----------------------------------------------|:--------:|:------------------:|:------------------:||
        [chardet](https://github.com/chardet/chardet) |   86 %   |       200 ms       |
        \    5 file/sec     || charset-normalizer                            | **98
        %** |     **10 ms**      |    100 file/sec    || Package                                       |
        99th percentile | 95th percentile | 50th percentile ||-----------------------------------------------|:---------------:|:---------------:|:---------------:||
        [chardet](https://github.com/chardet/chardet) |     1200 ms     |     287
        ms      |      23 ms      || charset-normalizer                            |
        \    100 ms      |      50 ms      |      5 ms       |Chardet's performance
        on larger file (1MB+) are very poor. Expect huge difference on large payload.>
        Stats are generated using 400+ files using default parameters. More details
        on used files, see GHA workflows.> And yes, these results might change at
        any time. The dataset can be updated to include more files.> The actual delays
        heavily depends on your CPU capabilities. The factors should remain the same.>
        Keep in mind that the stats are generous and that Chardet accuracy vs our
        is measured using Chardet initial capability> (eg. Supported Encoding) Challenge-them
        if you want.## \u2728 InstallationUsing pip:```shpip install charset-normalizer
        -U```## \U0001F680 Basic Usage### CLIThis package comes with a CLI.```usage:
        normalizer [-h] [-v] [-a] [-n] [-m] [-r] [-f] [-t THRESHOLD]                  file
        [file ...]The Real First Universal Charset Detector. Discover originating
        encoding usedon text file. Normalize text to unicode.positional arguments:
        \ files                 File(s) to be analysedoptional arguments:  -h, --help
        \           show this help message and exit  -v, --verbose         Display
        complementary information about file if any.                        Stdout
        will contain logs about the detection process.  -a, --with-alternative                        Output
        complementary possibilities if any. Top-level                        JSON
        WILL be a list.  -n, --normalize       Permit to normalize input file. If
        not set, program                        does not write anything.  -m, --minimal
        \        Only output the charset detected to STDOUT. Disabling                        JSON
        output.  -r, --replace         Replace file when trying to normalize it instead
        of                        creating a new one.  -f, --force           Replace
        file without asking if you are sure, use this                        flag
        with caution.  -t THRESHOLD, --threshold THRESHOLD                        Define
        a custom maximum amount of chaos allowed in                        decoded
        content. 0. <= chaos <= 1.  --version             Show version information
        and exit.``````bashnormalizer ./data/sample.1.fr.srt```or```bashpython -m
        charset_normalizer ./data/sample.1.fr.srt```\U0001F389 Since version 1.4.0
        the CLI produce easily usable stdout result in JSON format.```json{    \"path\":
        \"/home/default/projects/charset_normalizer/data/sample.1.fr.srt\",    \"encoding\":
        \"cp1252\",    \"encoding_aliases\": [        \"1252\",        \"windows_1252\"
        \   ],    \"alternative_encodings\": [        \"cp1254\",        \"cp1256\",
        \       \"cp1258\",        \"iso8859_14\",        \"iso8859_15\",        \"iso8859_16\",
        \       \"iso8859_3\",        \"iso8859_9\",        \"latin_1\",        \"mbcs\"
        \   ],    \"language\": \"French\",    \"alphabets\": [        \"Basic Latin\",
        \       \"Latin-1 Supplement\"    ],    \"has_sig_or_bom\": false,    \"chaos\":
        0.149,    \"coherence\": 97.152,    \"unicode_path\": null,    \"is_preferred\":
        true}```### Python*Just print out normalized text*```pythonfrom charset_normalizer
        import from_pathresults = from_path('./my_subtitle.srt')print(str(results.best()))```*Upgrade
        your code without effort*```pythonfrom charset_normalizer import detect```The
        above code will behave the same as **chardet**. We ensure that we offer the
        best (reasonable) BC result possible.See the docs for advanced usage : [readthedocs.io](https://charset-normalizer.readthedocs.io/en/latest/)##
        \U0001F607 WhyWhen I started using Chardet, I noticed that it was not suited
        to my expectations, and I wanted to propose areliable alternative using a
        completely different method. Also! I never back down on a good challenge!I
        **don't care** about the **originating charset** encoding, because **two different
        tables** canproduce **two identical rendered string.**What I want is to get
        readable text, the best I can. In a way, **I'm brute forcing text decoding.**
        How cool is that ? \U0001F60EDon't confuse package **ftfy** with charset-normalizer
        or chardet. ftfy goal is to repair unicode string whereas charset-normalizer
        to convert raw file in unknown encoding to unicode.## \U0001F370 How  - Discard
        all charset encoding table that could not fit the binary content.  - Measure
        noise, or the mess once opened (by chunks) with a corresponding charset encoding.
        \ - Extract matches with the lowest mess detected.  - Additionally, we measure
        coherence / probe for a language.**Wait a minute**, what is noise/mess and
        coherence according to **YOU ?***Noise :* I opened hundred of text files,
        **written by humans**, with the wrong encoding table. **I observed**, then**I
        established** some ground rules about **what is obvious** when **it seems
        like** a mess. I know that my interpretation of what is noise is probably
        incomplete, feel free to contribute in order to improve or rewrite it.*Coherence
        :* For each language there is on earth, we have computed ranked letter appearance
        occurrences (the best we can). So I thoughtthat intel is worth something here.
        So I use those records against decoded text to check if I can detect intelligent
        design.## \u26A1 Known limitations  - Language detection is unreliable when
        text contains two or more languages sharing identical letters. (eg. HTML (english
        tags) + Turkish content (Sharing Latin characters))  - Every charset detector
        heavily depends on sufficient content. In common cases, do not bother run
        detection on very tiny content.## \u26A0\uFE0F About Python EOLs**If you are
        running:**- Python >=2.7,<3.5: Unsupported- Python 3.5: charset-normalizer
        < 2.1- Python 3.6: charset-normalizer < 3.1- Python 3.7: charset-normalizer
        < 4.0Upgrade your Python interpreter as soon as possible.## \U0001F464 ContributingContributions,
        issues and feature requests are very much welcome.<br />Feel free to check
        [issues page](https://github.com/ousret/charset_normalizer/issues) if you
        want to contribute.## \U0001F4DD LicenseCopyright \xA9 [Ahmed TAHRI @Ousret](https://github.com/Ousret).<br
        />This project is [MIT](https://github.com/Ousret/charset_normalizer/blob/master/LICENSE)
        licensed.Characters frequencies used in this project \xA9 2012 [Denny Vrande\u010Di\u0107](http://simia.net/letters/)##
        \U0001F4BC For EnterpriseProfessional support for charset-normalizer is available
        as part of the [TideliftSubscription][1]. Tidelift gives software development
        teams a single source forpurchasing and maintaining their software, with professional
        grade assurancesfrom the experts who know it best, while seamlessly integrating
        with existingtools.[1]: https://tidelift.com/subscription/pkg/pypi-charset-normalizer?utm_source=pypi-charset-normalizer&utm_medium=readme#
        ChangelogAll notable changes to charset-normalizer will be documented in this
        file. This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).The
        format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).##
        [3.3.2](https://github.com/Ousret/charset_normalizer/compare/3.3.1...3.3.2)
        (2023-10-31)### Fixed- Unintentional memory usage regression when using large
        payload that match several encoding (#376)- Regression on some detection case
        showcased in the documentation (#371)### Added- Noise (md) probe that identify
        malformed arabic representation due to the presence of letters in isolated
        form (credit to my wife)## [3.3.1](https://github.com/Ousret/charset_normalizer/compare/3.3.0...3.3.1)
        (2023-10-22)### Changed- Optional mypyc compilation upgraded to version 1.6.1
        for Python >= 3.8- Improved the general detection reliability based on reports
        from the community## [3.3.0](https://github.com/Ousret/charset_normalizer/compare/3.2.0...3.3.0)
        (2023-09-30)### Added- Allow to execute the CLI (e.g. normalizer) through
        `python -m charset_normalizer.cli` or `python -m charset_normalizer`- Support
        for 9 forgotten encoding that are supported by Python but unlisted in `encoding.aliases`
        as they have no alias (#323)### Removed- (internal) Redundant utils.is_ascii
        function and unused function is_private_use_only- (internal) charset_normalizer.assets
        is moved inside charset_normalizer.constant### Changed- (internal) Unicode
        code blocks in constants are updated using the latest v15.0.0 definition to
        improve detection- Optional mypyc compilation upgraded to version 1.5.1 for
        Python >= 3.8### Fixed- Unable to properly sort CharsetMatch when both chaos/noise
        and coherence were close due to an unreachable condition in \\_\\_lt\\_\\_
        (#350)## [3.2.0](https://github.com/Ousret/charset_normalizer/compare/3.1.0...3.2.0)
        (2023-06-07)### Changed- Typehint for function `from_path` no longer enforce
        `PathLike` as its first argument- Minor improvement over the global detection
        reliability### Added- Introduce function `is_binary` that relies on main capabilities,
        and optimized to detect binaries- Propagate `enable_fallback` argument throughout
        `from_bytes`, `from_path`, and `from_fp` that allow a deeper control over
        the detection (default True)- Explicit support for Python 3.12### Fixed- Edge
        case detection failure where a file would contain 'very-long' camel cased
        word (Issue #289)## [3.1.0](https://github.com/Ousret/charset_normalizer/compare/3.0.1...3.1.0)
        (2023-03-06)### Added- Argument `should_rename_legacy` for legacy function
        `detect` and disregard any new arguments without errors (PR #262)### Removed-
        Support for Python 3.6 (PR #260)### Changed- Optional speedup provided by
        mypy/c 1.0.1## [3.0.1](https://github.com/Ousret/charset_normalizer/compare/3.0.0...3.0.1)
        (2022-11-18)### Fixed- Multi-bytes cutter/chunk generator did not always cut
        correctly (PR #233)### Changed- Speedup provided by mypy/c 0.990 on Python
        >= 3.7## [3.0.0](https://github.com/Ousret/charset_normalizer/compare/2.1.1...3.0.0)
        (2022-10-20)### Added- Extend the capability of explain=True when cp_isolation
        contains at most two entries (min one), will log in details of the Mess-detector
        results- Support for alternative language frequency set in charset_normalizer.assets.FREQUENCIES-
        Add parameter `language_threshold` in `from_bytes`, `from_path` and `from_fp`
        to adjust the minimum expected coherence ratio- `normalizer --version` now
        specify if current version provide extra speedup (meaning mypyc compilation
        whl)### Changed- Build with static metadata using 'build' frontend- Make the
        language detection stricter- Optional: Module `md.py` can be compiled using
        Mypyc to provide an extra speedup up to 4x faster than v2.1### Fixed- CLI
        with opt --normalize fail when using full path for files- TooManyAccentuatedPlugin
        induce false positive on the mess detection when too few alpha character have
        been fed to it- Sphinx warnings when generating the documentation### Removed-
        Coherence detector no longer return 'Simple English' instead return 'English'-
        Coherence detector no longer return 'Classical Chinese' instead return 'Chinese'-
        Breaking: Method `first()` and `best()` from CharsetMatch- UTF-7 will no longer
        appear as \"detected\" without a recognized SIG/mark (is unreliable/conflict
        with ASCII)- Breaking: Class aliases CharsetDetector, CharsetDoctor, CharsetNormalizerMatch
        and CharsetNormalizerMatches- Breaking: Top-level function `normalize`- Breaking:
        Properties `chaos_secondary_pass`, `coherence_non_latin` and `w_counter` from
        CharsetMatch- Support for the backport `unicodedata2`## [3.0.0rc1](https://github.com/Ousret/charset_normalizer/compare/3.0.0b2...3.0.0rc1)
        (2022-10-18)### Added- Extend the capability of explain=True when cp_isolation
        contains at most two entries (min one), will log in details of the Mess-detector
        results- Support for alternative language frequency set in charset_normalizer.assets.FREQUENCIES-
        Add parameter `language_threshold` in `from_bytes`, `from_path` and `from_fp`
        to adjust the minimum expected coherence ratio### Changed- Build with static
        metadata using 'build' frontend- Make the language detection stricter### Fixed-
        CLI with opt --normalize fail when using full path for files- TooManyAccentuatedPlugin
        induce false positive on the mess detection when too few alpha character have
        been fed to it### Removed- Coherence detector no longer return 'Simple English'
        instead return 'English'- Coherence detector no longer return 'Classical Chinese'
        instead return 'Chinese'## [3.0.0b2](https://github.com/Ousret/charset_normalizer/compare/3.0.0b1...3.0.0b2)
        (2022-08-21)### Added- `normalizer --version` now specify if current version
        provide extra speedup (meaning mypyc compilation whl)### Removed- Breaking:
        Method `first()` and `best()` from CharsetMatch- UTF-7 will no longer appear
        as \"detected\" without a recognized SIG/mark (is unreliable/conflict with
        ASCII)### Fixed- Sphinx warnings when generating the documentation## [3.0.0b1](https://github.com/Ousret/charset_normalizer/compare/2.1.0...3.0.0b1)
        (2022-08-15)### Changed- Optional: Module `md.py` can be compiled using Mypyc
        to provide an extra speedup up to 4x faster than v2.1### Removed- Breaking:
        Class aliases CharsetDetector, CharsetDoctor, CharsetNormalizerMatch and CharsetNormalizerMatches-
        Breaking: Top-level function `normalize`- Breaking: Properties `chaos_secondary_pass`,
        `coherence_non_latin` and `w_counter` from CharsetMatch- Support for the backport
        `unicodedata2`## [2.1.1](https://github.com/Ousret/charset_normalizer/compare/2.1.0...2.1.1)
        (2022-08-19)### Deprecated- Function `normalize` scheduled for removal in
        3.0### Changed- Removed useless call to decode in fn is_unprintable (#206)###
        Fixed- Third-party library (i18n xgettext) crashing not recognizing utf_8
        (PEP 263) with underscore from [@aleksandernovikov](https://github.com/aleksandernovikov)
        (#204)## [2.1.0](https://github.com/Ousret/charset_normalizer/compare/2.0.12...2.1.0)
        (2022-06-19)### Added- Output the Unicode table version when running the CLI
        with `--version` (PR #194)### Changed- Re-use decoded buffer for single byte
        character sets from [@nijel](https://github.com/nijel) (PR #175)- Fixing some
        performance bottlenecks from [@deedy5](https://github.com/deedy5) (PR #183)###
        Fixed- Workaround potential bug in cpython with Zero Width No-Break Space
        located in Arabic Presentation Forms-B, Unicode 1.1 not acknowledged as space
        (PR #175)- CLI default threshold aligned with the API threshold from [@oleksandr-kuzmenko](https://github.com/oleksandr-kuzmenko)
        (PR #181)### Removed- Support for Python 3.5 (PR #192)### Deprecated- Use
        of backport unicodedata from `unicodedata2` as Python is quickly catching
        up, scheduled for removal in 3.0 (PR #194)## [2.0.12](https://github.com/Ousret/charset_normalizer/compare/2.0.11...2.0.12)
        (2022-02-12)### Fixed- ASCII miss-detection on rare cases (PR #170) ## [2.0.11](https://github.com/Ousret/charset_normalizer/compare/2.0.10...2.0.11)
        (2022-01-30)### Added- Explicit support for Python 3.11 (PR #164)### Changed-
        The logging behavior have been completely reviewed, now using only TRACE and
        DEBUG levels (PR #163 #165)## [2.0.10](https://github.com/Ousret/charset_normalizer/compare/2.0.9...2.0.10)
        (2022-01-04)### Fixed- Fallback match entries might lead to UnicodeDecodeError
        for large bytes sequence (PR #154)### Changed- Skipping the language-detection
        (CD) on ASCII (PR #155)## [2.0.9](https://github.com/Ousret/charset_normalizer/compare/2.0.8...2.0.9)
        (2021-12-03)### Changed- Moderating the logging impact (since 2.0.8) for specific
        environments (PR #147)### Fixed- Wrong logging level applied when setting
        kwarg `explain` to True (PR #146)## [2.0.8](https://github.com/Ousret/charset_normalizer/compare/2.0.7...2.0.8)
        (2021-11-24)### Changed- Improvement over Vietnamese detection (PR #126)-
        MD improvement on trailing data and long foreign (non-pure latin) data (PR
        #124)- Efficiency improvements in cd/alphabet_languages from [@adbar](https://github.com/adbar)
        (PR #122)- call sum() without an intermediary list following PEP 289 recommendations
        from [@adbar](https://github.com/adbar) (PR #129)- Code style as refactored
        by Sourcery-AI (PR #131) - Minor adjustment on the MD around european words
        (PR #133)- Remove and replace SRTs from assets / tests (PR #139)- Initialize
        the library logger with a `NullHandler` by default from [@nmaynes](https://github.com/nmaynes)
        (PR #135)- Setting kwarg `explain` to True will add provisionally (bounded
        to function lifespan) a specific stream handler (PR #135)### Fixed- Fix large
        (misleading) sequence giving UnicodeDecodeError (PR #137)- Avoid using too
        insignificant chunk (PR #137)### Added- Add and expose function `set_logging_handler`
        to configure a specific StreamHandler from [@nmaynes](https://github.com/nmaynes)
        (PR #135)- Add `CHANGELOG.md` entries, format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/)
        (PR #141)## [2.0.7](https://github.com/Ousret/charset_normalizer/compare/2.0.6...2.0.7)
        (2021-10-11)### Added- Add support for Kazakh (Cyrillic) language detection
        (PR #109)### Changed- Further, improve inferring the language from a given
        single-byte code page (PR #112)- Vainly trying to leverage PEP263 when PEP3120
        is not supported (PR #116)- Refactoring for potential performance improvements
        in loops from [@adbar](https://github.com/adbar) (PR #113)- Various detection
        improvement (MD+CD) (PR #117)### Removed- Remove redundant logging entry about
        detected language(s) (PR #115)### Fixed- Fix a minor inconsistency between
        Python 3.5 and other versions regarding language detection (PR #117 #102)##
        [2.0.6](https://github.com/Ousret/charset_normalizer/compare/2.0.5...2.0.6)
        (2021-09-18)### Fixed- Unforeseen regression with the loss of the backward-compatibility
        with some older minor of Python 3.5.x (PR #100)- Fix CLI crash when using
        --minimal output in certain cases (PR #103)### Changed- Minor improvement
        to the detection efficiency (less than 1%) (PR #106 #101)## [2.0.5](https://github.com/Ousret/charset_normalizer/compare/2.0.4...2.0.5)
        (2021-09-14)### Changed- The project now comply with: flake8, mypy, isort
        and black to ensure a better overall quality (PR #81)- The BC-support with
        v1.x was improved, the old staticmethods are restored (PR #82)- The Unicode
        detection is slightly improved (PR #93)- Add syntax sugar \\_\\_bool\\_\\_
        for results CharsetMatches list-container (PR #91)### Removed- The project
        no longer raise warning on tiny content given for detection, will be simply
        logged as warning instead (PR #92)### Fixed- In some rare case, the chunks
        extractor could cut in the middle of a multi-byte character and could mislead
        the mess detection (PR #95)- Some rare 'space' characters could trip up the
        UnprintablePlugin/Mess detection (PR #96)- The MANIFEST.in was not exhaustive
        (PR #78)## [2.0.4](https://github.com/Ousret/charset_normalizer/compare/2.0.3...2.0.4)
        (2021-07-30)### Fixed- The CLI no longer raise an unexpected exception when
        no encoding has been found (PR #70)- Fix accessing the 'alphabets' property
        when the payload contains surrogate characters (PR #68)- The logger could
        mislead (explain=True) on detected languages and the impact of one MBCS match
        (PR #72)- Submatch factoring could be wrong in rare edge cases (PR #72)- Multiple
        files given to the CLI were ignored when publishing results to STDOUT. (After
        the first path) (PR #72)- Fix line endings from CRLF to LF for certain project
        files (PR #67)### Changed- Adjust the MD to lower the sensitivity, thus improving
        the global detection reliability (PR #69 #76)- Allow fallback on specified
        encoding if any (PR #71)## [2.0.3](https://github.com/Ousret/charset_normalizer/compare/2.0.2...2.0.3)
        (2021-07-16)### Changed- Part of the detection mechanism has been improved
        to be less sensitive, resulting in more accurate detection results. Especially
        ASCII. (PR #63)- According to the community wishes, the detection will fall
        back on ASCII or UTF-8 in a last-resort case. (PR #64)## [2.0.2](https://github.com/Ousret/charset_normalizer/compare/2.0.1...2.0.2)
        (2021-07-15)### Fixed- Empty/Too small JSON payload miss-detection fixed.
        Report from [@tseaver](https://github.com/tseaver) (PR #59) ### Changed- Don't
        inject unicodedata2 into sys.modules from [@akx](https://github.com/akx) (PR
        #57)## [2.0.1](https://github.com/Ousret/charset_normalizer/compare/2.0.0...2.0.1)
        (2021-07-13)### Fixed- Make it work where there isn't a filesystem available,
        dropping assets frequencies.json. Report from [@sethmlarson](https://github.com/sethmlarson).
        (PR #55)- Using explain=False permanently disable the verbose output in the
        current runtime (PR #47)- One log entry (language target preemptive) was not
        show in logs when using explain=True (PR #47)- Fix undesired exception (ValueError)
        on getitem of instance CharsetMatches (PR #52)### Changed- Public function
        normalize default args values were not aligned with from_bytes (PR #53)###
        Added- You may now use charset aliases in cp_isolation and cp_exclusion arguments
        (PR #47)## [2.0.0](https://github.com/Ousret/charset_normalizer/compare/1.4.1...2.0.0)
        (2021-07-02)### Changed- 4x to 5 times faster than the previous 1.4.0 release.
        At least 2x faster than Chardet.- Accent has been made on UTF-8 detection,
        should perform rather instantaneous.- The backward compatibility with Chardet
        has been greatly improved. The legacy detect function returns an identical
        charset name whenever possible.- The detection mechanism has been slightly
        improved, now Turkish content is detected correctly (most of the time)- The
        program has been rewritten to ease the readability and maintainability. (+Using
        static typing)+- utf_7 detection has been reinstated.### Removed- This package
        no longer require anything when used with Python 3.5 (Dropped cached_property)-
        Removed support for these languages: Catalan, Esperanto, Kazakh, Baque, Volap\xFCk,
        Azeri, Galician, Nynorsk, Macedonian, and Serbocroatian.- The exception hook
        on UnicodeDecodeError has been removed.### Deprecated- Methods coherence_non_latin,
        w_counter, chaos_secondary_pass of the class CharsetMatch are now deprecated
        and scheduled for removal in v3.0### Fixed- The CLI output used the relative
        path of the file(s). Should be absolute.## [1.4.1](https://github.com/Ousret/charset_normalizer/compare/1.4.0...1.4.1)
        (2021-05-28)### Fixed- Logger configuration/usage no longer conflict with
        others (PR #44)## [1.4.0](https://github.com/Ousret/charset_normalizer/compare/1.3.9...1.4.0)
        (2021-05-21)### Removed- Using standard logging instead of using the package
        loguru.- Dropping nose test framework in favor of the maintained pytest.-
        Choose to not use dragonmapper package to help with gibberish Chinese/CJK
        text.- Require cached_property only for Python 3.5 due to constraint. Dropping
        for every other interpreter version.- Stop support for UTF-7 that does not
        contain a SIG.- Dropping PrettyTable, replaced with pure JSON output in CLI.###
        Fixed- BOM marker in a CharsetNormalizerMatch instance could be False in rare
        cases even if obviously present. Due to the sub-match factoring process.-
        Not searching properly for the BOM when trying utf32/16 parent codec.### Changed-
        Improving the package final size by compressing frequencies.json.- Huge improvement
        over the larges payload.### Added- CLI now produces JSON consumable output.-
        Return ASCII if given sequences fit. Given reasonable confidence.## [1.3.9](https://github.com/Ousret/charset_normalizer/compare/1.3.8...1.3.9)
        (2021-05-13)### Fixed- In some very rare cases, you may end up getting encode/decode
        errors due to a bad bytes payload (PR #40)## [1.3.8](https://github.com/Ousret/charset_normalizer/compare/1.3.7...1.3.8)
        (2021-05-12)### Fixed- Empty given payload for detection may cause an exception
        if trying to access the `alphabets` property. (PR #39)## [1.3.7](https://github.com/Ousret/charset_normalizer/compare/1.3.6...1.3.7)
        (2021-05-12)### Fixed- The legacy detect function should return UTF-8-SIG
        if sig is present in the payload. (PR #38)## [1.3.6](https://github.com/Ousret/charset_normalizer/compare/1.3.5...1.3.6)
        (2021-02-09)### Changed- Amend the previous release to allow prettytable 2.0
        (PR #35)## [1.3.5](https://github.com/Ousret/charset_normalizer/compare/1.3.4...1.3.5)
        (2021-02-08)### Fixed- Fix error while using the package with a python pre-release
        interpreter (PR #33)### Changed- Dependencies refactoring, constraints revised.###
        Added- Add python 3.9 and 3.10 to the supported interpretersMIT LicenseCopyright
        (c) 2019 TAHRI Ahmed R.Permission is hereby granted, free of charge, to any
        person obtaining a copyof this software and associated documentation files
        (the \"Software\"), to dealin the Software without restriction, including
        without limitation the rightsto use, copy, modify, merge, publish, distribute,
        sublicense, and/or sellcopies of the Software, and to permit persons to whom
        the Software isfurnished to do so, subject to the following conditions:The
        above copyright notice and this permission notice shall be included in allcopies
        or substantial portions of the Software.THE SOFTWARE IS PROVIDED \"AS IS\",
        WITHOUT WARRANTY OF ANY KIND, EXPRESS ORIMPLIED, INCLUDING BUT NOT LIMITED
        TO THE WARRANTIES OF MERCHANTABILITY,FITNESS FOR A PARTICULAR PURPOSE AND
        NONINFRINGEMENT. IN NO EVENT SHALL THEAUTHORS OR COPYRIGHT HOLDERS BE LIABLE
        FOR ANY CLAIM, DAMAGES OR OTHERLIABILITY, WHETHER IN AN ACTION OF CONTRACT,
        TORT OR OTHERWISE, ARISING FROM,OUT OF OR IN CONNECTION WITH THE SOFTWARE
        OR THE USE OR OTHER DEALINGS IN THESOFTWARE."
      Package: charset-normalizer
      Source: pip
      Version: '3.3.2'
      Hash: ''
      licenses:
      - MIT
      Title: charset-normalizer
      DownloadURL: https://files.pythonhosted.org/packages/63/09/c1bc53dab74b1816a00d8d030de5bf98f724c52c1635e07681d312f20be8/charset-normalizer-3.3.2.tar.gz
  bazaar:
    register: 'no'
    prim: 21/CTX1032640
    community_link: https://github.com/Ousret/charset_normalizer
    community_name: https://github.com/Ousret/charset_normalizer
    community_url: https://github.com/Ousret/charset_normalizer
    component_comment: ''
    component_highlevel_description: ''
    component_name: charset-normalizer
    component_platform: linux
    component_programing_language: ''
    component_version: '3.3.2'
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://github.com/Ousret/charset_normalizer/archive/3.3.2.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1068085&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: France
    crypto: ''
    programming_language: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: charset-normalizer
    target_sw: linux
    vendor: pip
    version: '3.3.2'
    web_url: https://github.com/Ousret/charset_normalizer
  licenses:
  - MIT
  name: charset-normalizer
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '3.3.2'
  mimer:
    linking: Static
    product_number: CTX1032640
    product_version_label: '3.3.2'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: colorama+0.4.6
  additional_info:
    fossa-attribution:
      Description: ".. image:: https://img.shields.io/pypi/v/colorama.svg    :target:
        https://pypi.org/project/colorama/    :alt: Latest Version.. image:: https://img.shields.io/pypi/pyversions/colorama.svg
        \   :target: https://pypi.org/project/colorama/    :alt: Supported Python
        versions.. image:: https://github.com/tartley/colorama/actions/workflows/test.yml/badge.svg
        \   :target: https://github.com/tartley/colorama/actions/workflows/test.yml
        \   :alt: Build StatusColorama========Makes ANSI escape character sequences
        (for producing colored terminal text andcursor positioning) work under MS
        Windows... |donate| image:: https://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif
        \ :target: https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=2MZ9D2GMLYCUJ&item_name=Colorama&currency_code=USD
        \ :alt: Donate with Paypal`PyPI for releases <https://pypi.org/project/colorama/>`_
        |`Github for source <https://github.com/tartley/colorama>`_ |`Colorama for
        enterprise on Tidelift <https://github.com/tartley/colorama/blob/master/ENTERPRISE.md>`_If
        you find Colorama useful, please |donate| to the authors. Thank you!Installation------------Tested
        on CPython 2.7, 3.7, 3.8, 3.9 and 3.10 and Pypy 2.7 and 3.8.No requirements
        other than the standard library... code-block:: bash    pip install colorama
        \   # or    conda install -c anaconda coloramaDescription-----------ANSI escape
        character sequences have long been used to produce colored terminaltext and
        cursor positioning on Unix and Macs. Colorama makes this work onWindows, too,
        by wrapping ``stdout``, stripping ANSI sequences it finds (whichwould appear
        as gobbledygook in the output), and converting them into theappropriate win32
        calls to modify the state of the terminal. On other platforms,Colorama does
        nothing.This has the upshot of providing a simple cross-platform API for printingcolored
        terminal text from Python, and has the happy side-effect that existingapplications
        or libraries which use ANSI sequences to produce colored output onLinux or
        Macs can now also work on Windows, simply by calling``colorama.just_fix_windows_console()``
        (since v0.4.6) or ``colorama.init()``(all versions, but may have other side-effects
        \u2013 see below).An alternative approach is to install ``ansi.sys`` on Windows
        machines, whichprovides the same behaviour for all applications running in
        terminals. Coloramais intended for situations where that isn't easy (e.g.,
        maybe your app doesn'thave an installer.)Demo scripts in the source code repository
        print some colored text usingANSI sequences. Compare their output under Gnome-terminal's
        built in ANSIhandling, versus on Windows Command-Prompt using Colorama:..
        image:: https://github.com/tartley/colorama/raw/master/screenshots/ubuntu-demo.png
        \   :width: 661    :height: 357    :alt: ANSI sequences on Ubuntu under gnome-terminal...
        image:: https://github.com/tartley/colorama/raw/master/screenshots/windows-demo.png
        \   :width: 668    :height: 325    :alt: Same ANSI sequences on Windows, using
        Colorama.These screenshots show that, on Windows, Colorama does not support
        ANSI 'dimtext'; it looks the same as 'normal text'.Usage-----Initialisation..............If
        the only thing you want from Colorama is to get ANSI escapes to work onWindows,
        then run:.. code-block:: python    from colorama import just_fix_windows_console
        \   just_fix_windows_console()If you're on a recent version of Windows 10
        or better, and your stdout/stderrare pointing to a Windows console, then this
        will flip the magic configurationswitch to enable Windows' built-in ANSI support.If
        you're on an older version of Windows, and your stdout/stderr are pointing
        toa Windows console, then this will wrap ``sys.stdout`` and/or ``sys.stderr``
        in amagic file object that intercepts ANSI escape sequences and issues theappropriate
        Win32 calls to emulate them.In all other circumstances, it does nothing whatsoever.
        Basically the idea isthat this makes Windows act like Unix with respect to
        ANSI escape handling.It's safe to call this function multiple times. It's
        safe to call this functionon non-Windows platforms, but it won't do anything.
        It's safe to call thisfunction when one or both of your stdout/stderr are
        redirected to a file \u2013 itwon't do anything to those streams.Alternatively,
        you can use the older interface with more features (but also morepotential
        footguns):.. code-block:: python    from colorama import init    init()This
        does the same thing as ``just_fix_windows_console``, except for thefollowing
        differences:- It's not safe to call ``init`` multiple times; you can end up
        with multiple  layers of wrapping and broken ANSI support.- Colorama will
        apply a heuristic to guess whether stdout/stderr support ANSI,  and if it
        thinks they don't, then it will wrap ``sys.stdout`` and  ``sys.stderr`` in
        a magic file object that strips out ANSI escape sequences  before printing
        them. This happens on all platforms, and can be convenient if  you want to
        write your code to emit ANSI escape sequences unconditionally, and  let Colorama
        decide whether they should actually be output. But note that  Colorama's heuristic
        is not particularly clever.- ``init`` also accepts explicit keyword args to
        enable/disable various  functionality \u2013 see below.To stop using Colorama
        before your program exits, simply call ``deinit()``.This will restore ``stdout``
        and ``stderr`` to their original values, so thatColorama is disabled. To resume
        using Colorama again, call ``reinit()``; it ischeaper than calling ``init()``
        again (but does the same thing).Most users should depend on ``colorama >=
        0.4.6``, and use``just_fix_windows_console``. The old ``init`` interface will
        be supportedindefinitely for backwards compatibility, but we don't plan to
        fix any issueswith it, also for backwards compatibility.Colored Output..............Cross-platform
        printing of colored text can then be done using Colorama'sconstant shorthand
        for ANSI escape sequences. These are deliberatelyrudimentary, see below...
        code-block:: python    from colorama import Fore, Back, Style    print(Fore.RED
        + 'some red text')    print(Back.GREEN + 'and with a green background')    print(Style.DIM
        + 'and in dim text')    print(Style.RESET_ALL)    print('back to normal now')...or
        simply by manually printing ANSI sequences from your own code:.. code-block::
        python    print('\\033[31m' + 'some red text')    print('\\033[39m') # and
        reset to default color...or, Colorama can be used in conjunction with existing
        ANSI librariessuch as the venerable `Termcolor <https://pypi.org/project/termcolor/>`_the
        fabulous `Blessings <https://pypi.org/project/blessings/>`_,or the incredible
        `_Rich <https://pypi.org/project/rich/>`_.If you wish Colorama's Fore, Back
        and Style constants were more capable,then consider using one of the above
        highly capable libraries to generatecolors, etc, and use Colorama just for
        its primary purpose: to convertthose ANSI sequences to also work on Windows:SIMILARLY,
        do not send PRs adding the generation of new ANSI types to Colorama.We are
        only interested in converting ANSI codes to win32 API calls, notshortcuts
        like the above to generate ANSI characters... code-block:: python    from
        colorama import just_fix_windows_console    from termcolor import colored
        \   # use Colorama to make Termcolor work on Windows too    just_fix_windows_console()
        \   # then use Termcolor for all colored text output    print(colored('Hello,
        World!', 'green', 'on_red'))Available formatting constants are::    Fore:
        BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET.    Back: BLACK,
        RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET.    Style: DIM, NORMAL,
        BRIGHT, RESET_ALL``Style.RESET_ALL`` resets foreground, background, and brightness.
        Colorama willperform this reset automatically on program exit.These are fairly
        well supported, but not part of the standard::    Fore: LIGHTBLACK_EX, LIGHTRED_EX,
        LIGHTGREEN_EX, LIGHTYELLOW_EX, LIGHTBLUE_EX, LIGHTMAGENTA_EX, LIGHTCYAN_EX,
        LIGHTWHITE_EX    Back: LIGHTBLACK_EX, LIGHTRED_EX, LIGHTGREEN_EX, LIGHTYELLOW_EX,
        LIGHTBLUE_EX, LIGHTMAGENTA_EX, LIGHTCYAN_EX, LIGHTWHITE_EXCursor Positioning..................ANSI
        codes to reposition the cursor are supported. See ``demos/demo06.py`` foran
        example of how to generate them.Init Keyword Args.................``init()``
        accepts some ``**kwargs`` to override default behaviour.init(autoreset=False):
        \   If you find yourself repeatedly sending reset sequences to turn off color
        \   changes at the end of every print, then ``init(autoreset=True)`` will
        \   automate that:    .. code-block:: python        from colorama import init
        \       init(autoreset=True)        print(Fore.RED + 'some red text')        print('automatically
        back to default color again')init(strip=None):    Pass ``True`` or ``False``
        to override whether ANSI codes should be    stripped from the output. The
        default behaviour is to strip if on Windows    or if output is redirected
        (not a tty).init(convert=None):    Pass ``True`` or ``False`` to override
        whether to convert ANSI codes in the    output into win32 calls. The default
        behaviour is to convert if on Windows    and output is to a tty (terminal).init(wrap=True):
        \   On Windows, Colorama works by replacing ``sys.stdout`` and ``sys.stderr``
        \   with proxy objects, which override the ``.write()`` method to do their
        work.    If this wrapping causes you problems, then this can be disabled by
        passing    ``init(wrap=False)``. The default behaviour is to wrap if ``autoreset``
        or    ``strip`` or ``convert`` are True.    When wrapping is disabled, colored
        printing on non-Windows platforms will    continue to work as normal. To do
        cross-platform colored output, you can    use Colorama's ``AnsiToWin32`` proxy
        directly:    .. code-block:: python        import sys        from colorama
        import init, AnsiToWin32        init(wrap=False)        stream = AnsiToWin32(sys.stderr).stream
        \       # Python 2        print >>stream, Fore.BLUE + 'blue text on stderr'
        \       # Python 3        print(Fore.BLUE + 'blue text on stderr', file=stream)Recognised
        ANSI Sequences.........................ANSI sequences generally take the form::
        \   ESC [ <param> ; <param> ... <command>Where ``<param>`` is an integer,
        and ``<command>`` is a single letter. Zero ormore params are passed to a ``<command>``.
        If no params are passed, it isgenerally synonymous with passing a single zero.
        No spaces exist in thesequence; they have been inserted here simply to read
        more easily.The only ANSI sequences that Colorama converts into win32 calls
        are::    ESC [ 0 m       # reset all (colors and brightness)    ESC [ 1 m
        \      # bright    ESC [ 2 m       # dim (looks same as normal brightness)
        \   ESC [ 22 m      # normal brightness    # FOREGROUND:    ESC [ 30 m      #
        black    ESC [ 31 m      # red    ESC [ 32 m      # green    ESC [ 33 m      #
        yellow    ESC [ 34 m      # blue    ESC [ 35 m      # magenta    ESC [ 36
        m      # cyan    ESC [ 37 m      # white    ESC [ 39 m      # reset    # BACKGROUND
        \   ESC [ 40 m      # black    ESC [ 41 m      # red    ESC [ 42 m      #
        green    ESC [ 43 m      # yellow    ESC [ 44 m      # blue    ESC [ 45 m
        \     # magenta    ESC [ 46 m      # cyan    ESC [ 47 m      # white    ESC
        [ 49 m      # reset    # cursor positioning    ESC [ y;x H     # position
        cursor at x across, y down    ESC [ y;x f     # position cursor at x across,
        y down    ESC [ n A       # move cursor n lines up    ESC [ n B       # move
        cursor n lines down    ESC [ n C       # move cursor n characters forward
        \   ESC [ n D       # move cursor n characters backward    # clear the screen
        \   ESC [ mode J    # clear the screen    # clear the line    ESC [ mode K
        \   # clear the lineMultiple numeric params to the ``'m'`` command can be
        combined into a singlesequence::    ESC [ 36 ; 45 ; 1 m     # bright cyan
        text on magenta backgroundAll other ANSI sequences of the form ``ESC [ <param>
        ; <param> ... <command>``are silently stripped from the output on Windows.Any
        other form of ANSI sequence, such as single-character codes or alternativeinitial
        characters, are not recognised or stripped. It would be cool to addthem though.
        Let me know if it would be useful for you, via the Issues onGitHub.Status
        & Known Problems-----------------------I've personally only tested it on Windows
        XP (CMD, Console2), Ubuntu(gnome-terminal, xterm), and OS X.Some valid ANSI
        sequences aren't recognised.If you're hacking on the code, see `README-hacking.md`_.
        ESPECIALLY, see theexplanation there of why we do not want PRs that allow
        Colorama to generate newtypes of ANSI codes.See outstanding issues and wish-list:https://github.com/tartley/colorama/issuesIf
        anything doesn't work for you, or doesn't do what you expected or hoped for,I'd
        love to hear about it on that issues list, would be delighted by patches,and
        would be happy to grant commit access to anyone who submits a working patchor
        two... _README-hacking.md: README-hacking.mdLicense-------Copyright Jonathan
        Hartley & Arnon Yaari, 2013-2020. BSD 3-Clause license; seeLICENSE file.Professional
        support--------------------.. |tideliftlogo| image:: https://cdn2.hubspot.net/hubfs/4008838/website/logos/logos_for_download/Tidelift_primary-shorthand-logo.png
        \  :alt: Tidelift   :target: https://tidelift.com/subscription/pkg/pypi-colorama?utm_source=pypi-colorama&utm_medium=referral&utm_campaign=readme..
        list-table::   :widths: 10 100   * - |tideliftlogo|     - Professional support
        for colorama is available as part of the       `Tidelift Subscription`_.       Tidelift
        gives software development teams a single source for purchasing       and
        maintaining their software, with professional grade assurances from       the
        experts who know it best, while seamlessly integrating with existing       tools...
        _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-colorama?utm_source=pypi-colorama&utm_medium=referral&utm_campaign=readmeThanks------See
        the CHANGELOG for more thanks!* Marc Schlaich (schlamar) for a ``setup.py``
        fix for Python2.5.* Marc Abramowitz, reported & fixed a crash on exit with
        closed ``stdout``,  providing a solution to issue #7's setuptools/distutils
        debate,  and other fixes.* User 'eryksun', for guidance on correctly instantiating
        ``ctypes.windll``.* Matthew McCormick for politely pointing out a longstanding
        crash on non-Win.* Ben Hoyt, for a magnificent fix under 64-bit Windows.*
        Jesse at Empty Square for submitting a fix for examples in the README.* User
        'jamessp', an observant documentation fix for cursor positioning.* User 'vaal1239',
        Dave Mckee & Lackner Kristof for a tiny but much-needed Win7  fix.* Julien
        Stuyck, for wisely suggesting Python3 compatible updates to README.* Daniel
        Griffith for multiple fabulous patches.* Oscar Lesta for a valuable fix to
        stop ANSI chars being sent to non-tty  output.* Roger Binns, for many suggestions,
        valuable feedback, & bug reports.* Tim Golden for thought and much appreciated
        feedback on the initial idea.* User 'Zearin' for updates to the README file.*
        John Szakmeister for adding support for light colors* Charles Merriam for
        adding documentation to demos* Jurko for a fix on 64-bit Windows CPython2.5
        w/o ctypes* Florian Bruhin for a fix when stdout or stderr are None* Thomas
        Weininger for fixing ValueError on Windows* Remi Rampin for better Github
        integration and fixes to the README file* Simeon Visser for closing a file
        handle using 'with' and updating classifiers  to include Python 3.3 and 3.4*
        Andy Neff for fixing RESET of LIGHT_EX colors.* Jonathan Hartley for the initial
        idea and implementation."
      Package: colorama
      Source: pip
      Version: '0.4.6'
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: colorama
      DownloadURL: https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz
  bazaar:
    register: 'no'
    prim: 8/CAX1059078
    community_link: https://pypi.org/project/colorama
    community_name: https://pypi.org/project/colorama
    community_url: https://pypi.org/project/colorama
    component_comment: ''
    component_highlevel_description: Simple cross-platform colored terminal text in
      Python
    component_name: colorama
    component_platform: linux
    component_programing_language: Python
    component_version: '0.4.6'
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1027051&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: colorama
    target_sw: linux
    vendor: pip
    version: '0.4.6'
    web_url: https://pypi.org/project/colorama/0.4.6/
  licenses:
  - BSD-3-Clause
  name: colorama
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '0.4.6'
  mimer:
    linking: Static
    product_number: CAX1059078
    product_version_label: '0.4.6'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: coloredlogs+15.0.1
  additional_info:
    fossa-attribution:
      Description: "coloredlogs: Colored terminal output for Python's logging module================================================================..
        image:: https://travis-ci.org/xolox/python-coloredlogs.svg?branch=master   :target:
        https://travis-ci.org/xolox/python-coloredlogs.. image:: https://coveralls.io/repos/github/xolox/python-coloredlogs/badge.svg?branch=master
        \  :target: https://coveralls.io/github/xolox/python-coloredlogs?branch=masterThe
        `coloredlogs` package enables colored terminal output for Python's logging_module.
        The ColoredFormatter_ class inherits from `logging.Formatter`_ and uses`ANSI
        escape sequences`_ to render your logging messages in color. It uses onlystandard
        colors so it should work on any UNIX terminal. It's currently testedon Python
        2.7, 3.5+ and PyPy (2 and 3). On Windows `coloredlogs` automaticallytries
        to enable native ANSI support (on up-to-date Windows 10 installations)and
        falls back on using colorama_ (if installed). Here is a screen shot of thedemo
        that is printed when the command ``coloredlogs --demo`` is executed:.. image::
        https://coloredlogs.readthedocs.io/en/latest/_images/defaults.pngNote that
        the screenshot above includes custom logging levels defined by myverboselogs_
        package: if you install both `coloredlogs` and `verboselogs` itwill Just Work
        (`verboselogs` is of course not required to use`coloredlogs`)... contents::
        \  :local:Installation------------The `coloredlogs` package is available on
        PyPI_ which means installation shouldbe as simple as:.. code-block:: console
        \  $ pip install coloredlogsThere's actually a multitude of ways to install
        Python packages (e.g. the `peruser site-packages directory`_, `virtual environments`_
        or just installingsystem wide) and I have no intention of getting into that
        discussion here, soif this intimidates you then read up on your options before
        returning to theseinstructions \U0001F609.Optional dependencies~~~~~~~~~~~~~~~~~~~~~Native
        ANSI support on Windows requires an up-to-date Windows 10 installation.If
        this is not working for you then consider installing the colorama_ package:..
        code-block:: console   $ pip install coloramaOnce colorama_ is installed it
        will be used automatically.Usage-----Here's an example of how easy it is to
        get started:.. code-block:: python   import coloredlogs, logging   # Create
        a logger object.   logger = logging.getLogger(__name__)   # By default the
        install() function installs a handler on the root logger,   # this means that
        log messages from your code and log messages from the   # libraries that you
        use will all show up on the terminal.   coloredlogs.install(level='DEBUG')
        \  # If you don't want to see log messages from libraries, you can pass a
        \  # specific logger object to the install() function. In this case only log
        \  # messages originating from that logger will show up on the terminal.   coloredlogs.install(level='DEBUG',
        logger=logger)   # Some examples.   logger.debug(\"this is a debugging message\")
        \  logger.info(\"this is an informational message\")   logger.warning(\"this
        is a warning message\")   logger.error(\"this is an error message\")   logger.critical(\"this
        is a critical message\")Format of log messages----------------------The ColoredFormatter_
        class supports user defined log formats so you can useany log format you like.
        The default log format is as follows:: %(asctime)s %(hostname)s %(name)s[%(process)d]
        %(levelname)s %(message)sThis log format results in the following output::
        2015-10-23 03:32:22 peter-macbook coloredlogs.demo[30462] DEBUG message with
        level 'debug' 2015-10-23 03:32:23 peter-macbook coloredlogs.demo[30462] VERBOSE
        message with level 'verbose' 2015-10-23 03:32:24 peter-macbook coloredlogs.demo[30462]
        INFO message with level 'info' ...You can customize the log format and styling
        using environment variables aswell as programmatically, please refer to the
        `online documentation`_ fordetails.Enabling millisecond precision~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~If
        you're switching from `logging.basicConfig()`_ to `coloredlogs.install()`_you
        may notice that timestamps no longer include milliseconds. This is becausecoloredlogs
        doesn't output milliseconds in timestamps unless you explicitlytell it to.
        There are three ways to do that:1. The easy way is to pass the `milliseconds`
        argument to `coloredlogs.install()`_::    coloredlogs.install(milliseconds=True)
        \  This became supported in `release 7.1`_ (due to `#16`_).2. Alternatively
        you can change the log format `to include 'msecs'`_::    %(asctime)s,%(msecs)03d
        %(hostname)s %(name)s[%(process)d] %(levelname)s %(message)s   Here's what
        the call to `coloredlogs.install()`_ would then look like::    coloredlogs.install(fmt='%(asctime)s,%(msecs)03d
        %(hostname)s %(name)s[%(process)d] %(levelname)s %(message)s')   Customizing
        the log format also enables you to change the delimiter that   separates seconds
        from milliseconds (the comma above). This became possible   in `release 3.0`_
        which added support for user defined log formats.3. If the use of ``%(msecs)d``
        isn't flexible enough you can instead add ``%f``   to the date/time format,
        it will be replaced by the value of ``%(msecs)03d``.   Support for the ``%f``
        directive was added to `release 9.3`_ (due to `#45`_).Custom logging fields~~~~~~~~~~~~~~~~~~~~~The
        following custom log format fields are supported:- ``%(hostname)s`` provides
        the hostname of the local system.- ``%(programname)s`` provides the name of
        the currently running program.- ``%(username)s`` provides the username of
        the currently logged in user.When `coloredlogs.install()`_ detects that any
        of these fields are used in theformat string the applicable logging.Filter_
        subclasses are automaticallyregistered to populate the relevant log record
        fields.Changing text styles and colors-------------------------------The online
        documentation contains `an example of customizing the text styles andcolors
        <https://coloredlogs.readthedocs.io/en/latest/api.html#changing-the-colors-styles>`_.Colored
        output from cron------------------------When `coloredlogs` is used in a cron_
        job, the output that's e-mailed to you bycron won't contain any ANSI escape
        sequences because `coloredlogs` realizesthat it's not attached to an interactive
        terminal. If you'd like to have colorse-mailed to you by cron there are two
        ways to make it happen:.. contents::   :local:Modifying your crontab~~~~~~~~~~~~~~~~~~~~~~Here's
        an example of a minimal crontab::    MAILTO=\"your-email-address@here\"    CONTENT_TYPE=\"text/html\"
        \   * * * * * root coloredlogs --to-html your-commandThe ``coloredlogs`` program
        is installed when you install the `coloredlogs`Python package. When you execute
        ``coloredlogs --to-html your-command`` it runs``your-command`` under the external
        program ``script`` (you need to have thisinstalled). This makes ``your-command``
        think that it's attached to aninteractive terminal which means it will output
        ANSI escape sequences whichwill then be converted to HTML by the ``coloredlogs``
        program. Yes, this is abit convoluted, but it works great :-)Modifying your
        Python code~~~~~~~~~~~~~~~~~~~~~~~~~~The ColoredCronMailer_ class provides
        a context manager that automaticallyenables HTML output when the ``$CONTENT_TYPE``
        variable has been correctly setin the crontab.This requires my capturer_ package
        which you can install using ``pip install'coloredlogs[cron]'``. The ``[cron]``
        extra will pull in capturer_ 2.4 or newerwhich is required to capture the
        output while silencing it - otherwise you'dget duplicate output in the emails
        sent by ``cron``.The context manager can also be used to retroactively silence
        output that hasalready been produced, this can be useful to avoid spammy cron
        jobs that havenothing useful to do but still email their output to the system
        administratorevery few minutes :-).Contact-------The latest version of `coloredlogs`
        is available on PyPI_ and GitHub_. The`online documentation`_ is available
        on Read The Docs and includes achangelog_. For bug reports please create an
        issue on GitHub_. If you havequestions, suggestions, etc. feel free to send
        me an e-mail at`peter@peterodding.com`_.License-------This software is licensed
        under the `MIT license`_.\xA9 2020 Peter Odding... External references:..
        _#16: https://github.com/xolox/python-coloredlogs/issues/16.. _#45: https://github.com/xolox/python-coloredlogs/issues/45..
        _ANSI escape sequences: https://en.wikipedia.org/wiki/ANSI_escape_code#Colors..
        _capturer: https://pypi.org/project/capturer.. _changelog: https://coloredlogs.readthedocs.org/en/latest/changelog.html..
        _colorama: https://pypi.org/project/colorama.. _ColoredCronMailer: https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.converter.ColoredCronMailer..
        _ColoredFormatter: https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.ColoredFormatter..
        _coloredlogs.install(): https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.install..
        _cron: https://en.wikipedia.org/wiki/Cron.. _GitHub: https://github.com/xolox/python-coloredlogs..
        _logging.basicConfig(): https://docs.python.org/2/library/logging.html#logging.basicConfig..
        _logging.Filter: https://docs.python.org/3/library/logging.html#filter-objects..
        _logging.Formatter: https://docs.python.org/2/library/logging.html#logging.Formatter..
        _logging: https://docs.python.org/2/library/logging.html.. _MIT license: https://en.wikipedia.org/wiki/MIT_License..
        _online documentation: https://coloredlogs.readthedocs.io/.. _per user site-packages
        directory: https://www.python.org/dev/peps/pep-0370/.. _peter@peterodding.com:
        peter@peterodding.com.. _PyPI: https://pypi.org/project/coloredlogs.. _release
        3.0: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-3-0-2015-10-23..
        _release 7.1: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-7-1-2017-07-15..
        _release 9.3: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-9-3-2018-04-29..
        _to include 'msecs': https://stackoverflow.com/questions/6290739/python-logging-use-milliseconds-in-time-format..
        _verboselogs: https://pypi.org/project/verboselogs.. _virtual environments:
        http://docs.python-guide.org/en/latest/dev/virtualenvs/"
      Package: coloredlogs
      Source: pip
      Version: '15.0.1'
      Hash: ''
      licenses:
      - MIT
      Title: coloredlogs
      DownloadURL: https://files.pythonhosted.org/packages/cc/c7/eed8f27100517e8c0e6b923d5f0845d0cb99763da6fdee00478f91db7325/coloredlogs-15.0.1.tar.gz
  bazaar:
    register: 'no'
    prim: 1/CTX1036515
    community_link: https://pypi.python.org/pypi/coloredlogs
    community_name: https://pypi.python.org/pypi/coloredlogs
    community_url: https://pypi.python.org/pypi/coloredlogs
    component_comment: ''
    component_highlevel_description: Colored terminal output for Python's logging module
    component_name: coloredlogs
    component_platform: linux
    component_programing_language: ''
    component_version: '15.0.1'
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://files.pythonhosted.org/packages/cc/c7/eed8f27100517e8c0e6b923d5f0845d0cb99763da6fdee00478f91db7325/coloredlogs-15.0.1.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1026456&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Netherlands
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: coloredlogs
    target_sw: linux
    vendor: pip
    version: '15.0.1'
    web_url: https://coloredlogs.readthedocs.io
  licenses:
  - MIT
  name: coloredlogs
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '15.0.1'
  mimer:
    linking: Static
    product_number: CTX1036515
    product_version_label: '15.0.1'
    selected_licenses:
    - MIT
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: datasets+2.14.4
  additional_info:
    fossa-attribution:
      Description: "<p align=\"center\">  <picture>    <source media=\"(prefers-color-scheme:
        dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg\">
        \   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg\">
        \   <img alt=\"Hugging Face Datasets Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg\"
        width=\"352\" height=\"59\" style=\"max-width: 100%;\">  </picture>  <br/>
        \ <br/></p><p align=\"center\">    <a href=\"https://github.com/huggingface/datasets/actions/workflows/ci.yml?query=branch%3Amain\">
        \       <img alt=\"Build\" src=\"https://github.com/huggingface/datasets/actions/workflows/ci.yml/badge.svg?branch=main\">
        \   </a>    <a href=\"https://github.com/huggingface/datasets/blob/main/LICENSE\">
        \       <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/datasets.svg?color=blue\">
        \   </a>    <a href=\"https://huggingface.co/docs/datasets/index.html\">        <img
        alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/datasets/index.html.svg?down_color=red&down_message=offline&up_message=online\">
        \   </a>    <a href=\"https://github.com/huggingface/datasets/releases\">
        \       <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/datasets.svg\">
        \   </a>    <a href=\"https://huggingface.co/datasets/\">        <img alt=\"Number
        of datasets\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen\">
        \   </a>    <a href=\"CODE_OF_CONDUCT.md\">        <img alt=\"Contributor
        Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg\">
        \   </a>    <a href=\"https://zenodo.org/badge/latestdoi/250213286\"><img
        src=\"https://zenodo.org/badge/250213286.svg\" alt=\"DOI\"></a></p>\U0001F917
        Datasets is a lightweight library providing **two** main features:- **one-line
        dataloaders for many public datasets**: one-liners to download and pre-process
        any of the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen)
        major public datasets (image datasets, audio datasets, text datasets in 467
        languages and dialects, etc.) provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets).
        With a simple command like `squad_dataset = load_dataset(\"squad\")`, get
        any of these datasets ready to use in a dataloader for training/evaluating
        a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX),- **efficient data pre-processing**:
        simple, fast and reproducible data pre-processing for the public datasets
        as well as your own local datasets in CSV, JSON, text, PNG, JPEG, WAV, MP3,
        Parquet, etc. With simple commands like `processed_dataset = dataset.map(process_example)`,
        efficiently prepare the dataset for inspection and ML model evaluation and
        training.[\U0001F393 **Documentation**](https://huggingface.co/docs/datasets/)
        [\U0001F50E **Find a dataset in the Hub**](https://huggingface.co/datasets)
        [\U0001F31F **Share a dataset on the Hub**](https://huggingface.co/docs/datasets/share)<h3
        align=\"center\">    <a href=\"https://hf.co/course\"><img src=\"https://raw.githubusercontent.com/huggingface/datasets/main/docs/source/imgs/course_banner.png\"></a></h3>\U0001F917
        Datasets is designed to let the community easily add and share new datasets.\U0001F917
        Datasets has many additional interesting features:- Thrive on large datasets:
        \U0001F917 Datasets naturally frees the user from RAM memory limitation, all
        datasets are memory-mapped using an efficient zero-serialization cost backend
        (Apache Arrow).- Smart caching: never wait for your data to process several
        times.- Lightweight and fast with a transparent and pythonic API (multi-processing/caching/memory-mapping).-
        Built-in interoperability with NumPy, pandas, PyTorch, TensorFlow 2 and JAX.-
        Native support for audio and image data.- Enable streaming mode to save disk
        space and start iterating over the dataset immediately.\U0001F917 Datasets
        originated from a fork of the awesome [TensorFlow Datasets](https://github.com/tensorflow/datasets)
        and the HuggingFace team want to deeply thank the TensorFlow Datasets team
        for building this amazing library. More details on the differences between
        \U0001F917 Datasets and `tfds` can be found in the section [Main differences
        between \U0001F917 Datasets and `tfds`](#main-differences-between--datasets-and-tfds).#
        Installation## With pip\U0001F917 Datasets can be installed from PyPi and
        has to be installed in a virtual environment (venv or conda for instance)```bashpip
        install datasets```## With conda\U0001F917 Datasets can be installed using
        conda as follows:```bashconda install -c huggingface -c conda-forge datasets```Follow
        the installation pages of TensorFlow and PyTorch to see how to install them
        with conda.For more details on installation, check the installation page in
        the documentation: https://huggingface.co/docs/datasets/installation## Installation
        to use with PyTorch/TensorFlow/pandasIf you plan to use \U0001F917 Datasets
        with PyTorch (1.0+), TensorFlow (2.2+) or pandas, you should also install
        PyTorch, TensorFlow or pandas.For more details on using the library with NumPy,
        pandas, PyTorch or TensorFlow, check the quick start page in the documentation:
        https://huggingface.co/docs/datasets/quickstart# Usage\U0001F917 Datasets
        is made to be very simple to use - the API is centered around a single function,
        `datasets.load_dataset(dataset_name, **kwargs)`, that instantiates a dataset.This
        library can be used for text/image/audio/etc. datasets. Here is an example
        to load a text dataset:Here is a quick example:```pythonfrom datasets import
        load_dataset# Print all the available datasetsfrom huggingface_hub import
        list_datasetsprint([dataset.id for dataset in list_datasets()])# Load a dataset
        and print the first example in the training setsquad_dataset = load_dataset('squad')print(squad_dataset['train'][0])#
        Process the dataset - add a column with the length of the context textsdataset_with_length
        = squad_dataset.map(lambda x: {\"length\": len(x[\"context\"])})# Process
        the dataset - tokenize the context texts (using a tokenizer from the \U0001F917
        Transformers library)from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained('bert-base-cased')tokenized_dataset
        = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)```If
        your dataset is bigger than your disk or if you don't want to wait to download
        the data, you can use streaming:```python# If you want to use the dataset
        immediately and efficiently stream the data as you iterate over the datasetimage_dataset
        = load_dataset('cifar100', streaming=True)for example in image_dataset[\"train\"]:
        \   break```For more details on using the library, check the quick start page
        in the documentation: https://huggingface.co/docs/datasets/quickstart and
        the specific pages on:- Loading a dataset: https://huggingface.co/docs/datasets/loading-
        What's in a Dataset: https://huggingface.co/docs/datasets/access- Processing
        data with \U0001F917 Datasets: https://huggingface.co/docs/datasets/process
        \   - Processing audio data: https://huggingface.co/docs/datasets/audio_process
        \   - Processing image data: https://huggingface.co/docs/datasets/image_process
        \   - Processing text data: https://huggingface.co/docs/datasets/nlp_process-
        Streaming a dataset: https://huggingface.co/docs/datasets/stream- Writing
        your own dataset loading script: https://huggingface.co/docs/datasets/dataset_script-
        etc.# Add a new dataset to the HubWe have a very detailed step-by-step guide
        to add a new dataset to the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen)
        datasets already provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets).You
        can find:- [how to upload a dataset to the Hub using your web browser or Python](https://huggingface.co/docs/datasets/upload_dataset)
        and also- [how to upload it using Git](https://huggingface.co/docs/datasets/share).#
        Main differences between \U0001F917 Datasets and `tfds`If you are familiar
        with the great TensorFlow Datasets, here are the main differences between
        \U0001F917 Datasets and `tfds`:- the scripts in \U0001F917 Datasets are not
        provided within the library but are queried, downloaded/cached and dynamically
        loaded upon request- the backend serialization of \U0001F917 Datasets is based
        on [Apache Arrow](https://arrow.apache.org/) instead of TF Records and leverage
        python dataclasses for info and features with some diverging features (we
        mostly don't do encoding and store the raw data as much as possible in the
        backend serialization cache).- the user-facing dataset object of \U0001F917
        Datasets is not a `tf.data.Dataset` but a built-in framework-agnostic dataset
        class with methods inspired by what we like in `tf.data` (like a `map()` method).
        It basically wraps a memory-mapped Arrow table cache.# Disclaimers\U0001F917
        Datasets may run Python code defined by the dataset authors to parse certain
        data formats or structures. For security reasons, we ask users to:- check
        the dataset scripts they're going to run beforehand and- pin the `revision`
        of the repositories they use.If you're a dataset owner and wish to update
        any part of it (description, citation, license, etc.), or do not want your
        dataset to be included in the Hugging Face Hub, please get in touch by opening
        a discussion or a pull request in the Community tab of the dataset page. Thanks
        for your contribution to the ML community!## BibTeXIf you want to cite our
        \U0001F917 Datasets library, you can use our [paper](https://arxiv.org/abs/2109.02846):```bibtex@inproceedings{lhoest-etal-2021-datasets,
        \   title = \"Datasets: A Community Library for Natural Language Processing\",
        \   author = \"Lhoest, Quentin  and      Villanova del Moral, Albert  and
        \     Jernite, Yacine  and      Thakur, Abhishek  and      von Platen, Patrick
        \ and      Patil, Suraj  and      Chaumond, Julien  and      Drame, Mariama
        \ and      Plu, Julien  and      Tunstall, Lewis  and      Davison, Joe  and
        \     {\\v{S}}a{\\v{s}}ko, Mario  and      Chhablani, Gunjan  and      Malik,
        Bhavitvya  and      Brandeis, Simon  and      Le Scao, Teven  and      Sanh,
        Victor  and      Xu, Canwen  and      Patry, Nicolas  and      McMillan-Major,
        Angelina  and      Schmid, Philipp  and      Gugger, Sylvain  and      Delangue,
        Cl{\\'e}ment  and      Matussi{\\`e}re, Th{\\'e}o  and      Debut, Lysandre
        \ and      Bekman, Stas  and      Cistac, Pierric  and      Goehringer, Thibault
        \ and      Mustar, Victor  and      Lagunas, Fran{\\c{c}}ois  and      Rush,
        Alexander  and      Wolf, Thomas\",    booktitle = \"Proceedings of the 2021
        Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",
        \   month = nov,    year = \"2021\",    address = \"Online and Punta Cana,
        Dominican Republic\",    publisher = \"Association for Computational Linguistics\",
        \   url = \"https://aclanthology.org/2021.emnlp-demo.21\",    pages = \"175--184\",
        \   abstract = \"The scale, variety, and quantity of publicly-available NLP
        datasets has grown rapidly as researchers propose new tasks, larger models,
        and novel benchmarks. Datasets is a community library for contemporary NLP
        designed to support this ecosystem. Datasets aims to standardize end-user
        interfaces, versioning, and documentation, while providing a lightweight front-end
        that behaves similarly for small datasets as for internet-scale corpora. The
        design of the library incorporates a distributed, community-driven approach
        to adding datasets and documenting usage. After a year of development, the
        library now includes more than 650 unique datasets, has more than 250 contributors,
        and has helped support a variety of novel cross-dataset research projects
        and shared tasks. The library is available at https://github.com/huggingface/datasets.\",
        \   eprint={2109.02846},    archivePrefix={arXiv},    primaryClass={cs.CL},}```If
        you need to cite a specific version of our \U0001F917 Datasets library for
        reproducibility, you can use the corresponding version Zenodo DOI from this
        [list](https://zenodo.org/search?q=conceptrecid:%224817768%22&sort=-version&all_versions=True)."
      Package: datasets
      Source: pip
      Version: '2.14.4'
      Hash: ''
      licenses:
      - Apache-2.0
      - MIT
      - Unlicense
      Title: datasets
      DownloadURL: https://github.com/huggingface/datasets/archive/refs/tags/2.14.4.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/huggingface/datasets
    community_name: https://github.com/huggingface/datasets
    community_url: https://github.com/huggingface/datasets
    component_comment: ''
    component_highlevel_description: ''
    component_name: datasets
    component_platform: linux
    component_programing_language: ''
    component_version: '2.14.4'
    licenses: []
    src_download_link: https://github.com/huggingface/datasets/archive/refs/tags/2.14.4.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: datasets
    target_sw: linux
    vendor: pip
    version: '2.14.4'
    web_url: https://github.com/huggingface/datasets
  licenses:
  - Apache-2.0
  - MIT
  - Unlicense
  name: datasets
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '2.14.4'
  mimer:
    linking: Static
    product_number: CTX1039081
    product_version_label: '2.14.4'
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: dill+0.3.7
  additional_info:
    fossa-attribution:
      Description: "-----------------------------dill: serialize all of Python-----------------------------About
        Dill==========``dill`` extends Python's ``pickle`` module for serializing
        and de-serializingPython objects to the majority of the built-in Python types.
        Serializationis the process of converting an object to a byte stream, and
        the inverseof which is converting a byte stream back to a Python object hierarchy.``dill``
        provides the user the same interface as the ``pickle`` module, andalso includes
        some additional features. In addition to pickling Pythonobjects, ``dill``
        provides the ability to save the state of an interpretersession in a single
        command.  Hence, it would be feasible to save aninterpreter session, close
        the interpreter, ship the pickled file toanother computer, open a new interpreter,
        unpickle the session andthus continue from the 'saved' state of the original
        interpretersession.``dill`` can be used to store Python objects to a file,
        but the primaryusage is to send Python objects across the network as a byte
        stream.``dill`` is quite flexible, and allows arbitrary user defined classesand
        functions to be serialized.  Thus ``dill`` is not intended to besecure against
        erroneously or maliciously constructed data. It isleft to the user to decide
        whether the data they unpickle is froma trustworthy source.``dill`` is part
        of ``pathos``, a Python framework for heterogeneous computing.``dill`` is
        in active development, so any user feedback, bug reports, comments,or suggestions
        are highly appreciated.  A list of issues is located athttps://github.com/uqfoundation/dill/issues,
        with a legacy list maintained athttps://uqfoundation.github.io/project/pathos/query.Major
        Features==============``dill`` can pickle the following standard types:    -
        none, type, bool, int, float, complex, bytes, str,    - tuple, list, dict,
        file, buffer, builtin,    - Python classes, namedtuples, dataclasses, metaclasses,
        \   - instances of classes,    - set, frozenset, array, functions, exceptions``dill``
        can also pickle more 'exotic' standard types:    - functions with yields,
        nested functions, lambdas,    - cell, method, unboundmethod, module, code,
        methodwrapper,    - methoddescriptor, getsetdescriptor, memberdescriptor,
        wrapperdescriptor,    - dictproxy, slice, notimplemented, ellipsis, quit``dill``
        cannot yet pickle these standard types:    - frame, generator, traceback``dill``
        also provides the capability to:    - save and load Python interpreter sessions
        \   - save and extract the source code from functions and classes    - interactively
        diagnose pickling errorsCurrent Release===============The latest released
        version of ``dill`` is available from:    https://pypi.org/project/dill``dill``
        is distributed under a 3-clause BSD license.Development Version===================You
        can get the latest development version with all the shiny new features at:
        \   https://github.com/uqfoundationIf you have a new contribution, please
        submit a pull request.Installation============``dill`` can be installed with
        ``pip``::    $ pip install dillTo optionally include the ``objgraph`` diagnostic
        tool in the install::    $ pip install dill[graph]To optionally include the
        ``gprof2dot`` diagnostic tool in the install::    $ pip install dill[profile]For
        windows users, to optionally install session history tools::    $ pip install
        dill[readline]Requirements============``dill`` requires:    - ``python`` (or
        ``pypy``), **>=3.8**    - ``setuptools``, **>=42**Optional requirements:    -
        ``objgraph``, **>=1.7.2**    - ``gprof2dot``, **>=2022.7.29**    - ``pyreadline``,
        **>=1.7.1** (on windows)Basic Usage===========``dill`` is a drop-in replacement
        for ``pickle``. Existing code can beupdated to allow complete pickling using::
        \   >>> import dill as pickleor::    >>> from dill import dumps, loads``dumps``
        converts the object to a unique byte string, and ``loads`` performsthe inverse
        operation::    >>> squared = lambda x: x**2    >>> loads(dumps(squared))(3)
        \   9There are a number of options to control serialization which are providedas
        keyword arguments to several ``dill`` functions:* with *protocol*, the pickle
        protocol level can be set. This uses the  same value as the ``pickle`` module,
        *DEFAULT_PROTOCOL*.* with *byref=True*, ``dill`` to behave a lot more like
        pickle with  certain objects (like modules) pickled by reference as opposed
        to  attempting to pickle the object itself.* with *recurse=True*, objects
        referred to in the global dictionary are  recursively traced and pickled,
        instead of the default behavior of  attempting to store the entire global
        dictionary.* with *fmode*, the contents of the file can be pickled along with
        the file  handle, which is useful if the object is being sent over the wire
        to a  remote system which does not have the original file on disk. Options
        are  *HANDLE_FMODE* for just the handle, *CONTENTS_FMODE* for the file content
        \ and *FILE_FMODE* for content and handle.* with *ignore=False*, objects reconstructed
        with types defined in the  top-level script environment use the existing type
        in the environment  rather than a possibly different reconstructed type.The
        default serialization can also be set globally in *dill.settings*.Thus, we
        can modify how ``dill`` handles references to the global dictionarylocally
        or globally::    >>> import dill.settings    >>> dumps(absolute) == dumps(absolute,
        recurse=True)    False    >>> dill.settings['recurse'] = True    >>> dumps(absolute)
        == dumps(absolute, recurse=True)    True``dill`` also includes source code
        inspection, as an alternate to pickling::    >>> import dill.source    >>>
        print(dill.source.getsource(squared))    squared = lambda x:x**2To aid in
        debugging pickling issues, use *dill.detect* which providestools like pickle
        tracing::    >>> import dill.detect    >>> with dill.detect.trace():    >>>
        \    dumps(squared)    \u252C F1: <function <lambda> at 0x7fe074f8c280>    \u251C\u252C
        F2: <function _create_function at 0x7fe074c49c10>    \u2502\u2514 # F2 [34
        B]    \u251C\u252C Co: <code object <lambda> at 0x7fe07501eb30, file \"<stdin>\",
        line 1>    \u2502\u251C\u252C F2: <function _create_code at 0x7fe074c49ca0>
        \   \u2502\u2502\u2514 # F2 [19 B]    \u2502\u2514 # Co [87 B]    \u251C\u252C
        D1: <dict object at 0x7fe0750d4680>    \u2502\u2514 # D1 [22 B]    \u251C\u252C
        D2: <dict object at 0x7fe074c5a1c0>    \u2502\u2514 # D2 [2 B]    \u251C\u252C
        D2: <dict object at 0x7fe074f903c0>    \u2502\u251C\u252C D2: <dict object
        at 0x7fe074f8ebc0>    \u2502\u2502\u2514 # D2 [2 B]    \u2502\u2514 # D2 [23
        B]    \u2514 # F1 [180 B]With trace, we see how ``dill`` stored the lambda
        (``F1``) by first storing``_create_function``, the underlying code object
        (``Co``) and ``_create_code``(which is used to handle code objects), then
        we handle the reference tothe global dict (``D2``) plus other dictionaries
        (``D1`` and ``D2``) thatsave the lambda object's state. A ``#`` marks when
        the object is actually stored.More Information================Probably the
        best way to get started is to look at the documentation athttp://dill.rtfd.io.
        Also see ``dill.tests`` for a set of scripts thatdemonstrate how ``dill``
        can serialize different Python objects. You canrun the test suite with ``python
        -m dill.tests``. The contents of anypickle file can be examined with ``undill``.
        \ As ``dill`` conforms tothe ``pickle`` interface, the examples and documentation
        found athttp://docs.python.org/library/pickle.html also apply to ``dill``if
        one will ``import dill as pickle``. The source code is also generallywell
        documented, so further questions may be resolved by inspecting thecode itself.
        Please feel free to submit a ticket on github, or ask aquestion on stackoverflow
        (**@Mike McKerns**).If you would like to share how you use ``dill`` in your
        work, please sendan email (to **mmckerns at uqfoundation dot org**).Citation========If
        you use ``dill`` to do research that leads to publication, we ask that youacknowledge
        use of ``dill`` by citing the following in your publication::    M.M. McKerns,
        L. Strand, T. Sullivan, A. Fang, M.A.G. Aivazis,    \"Building a framework
        for predictive science\", Proceedings of    the 10th Python in Science Conference,
        2011;    http://arxiv.org/pdf/1202.1056    Michael McKerns and Michael Aivazis,
        \   \"pathos: a framework for heterogeneous computing\", 2010- ;    https://uqfoundation.github.io/project/pathosPlease
        see https://uqfoundation.github.io/project/pathos orhttp://arxiv.org/pdf/1202.1056
        for further information."
      Package: dill
      Source: pip
      Version: '0.3.7'
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: dill
      DownloadURL: https://files.pythonhosted.org/packages/c4/31/54dd222e02311c2dbc9e680d37cbd50f4494ce1ee9b04c69980e4ec26f38/dill-0.3.7.tar.gz
  bazaar:
    register: 'no'
    prim: 6/CTX1027132
    community_link: https://pypi.org/project/dill
    community_name: https://pypi.org/project/dill
    community_url: https://pypi.org/project/dill
    component_comment: ''
    component_highlevel_description: serialize all of python
    component_name: dill
    component_platform: linux
    component_programing_language: Python
    component_version: '0.3.7'
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/c4/31/54dd222e02311c2dbc9e680d37cbd50f4494ce1ee9b04c69980e4ec26f38/dill-0.3.7.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1054754&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: dill
    target_sw: linux
    vendor: pip
    version: '0.3.7'
    web_url: https://github.com/uqfoundation/dill
  licenses:
  - BSD-3-Clause
  name: dill
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '0.3.7'
  mimer:
    linking: Static
    product_number: CTX1027132
    product_version_label: '0.3.7'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: evaluate+0.4.1
  additional_info:
    fossa-attribution:
      Description: "<p align=\"center\">    <br>    <img src=\"https://huggingface.co/datasets/evaluate/media/resolve/main/evaluate-banner.png\"
        width=\"400\"/>    <br></p><p align=\"center\">    <a href=\"https://github.com/huggingface/evaluate/actions/workflows/ci.yml?query=branch%3Amain\">
        \       <img alt=\"Build\" src=\"https://github.com/huggingface/evaluate/actions/workflows/ci.yml/badge.svg?branch=main\">
        \   </a>    <a href=\"https://github.com/huggingface/evaluate/blob/master/LICENSE\">
        \       <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/evaluate.svg?color=blue\">
        \   </a>    <a href=\"https://huggingface.co/docs/evaluate/index\">        <img
        alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/evaluate/index.svg?down_color=red&down_message=offline&up_message=online\">
        \   </a>    <a href=\"https://github.com/huggingface/evaluate/releases\">
        \       <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/evaluate.svg\">
        \   </a>    <a href=\"CODE_OF_CONDUCT.md\">        <img alt=\"Contributor
        Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg\">
        \   </a></p>\U0001F917 Evaluate is a library that makes evaluating and comparing
        models and reporting their performance easier and more standardized. It currently
        contains:- **implementations of dozens of popular metrics**: the existing
        metrics cover a variety of tasks spanning from NLP to Computer Vision, and
        include dataset-specific metrics for datasets. With a simple command like
        `accuracy = load(\"accuracy\")`, get any of these metrics ready to use for
        evaluating a ML model in any framework (Numpy/Pandas/PyTorch/TensorFlow/JAX).-
        **comparisons and measurements**: comparisons are used to measure the difference
        between models and measurements are tools to evaluate datasets.- **an easy
        way of adding new evaluation modules to the \U0001F917 Hub**: you can create
        new evaluation modules and push them to a dedicated Space in the \U0001F917
        Hub with `evaluate-cli create [metric name]`, which allows you to see easily
        compare different metrics and their outputs for the same sets of references
        and predictions.[\U0001F393 **Documentation**](https://huggingface.co/docs/evaluate/)\U0001F50E
        **Find a [metric](https://huggingface.co/evaluate-metric), [comparison](https://huggingface.co/evaluate-comparison),
        [measurement](https://huggingface.co/evaluate-measurement) on the Hub**[\U0001F31F
        **Add a new evaluation module**](https://huggingface.co/docs/evaluate/)\U0001F917
        Evaluate also has lots of useful features like:- **Type checking**: the input
        types are checked to make sure that you are using the right input formats
        for each metric- **Metric cards**: each metrics comes with a card that describes
        the values, limitations and their ranges, as well as providing examples of
        their usage and usefulness.- **Community metrics:** Metrics live on the Hugging
        Face Hub and you can easily add your own metrics for your project or to collaborate
        with others.# Installation## With pip\U0001F917 Evaluate can be installed
        from PyPi and has to be installed in a virtual environment (venv or conda
        for instance)```bashpip install evaluate```# Usage\U0001F917 Evaluate's main
        methods are:- `evaluate.list_evaluation_modules()` to list the available metrics,
        comparisons and measurements- `evaluate.load(module_name, **kwargs)` to instantiate
        an evaluation module- `results = module.compute(*kwargs)` to compute the result
        of an evaluation module# Adding a new evaluation moduleFirst install the necessary
        dependencies to create a new metric with the following command:```bashpip
        install evaluate[template]```Then you can get started with the following command
        which will create a new folder for your metric and display the necessary steps:```bashevaluate-cli
        create \"Awesome Metric\"```See this [step-by-step guide](https://huggingface.co/docs/evaluate/creating_and_sharing)
        in the documentation for detailed instructions.## CreditsThanks to [@marella](https://github.com/marella)
        for letting us use the `evaluate` namespace on PyPi previously used by his
        [library](https://github.com/marella/evaluate)."
      Package: evaluate
      Source: pip
      Version: '0.4.1'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: evaluate
      DownloadURL: https://files.pythonhosted.org/packages/ee/55/198d8bef9179e9f4f8ed34580cec0706abe1a3c98659bce551734eff6b8f/evaluate-0.4.1.tar.gz
  bazaar:
    register: 'no'
    prim: 1/CTX1039077
    community_link: https://pypi.org/project/evaluate/
    community_name: https://pypi.org/project/evaluate/
    community_url: https://pypi.org/project/evaluate/
    component_comment: ''
    component_highlevel_description: Evaluate is a library that makes evaluating and
      comparing models and reporting their performance easier and more standardized.
    component_name: evaluate
    component_platform: linux
    component_programing_language: Python
    component_version: '0.4.1'
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://files.pythonhosted.org/packages/ee/55/198d8bef9179e9f4f8ed34580cec0706abe1a3c98659bce551734eff6b8f/evaluate-0.4.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1073777&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: France
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: evaluate
    target_sw: linux
    vendor: pip
    version: '0.4.1'
    web_url: https://github.com/huggingface/evaluate
  licenses:
  - Apache-2.0
  name: evaluate
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '0.4.1'
  mimer:
    linking: Static
    product_number: CTX1039077
    product_version_label: '0.4.1'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: filelock+3.13.1
  additional_info:
    fossa-attribution:
      Description: '# py-filelock![travis-ci](https://travis-ci.org/benediktschmitt/py-filelock.svg?branch=master)This
        package contains a single module, which implements a platform independentfile
        lock in Python, which provides a simple way of inter-process communication:```Pythonfrom
        filelock import Timeout, FileLocklock = FileLock("high_ground.txt.lock")with
        lock:    open("high_ground.txt", "a").write("You were the chosen one.")        ```**Don''t
        use** a *FileLock* to lock the file you want to write to, instead createa
        separate *.lock* file as shown above.![animated example](https://raw.githubusercontent.com/benediktschmitt/py-filelock/master/example/example.gif)##
        Similar librariesPerhaps you are looking for something like*   https://pypi.python.org/pypi/pid/2.1.1*   https://docs.python.org/3.6/library/msvcrt.html#msvcrt.locking*   or
        https://docs.python.org/3/library/fcntl.html#fcntl.flock## Installation*py-filelock*
        is available via PyPi:```$ pip3 install filelock```## DocumentationThe documentation
        for the API is available on[readthedocs.org](https://filelock.readthedocs.io/).###
        ExamplesA *FileLock* is used to indicate another process of your application
        that aresource or workingdirectory is currently used. To do so, create a *FileLock*
        first:```Pythonfrom filelock import Timeout, FileLockfile_path = "high_ground.txt"lock_path
        = "high_ground.txt.lock"lock = FileLock(lock_path, timeout=1)```The lock object
        supports multiple ways for acquiring the lock, including theones used to acquire
        standard Python thread locks:```Pythonwith lock:    open(file_path, "a").write("Hello
        there!")lock.acquire()try:    open(file_path, "a").write("General Kenobi!")finally:    lock.release()```The
        *acquire()* method accepts also a *timeout* parameter. If the lock cannot
        beacquired within *timeout* seconds, a *Timeout* exception is raised:```Pythontry:    with
        lock.acquire(timeout=10):        open(file_path, "a").write("I have a bad
        feeling about this.")except Timeout:    print("Another instance of this application
        currently holds the lock.")```The lock objects are recursive locks, which
        means that once acquired, they willnot block on successive lock requests:```Pythondef
        cite1():    with lock:        open(file_path, "a").write("I hate it when he
        does that.")def cite2():    with lock:        open(file_path, "a").write("You
        don''t want to sell me death sticks.")# The lock is acquired here.with lock:    cite1()    cite2()#
        And released here.```## FileLock vs SoftFileLockThe *FileLock* is platform
        dependent while the *SoftFileLock* is not. Use the*FileLock* if all instances
        of your application are running on the same host anda *SoftFileLock* otherwise.The
        *SoftFileLock* only watches the existence of the lock file. This makes itultra
        portable, but also more prone to dead locks if the application crashes.You
        can simply delete the lock file in such cases.## ContributionsContributions
        are always welcome, please make sure they pass all tests beforecreating a
        pull request. Never hesitate to open a new issue, although it maytake some
        time for me to respond.## LicenseThis package is [public domain](./LICENSE.rst).'
      Package: filelock
      Source: pip
      Version: '3.13.1'
      Hash: ''
      licenses:
      - Unlicense
      Title: filelock
      DownloadURL: https://files.pythonhosted.org/packages/70/70/41905c80dcfe71b22fb06827b8eae65781783d4a14194bce79d16a013263/filelock-3.13.1.tar.gz
  bazaar:
    register: 'no'
    prim: 18/CTX1026319
    community_link: https://pypi.org/project/filelock/
    community_name: https://pypi.org/project/filelock/
    community_url: https://pypi.org/project/filelock/
    component_comment: ''
    component_highlevel_description: A platform-independent file lock for Python.
    component_name: py-filelock
    component_platform: linux
    component_programing_language: Python
    component_version: '3.13.1'
    licenses:
    - FAL1159217 (The Unlicense (Unlicense))
    src_download_link: https://files.pythonhosted.org/packages/70/70/41905c80dcfe71b22fb06827b8eae65781783d4a14194bce79d16a013263/filelock-3.13.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078632&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: filelock
    target_sw: linux
    vendor: pip
    version: '3.13.1'
    web_url: https://github.com/benediktschmitt/py-filelock
  licenses:
  - Unlicense
  name: filelock
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '3.13.1'
  mimer:
    linking: Static
    product_number: CTX1026319
    product_version_label: '3.13.1'
    selected_licenses:
    - Unlicense
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: flatbuffers+23.5.26
  additional_info:
    fossa-attribution:
      Description: Python runtime library for use with the `Flatbuffers <https://google.github.io/flatbuffers/>`_
        serialization format.
      Package: flatbuffers
      Source: pip
      Version: '23.5.26'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: flatbuffers
      DownloadURL: https://files.pythonhosted.org/packages/0c/6e/3e52cd294d8e7a61e010973cce076a0cb2c6c0dfd4d0b7a13648c1b98329/flatbuffers-23.5.26.tar.gz
  bazaar:
    register: 'no'
    prim: 17/CAX1059918
    community_link: https://github.com/google/flatbuffers
    community_name: https://github.com/google/flatbuffers
    community_url: https://github.com/google/flatbuffers
    component_comment: ''
    component_highlevel_description: FlatBuffers is an efficient cross platform serialization
      library for games and other memory constrained apps. It allows you to directly
      access serialized data without unpacking/parsing it first, while still having
      great forwards/backwards compatibility.
    component_name: FlatBuffers
    component_platform: linux
    component_programing_language: C++
    component_version: V23.5.26
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/google/flatbuffers/archive/refs/tags/v23.5.26.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1057242&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: C++
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: flatbuffers
    target_sw: linux
    vendor: pip
    version: '23.5.26'
    web_url: https://google.github.io/flatbuffers/
  licenses:
  - Apache-2.0
  name: flatbuffers
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '23.5.26'
  mimer:
    linking: Static
    product_number: CAX1059918
    product_version_label: v23.5.26
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: frozenlist+1.4.1
  additional_info:
    fossa-attribution:
      Description: "frozenlist==========.. image:: https://github.com/aio-libs/frozenlist/workflows/CI/badge.svg
        \  :target: https://github.com/aio-libs/frozenlist/actions   :alt: GitHub
        status for master branch.. image:: https://codecov.io/gh/aio-libs/frozenlist/branch/master/graph/badge.svg
        \  :target: https://codecov.io/gh/aio-libs/frozenlist   :alt: codecov.io status
        for master branch.. image:: https://img.shields.io/pypi/v/frozenlist.svg?logo=Python&logoColor=white
        \  :target: https://pypi.org/project/frozenlist   :alt: frozenlist @ PyPI..
        image:: https://readthedocs.org/projects/frozenlist/badge/?version=latest
        \  :target: https://frozenlist.aio-libs.org   :alt: Read The Docs build status
        badge.. image:: https://img.shields.io/matrix/aio-libs:matrix.org?label=Discuss%20on%20Matrix%20at%20%23aio-libs%3Amatrix.org&logo=matrix&server_fqdn=matrix.org&style=flat
        \  :target: https://matrix.to/#/%23aio-libs:matrix.org   :alt: Matrix Room
        \u2014 #aio-libs:matrix.org.. image:: https://img.shields.io/matrix/aio-libs-space:matrix.org?label=Discuss%20on%20Matrix%20at%20%23aio-libs-space%3Amatrix.org&logo=matrix&server_fqdn=matrix.org&style=flat
        \  :target: https://matrix.to/#/%23aio-libs-space:matrix.org   :alt: Matrix
        Space \u2014 #aio-libs-space:matrix.orgIntroduction------------``frozenlist.FrozenList``
        is a list-like structure which implements``collections.abc.MutableSequence``.
        The list is *mutable* until ``FrozenList.freeze``is called, after which list
        modifications raise ``RuntimeError``:>>> from frozenlist import FrozenList>>>
        fl = FrozenList([17, 42])>>> fl.append('spam')>>> fl.append('Vikings')>>>
        fl<FrozenList(frozen=False, [17, 42, 'spam', 'Vikings'])>>>> fl.freeze()>>>
        fl<FrozenList(frozen=True, [17, 42, 'spam', 'Vikings'])>>>> fl.frozenTrue>>>
        fl.append(\"Monty\")Traceback (most recent call last):  File \"<stdin>\",
        line 1, in <module>  File \"frozenlist/_frozenlist.pyx\", line 97, in frozenlist._frozenlist.FrozenList.append
        \   self._check_frozen()  File \"frozenlist/_frozenlist.pyx\", line 19, in
        frozenlist._frozenlist.FrozenList._check_frozen    raise RuntimeError(\"Cannot
        modify frozen list.\")RuntimeError: Cannot modify frozen list.FrozenList is
        also hashable, but only when frozen. Otherwise it also throws a RuntimeError:>>>
        fl = FrozenList([17, 42, 'spam'])>>> hash(fl)Traceback (most recent call last):
        \ File \"<stdin>\", line 1, in <module>  File \"frozenlist/_frozenlist.pyx\",
        line 111, in frozenlist._frozenlist.FrozenList.__hash__    raise RuntimeError(\"Cannot
        hash unfrozen list.\")RuntimeError: Cannot hash unfrozen list.>>> fl.freeze()>>>
        hash(fl)3713081631934410656>>> dictionary = {fl: 'Vikings'} # frozen fl can
        be a dict key>>> dictionary{<FrozenList(frozen=True, [1, 2])>: 'Vikings'}Installation------------::
        \  $ pip install frozenlistThe library requires Python 3.8 or newer.Documentation-------------https://frozenlist.aio-libs.orgCommunication
        channels----------------------We have a *Matrix Space* `#aio-libs-space:matrix.org<https://matrix.to/#/%23aio-libs-space:matrix.org>`_
        which isalso accessible via Gitter.Requirements------------- Python >= 3.8License-------``frozenlist``
        is offered under the Apache 2 license.Source code-----------The project is
        hosted on GitHub_Please file an issue in the `bug tracker<https://github.com/aio-libs/frozenlist/issues>`_
        if you have found a bugor have some suggestions to improve the library...
        _GitHub: https://github.com/aio-libs/frozenlist=========Changelog=========..
        \   You should *NOT* be adding new change log entries to this file, this    file
        is managed by towncrier. You *may* edit previous change logs to    fix problems
        like typo corrections or such.    To add a new change log entry, please see
        \   https://pip.pypa.io/en/latest/development/contributing/#news-entries    we
        named the news folder \"changes\".    WARNING: Don't drop the next directive!..
        towncrier release notes start1.4.1 (2023-12-15)==================Packaging
        updates and notes for downstreams--------------------------------------------
        Declared Python 3.12 and PyPy 3.8-3.10 supported officially  in the distribution
        package metadata.  *Related issues and pull requests on GitHub:*  `#553 <https://github.com/aio-libs/yarl/issues/553>`__.-
        Replaced the packaging is replaced from an old-fashioned ``setup.py`` to an
        \ in-tree `PEP 517 <https://peps.python.org/pep-517>`__ build backend -- by
        `@webknjaz <https://github.com/sponsors/webknjaz>`__.  Whenever the end-users
        or downstream packagers need to build ``frozenlist``  from source (a Git checkout
        or an sdist), they may pass a ``config_settings``  flag ``pure-python``. If
        this flag is not set, a C-extension will be built  and included into the distribution.
        \ Here is how this can be done with ``pip``:  .. code-block:: console      $
        python3 -m pip install . --config-settings=pure-python=  This will also work
        with ``-e | --editable``.  The same can be achieved via ``pypa/build``:  ..
        code-block:: console      $ python3 -m build --config-setting=pure-python=
        \ Adding ``-w | --wheel`` can force ``pypa/build`` produce a wheel from source
        \ directly, as opposed to building an ``sdist`` and then building from it.
        \ *Related issues and pull requests on GitHub:*  `#560 <https://github.com/aio-libs/yarl/issues/560>`__.Contributor-facing
        changes--------------------------- It is now possible to request line tracing
        in Cython builds using the  ``with-cython-tracing`` `PEP 517 <https://peps.python.org/pep-517>`__
        config setting  -- `@webknjaz <https://github.com/sponsors/webknjaz>`__.  This
        can be used in CI and development environment to measure coverage  on Cython
        modules, but is not normally useful to the end-users or  downstream packagers.
        \ Here's a usage example:  .. code-block:: console      $ python3 -Im pip
        install . --config-settings=with-cython-tracing=true  For editable installs,
        this setting is on by default. Otherwise, it's  off unless requested explicitly.
        \ The following produces C-files required for the Cython coverage  plugin
        to map the measurements back to the PYX-files:  .. code-block:: console      $
        python -Im pip install -e .  Alternatively, the ``FROZENLIST_CYTHON_TRACING=1``
        environment variable  can be set to do the same as the `PEP 517 <https://peps.python.org/pep-517>`__
        config setting.  *Related issues and pull requests on GitHub:*  `#560 <https://github.com/aio-libs/yarl/issues/560>`__.-
        Coverage collection has been implemented for the Cython modules  -- by `@webknjaz
        <https://github.com/sponsors/webknjaz>`__.  It will also be reported to Codecov
        from any non-release CI jobs.  *Related issues and pull requests on GitHub:*
        \ `#561 <https://github.com/aio-libs/yarl/issues/561>`__.- A step-by-step
        ``Release Guide`` guide has  been added, describing how to release *frozenlist*
        -- by `@webknjaz <https://github.com/sponsors/webknjaz>`__.  This is primarily
        targeting the maintainers.  *Related issues and pull requests on GitHub:*
        \ `#563 <https://github.com/aio-libs/yarl/issues/563>`__.- Detailed ``Contributing
        Guidelines`` on  authoring the changelog fragments have been published in
        the  documentation -- by `@webknjaz <https://github.com/sponsors/webknjaz>`__.
        \ *Related issues and pull requests on GitHub:*  `#564 <https://github.com/aio-libs/yarl/issues/564>`__.----1.4.0
        (2023-07-12)==================The published source distribution package became
        buildableunder Python 3.12.----Bugfixes--------- Removed an unused ``typing.Tuple``
        import  `#411 <https://github.com/aio-libs/frozenlist/issues/411>`_Deprecations
        and Removals-------------------------- Dropped Python 3.7 support.  `#413
        <https://github.com/aio-libs/frozenlist/issues/413>`_Misc----- `#410 <https://github.com/aio-libs/frozenlist/issues/410>`_,
        `#433 <https://github.com/aio-libs/frozenlist/issues/433>`_----1.3.3 (2022-11-08)==================-
        Fixed CI runs when creating a new release, where new towncrier versions  fail
        when the current version section is already present.----1.3.2 (2022-11-08)==================Misc-----
        Updated the CI runs to better check for test results and to avoid deprecated
        syntax. `#327 <https://github.com/aio-libs/frozenlist/issues/327>`_----1.3.1
        (2022-08-02)==================The published source distribution package became
        buildableunder Python 3.11.----1.3.0 (2022-01-18)==================Bugfixes---------
        Do not install C sources with binary distributions.  `#250 <https://github.com/aio-libs/frozenlist/issues/250>`_Deprecations
        and Removals-------------------------- Dropped Python 3.6 support  `#274 <https://github.com/aio-libs/frozenlist/issues/274>`_----1.2.0
        (2021-10-16)==================Features--------- ``FrozenList`` now supports
        being used as a generic type as per PEP 585, e.g. ``frozen_int_list: FrozenList[int]``
        (requires Python 3.9 or newer).  `#172 <https://github.com/aio-libs/frozenlist/issues/172>`_-
        Added support for Python 3.10.  `#227 <https://github.com/aio-libs/frozenlist/issues/227>`_-
        Started shipping platform-specific wheels with the ``musl`` tag targeting
        typical Alpine Linux runtimes.  `#227 <https://github.com/aio-libs/frozenlist/issues/227>`_-
        Started shipping platform-specific arm64 wheels for Apple Silicon.  `#227
        <https://github.com/aio-libs/frozenlist/issues/227>`_----1.1.1 (2020-11-14)==================Bugfixes---------
        Provide x86 Windows wheels.  `#169 <https://github.com/aio-libs/frozenlist/issues/169>`_----1.1.0
        (2020-10-13)==================Features--------- Add support for hashing of
        a frozen list.  `#136 <https://github.com/aio-libs/frozenlist/issues/136>`_-
        Support Python 3.8 and 3.9.- Provide wheels for ``aarch64``, ``i686``, ``ppc64le``,
        ``s390x`` architectures on  Linux as well as ``x86_64``.----1.0.0 (2019-11-09)==================Deprecations
        and Removals-------------------------- Dropped support for Python 3.5; only
        3.6, 3.7 and 3.8 are supported going forward.  `#24 <https://github.com/aio-libs/frozenlist/issues/24>`_"
      Package: frozenlist
      Source: pip
      Version: '1.4.1'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: frozenlist
      DownloadURL: https://files.pythonhosted.org/packages/cf/3d/2102257e7acad73efc4a0c306ad3953f68c504c16982bbdfee3ad75d8085/frozenlist-1.4.1.tar.gz
  bazaar:
    register: 'no'
    prim: 6/CTX1035563
    community_link: https://github.com/aio-libs/frozenlist
    community_name: https://github.com/aio-libs/frozenlist
    community_url: https://github.com/aio-libs/frozenlist
    component_comment: ''
    component_highlevel_description: FrozenList is a list-like structure which implements
      collections.abc.MutableSequence, and which can be made immutable.
    component_name: frozenlist
    component_platform: linux
    component_programing_language: Python
    component_version: V1.4.1
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/aio-libs/frozenlist/archive/v1.4.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1075929&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Ukraine
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: frozenlist
    target_sw: linux
    vendor: pip
    version: '1.4.1'
    web_url: https://github.com/aio-libs/frozenlist
  licenses:
  - Apache-2.0
  name: frozenlist
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '1.4.1'
  mimer:
    linking: Static
    product_number: CTX1035563
    product_version_label: v1.4.1
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: fsspec+2024.2.0
  additional_info:
    fossa-attribution:
      Description: '# filesystem_spec[![PyPI version](https://badge.fury.io/py/fsspec.svg)](https://pypi.python.org/pypi/fsspec/)[![Anaconda-Server
        Badge](https://anaconda.org/conda-forge/fsspec/badges/version.svg)](https://anaconda.org/conda-forge/fsspec)![Build](https://github.com/fsspec/filesystem_spec/workflows/CI/badge.svg)[![Docs](https://readthedocs.org/projects/filesystem-spec/badge/?version=latest)](https://filesystem-spec.readthedocs.io/en/latest/?badge=latest)[![PyPi
        downloads](https://img.shields.io/pypi/dm/fsspec?label=pypi%20downloads&style=flat)](https://pepy.tech/project/fsspec)A
        specification for pythonic filesystems.## Install```bashpip install fsspec```would
        install the base fsspec. Various optionally supported features might require
        specification of customextra require, e.g. `pip install fsspec[ssh]` will
        install dependencies for `ssh` backends support.Use `pip install fsspec[full]`
        for installation of all known extra dependencies.Up-to-date package also provided
        through conda-forge distribution:```bashconda install -c conda-forge fsspec```##
        PurposeTo produce a template or specification for a file-system interface,
        that specific implementations should follow,so that applications making use
        of them can rely on a common behaviour and not have to worry about the specificinternal
        implementation decisions with any given backend. Many such implementations
        are included in this package,or in sister projects such as `s3fs` and `gcsfs`.In
        addition, if this is well-designed, then additional functionality, such as
        a key-value store or FUSEmounting of the file-system implementation may be
        available for all implementations "for free".## DocumentationPlease refer
        to [RTD](https://filesystem-spec.readthedocs.io/en/latest/?badge=latest)##
        Developfsspec uses GitHub Actions for CI. Environment files can be foundin
        the "ci/" directory. Note that the main environment is called "py38",but it
        is expected that the version of python installed be adjustable atCI runtime.
        For local use, pick a version suitable for you.### TestingTests can be run
        in the dev environment, if activated, via ``pytest fsspec``.The full fsspec
        suite requires a system-level docker, docker-compose, and fuseinstallation.
        If only making changes to one backend implementation, it isnot generally necessary
        to run all tests locally.It is expected that contributors ensure that any
        change to fsspec does notcause issues or regressions for either other fsspec-related
        packages suchas gcsfs and s3fs, nor for downstream users of fsspec. The "downstream"
        CIrun and corresponding environment file run a set of tests from the dasktest
        suite, and very minimal tests against pandas and zarr from thetest_downstream.py
        module in this repo.### Code Formattingfsspec uses [Black](https://black.readthedocs.io/en/stable)
        to ensurea consistent code format throughout the project.Run ``black fsspec``
        from the root of the filesystem_spec repository toauto-format your code. Additionally,
        many editors have plugins that will apply``black`` as you edit files. ``black``
        is included in the ``tox`` environments.Optionally, you may wish to setup
        [pre-commit hooks](https://pre-commit.com) toautomatically run ``black`` when
        you make a git commit.Run ``pre-commit install --install-hooks`` from the
        root of thefilesystem_spec repository to setup pre-commit hooks. ``black``
        will now be runbefore you commit, reformatting any changed files. You can
        format withoutcommitting via ``pre-commit run`` or skip these checks with
        ``git commit--no-verify``.'
      Package: fsspec
      Source: pip
      Version: '2024.2.0'
      Hash: ''
      licenses:
      - BSD-3-Clause
      - Unlicense
      Title: fsspec
      DownloadURL: https://github.com/fsspec/filesystem_spec/archive/refs/tags/2024.2.0.tar.gz
  bazaar:
    register: 'no'
    prim: 15/CTX1033524
    community_link: https://github.com/fsspec/filesystem_spec
    community_name: https://github.com/fsspec/filesystem_spec
    community_url: https://github.com/fsspec/filesystem_spec
    component_comment: ''
    component_highlevel_description: S3FS builds on aiobotocore to provide a convenient Python filesystem interface for S3.
    component_name: fsspec
    component_platform: linux
    component_programing_language: Python
    component_version: 2024.2.0
    licenses:
    - FAL1159003/2 (BSD 3-Clause "New" or "Revised" License (BSD-3-Cla)
    src_download_link: https://github.com/fsspec/filesystem_spec/archive/refs/tags/2024.2.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1080447&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: fsspec
    target_sw: linux
    vendor: pip
    version: '2024.2.0'
    web_url: https://github.com/fsspec/filesystem_spec
  licenses:
  - BSD-3-Clause
  - Unlicense
  name: fsspec
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '2024.2.0'
  mimer:
    linking: Static
    product_number: CTX1033524
    product_version_label: 2024.2.0
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: gast+0.5.4
  additional_info:
    fossa-attribution:
      Description: A generic AST to represent Python2 and Python3's Abstract Syntax
        Tree(AST).GAST provides a compatibility layer between the AST of various Python
        versions,as produced by ``ast.parse`` from the standard ``ast`` module.
      Package: gast
      Source: pip
      Version: '0.5.4'
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: gast
      DownloadURL: https://files.pythonhosted.org/packages/e4/41/f26f62ebef1a80148e20951a6e9ef4d0ebbe2090124bc143da26e12a934c/gast-0.5.4.tar.gz
  bazaar:
    register: 'no'
    prim: 5/CTX1022524
    community_link: https://github.com/serge-sans-paille/gast
    community_name: https://github.com/serge-sans-paille/gast
    community_url: https://github.com/serge-sans-paille/gast
    component_comment: ''
    component_highlevel_description: A generic AST to represent Python2 and Python3's
      Abstract Syntax Tree(AST).
    component_name: gast
    component_platform: linux
    component_programing_language: Python
    component_version: '0.5.4'
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://github.com/serge-sans-paille/gast/archive/refs/tags/0.5.4.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Low comunity activity.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1082914&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: gast
    target_sw: linux
    vendor: pip
    version: '0.5.4'
    web_url: https://github.com/serge-sans-paille/gast/
  licenses:
  - BSD-3-Clause
  name: gast
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '0.5.4'
  mimer:
    linking: Static
    product_number: CTX1022524
    product_version_label: '0.5.4'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: google-auth+2.28.1
  additional_info:
    fossa-attribution:
      Description: 'Google Auth Python Library==========================|pypi|This
        library simplifies using Google''s various server-to-server authenticationmechanisms
        to access Google APIs... |pypi| image:: https://img.shields.io/pypi/v/google-auth.svg   :target:
        https://pypi.python.org/pypi/google-authInstalling----------You can install
        using `pip`_::    $ pip install google-auth.. _pip: https://pip.pypa.io/en/stable/For
        more information on setting up your Python development environment, please
        refer to `Python Development Environment Setup Guide`_ for Google Cloud Platform...
        _`Python Development Environment Setup Guide`: https://cloud.google.com/python/docs/setupExtras------google-auth
        has few extras that you can install. For example::    $ pip install google-auth[pyopenssl]Note
        that the extras pyopenssl and enterprise_cert should not be used together
        because they use conflicting versions of `cryptography`_... _`cryptography`:
        https://cryptography.io/en/latest/Supported Python Versions^^^^^^^^^^^^^^^^^^^^^^^^^Python
        >= 3.7**NOTE**:Python 3.7 was marked as `unsupported`_ by the python community
        in June 2023.We recommend that all developers upgrade to Python 3.8 and newer
        as soon asthey can. Support for Python 3.7 will be removed from this library
        afterJanuary 1 2024. Previous releases that support Python 3.7 will continue
        to be availablefor download, but releases after January 1 2024 will only target
        Python 3.8 andnewer... _unsupported: https://devguide.python.org/versions/#unsupported-versionsUnsupported
        Python Versions^^^^^^^^^^^^^^^^^^^^^^^^^^^- Python == 2.7:  The last version
        of this library with support for Python 2.7  was `google.auth == 1.34.0`.-
        Python 3.5:   The last version of this library with support for Python 3.5  was
        `google.auth == 1.23.0`.- Python 3.6:   The last version of this library with
        support for Python 3.6  was `google.auth == 2.22.0`.Documentation-------------Google
        Auth Python Library has usage and reference documentation at https://googleapis.dev/python/google-auth/latest/index.html.Current
        Maintainers-------------------- googleapis-auth@google.comAuthors--------
        `@theacodes <https://github.com/theacodes>`_ (Thea Flowers)- `@dhermes <https://github.com/dhermes>`_
        (Danny Hermes)- `@lukesneeringer <https://github.com/lukesneeringer>`_ (Luke
        Sneeringer)- `@busunkim96 <https://github.com/busunkim96>`_ (Bu Sun Kim)Contributing------------Contributions
        to this library are always welcome and highly encouraged.See `CONTRIBUTING.rst`_
        for more information on how to get started... _CONTRIBUTING.rst: https://github.com/googleapis/google-auth-library-python/blob/main/CONTRIBUTING.rstLicense-------Apache
        2.0 - See `the LICENSE`_ for more information... _the LICENSE: https://github.com/googleapis/google-auth-library-python/blob/main/LICENSE'
      Package: google-auth
      Source: pip
      Version: '2.28.1'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: google-auth
      DownloadURL: https://files.pythonhosted.org/packages/9a/15/ac42556763c08e1b1821a7e55f3a93982c50ca7f25adf8f61a01dd2ed98b/google-auth-2.28.1.tar.gz
  bazaar:
    register: 'no'
    prim: 70/CTX1025091
    community_link: https://pypi.org/project/google-auth
    community_name: https://pypi.org/project/google-auth
    community_url: https://pypi.org/project/google-auth
    component_comment: ''
    component_highlevel_description: Google Auth Python Library
    component_name: google-auth
    component_platform: linux
    component_programing_language: Python
    component_version: '2.28.1'
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://files.pythonhosted.org/packages/9a/15/ac42556763c08e1b1821a7e55f3a93982c50ca7f25adf8f61a01dd2ed98b/google-auth-2.28.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1083768&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: google-auth
    target_sw: linux
    vendor: pip
    version: '2.28.1'
    web_url: https://github.com/googleapis/google-auth-library-python
  licenses:
  - Apache-2.0
  name: google-auth
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '2.28.1'
  mimer:
    linking: Static
    product_number: CTX1025091
    product_version_label: '2.28.1'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: google-auth-oauthlib+1.0.0
  additional_info:
    fossa-attribution:
      Description: 'oauthlib integration for Google Auth====================================|pypi|This
        library provides `oauthlib`_ integration with `google-auth`_... |build| image::
        https://travis-ci.org/googleapis/google-auth-library-python-oauthlib.svg?branch=main   :target:
        https://googleapis.dev/python/google-auth-oauthlib/latest/index.html.. |pypi|
        image:: https://img.shields.io/pypi/v/google-auth-oauthlib.svg   :target:
        https://pypi.python.org/pypi/google-auth-oauthlib.. _oauthlib: https://github.com/idan/oauthlib..
        _google-auth: https://github.com/googleapis/google-auth-library-pythonInstalling----------You
        can install using `pip`_::    $ pip install google-auth-oauthlib.. _pip: https://pip.pypa.io/en/stable/Documentation-------------The
        latest documentation is available at `google-auth-oauthlib.googleapis.dev`_...
        _google-auth-oauthlib.googleapis.dev: https://googleapis.dev/python/google-auth-oauthlib/latest/index.htmlSupported
        Python Versions-------------------------Python >= 3.6Unsupported Python Versions---------------------------Python
        == 2.7, Python == 3.5.The last version of this library compatible with Python
        2.7 and 3.5 is`google-auth-oauthlib==0.4.1`.License-------Apache 2.0 - See
        `the LICENSE`_ for more information... _the LICENSE: https://github.com/googleapis/google-auth-library-python-oauthlib/blob/main/LICENSE'
      Package: google-auth-oauthlib
      Source: pip
      Version: '1.0.0'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: google-auth-oauthlib
      DownloadURL: https://github.com/googleapis/google-auth-library-python-oauthlib/archive/refs/tags/v1.0.0.tar.gz
  bazaar:
    register: 'no'
    prim: 5/CTX1029763
    community_link: https://github.com/GoogleCloudPlatform/google-auth-library-python-oauthlib
    community_name: https://github.com/GoogleCloudPlatform/google-auth-library-python-oauthlib
    community_url: https://github.com/GoogleCloudPlatform/google-auth-library-python-oauthlib
    component_comment: ''
    component_highlevel_description: This library provides oauthlib integration with google-auth.
    component_name: google-auth-oauthlib
    component_platform: linux
    component_programing_language: Python
    component_version: V1.0.0
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/googleapis/google-auth-library-python-oauthlib/archive/v1.0.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1072237&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: google-auth-oauthlib
    target_sw: linux
    vendor: pip
    version: '1.0.0'
    web_url: https://github.com/GoogleCloudPlatform/google-auth-library-python-oauthlib
  licenses:
  - Apache-2.0
  name: google-auth-oauthlib
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '1.0.0'
  mimer:
    linking: Static
    product_number: CTX1029763
    product_version_label: v1.0.0
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: google-pasta+0.2.0
  additional_info:
    fossa-attribution:
      Description: The fossa information is fetched from  FOSSA service. Don't Edit!
      Package: google-pasta
      Source: pip
      Version: '0.2.0'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: google-pasta
      DownloadURL: https://github.com/google/pasta/archive/refs/tags/v0.2.0.tar.gz
  bazaar:
    register: 'no'
    prim: 1/CTX1029578
    community_link: https://github.com/google/pasta/
    community_name: https://github.com/google/pasta/
    community_url: https://github.com/google/pasta/
    component_comment: ''
    component_highlevel_description: Library to refactor python code through AST manipulation.
    component_name: google-pasta
    component_platform: linux
    component_programing_language: Python
    component_version: 0.2.0
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/google/pasta/archive/refs/tags/v0.2.0.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=942319&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: google-pasta
    target_sw: linux
    vendor: pip
    version: '0.2.0'
    web_url: ''
  licenses:
  - Apache-2.0
  name: google-pasta
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '0.2.0'
  mimer:
    linking: Static
    product_number: CTX1029578
    product_version_label: v0.2.0
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: grpcio+1.62.0
  additional_info:
    fossa-attribution:
      Description: 'gRPC Python===========|compat_check_pypi|Package for gRPC Python...
        |compat_check_pypi| image:: https://python-compatibility-tools.appspot.com/one_badge_image?package=grpcio   :target:
        https://python-compatibility-tools.appspot.com/one_badge_target?package=grpcioSupported
        Python Versions-------------------------Python >= 3.8Installation------------gRPC
        Python is available for Linux, macOS, and Windows.Installing From PyPI~~~~~~~~~~~~~~~~~~~~If
        you are installing locally...::  $ pip install grpcioElse system wide (on
        Ubuntu)...::  $ sudo pip install grpcioIf you''re on Windows make sure that
        you installed the :code:`pip.exe` componentwhen you installed Python (if not
        go back and install it!) then invoke:::  $ pip.exe install grpcioWindows users
        may need to invoke :code:`pip.exe` from a command line ran asadministrator.n.b.
        On Windows and on Mac OS X one *must* have a recent release of :code:`pip`to
        retrieve the proper wheel from PyPI. Be sure to upgrade to the latestversion!Installing
        From Source~~~~~~~~~~~~~~~~~~~~~~Building from source requires that you have
        the Python headers (usually apackage named :code:`python-dev`).::  $ export
        REPO_ROOT=grpc  # REPO_ROOT can be any directory of your choice  $ git clone
        -b RELEASE_TAG_HERE https://github.com/grpc/grpc $REPO_ROOT  $ cd $REPO_ROOT  $
        git submodule update --init  # For the next two commands do `sudo pip install`
        if you get permission-denied errors  $ pip install -r requirements.txt  $
        GRPC_PYTHON_BUILD_WITH_CYTHON=1 pip install .You cannot currently install
        Python from source on Windows. Things might workout for you in MSYS2 (follow
        the Linux instructions), but it isn''t officiallysupported at the moment.Troubleshooting~~~~~~~~~~~~~~~Help,
        I ...* **... see the following error on some platforms**  ::    /tmp/pip-build-U8pSsr/cython/Cython/Plex/Scanners.c:4:20:
        fatal error: Python.h: No such file or directory    #include "Python.h"                    ^    compilation
        terminated.  You can fix it by installing `python-dev` package. i.e  ::    sudo
        apt-get install python-dev'
      Package: grpcio
      Source: pip
      Version: '1.62.0'
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-2-Clause
      - BSD-3-Clause
      - GPL-1.0-or-later
      - GPL-2.0-only
      - ISC
      - Info-ZIP
      - MIT
      - MPL-2.0
      - OpenSSL
      - Zlib
      - apache-2.0 WITH llvm-exception
      - mit-old-style
      - mit-old-style-no-advert
      - openssl-nokia-psk-contribution
      - openssl-ssleay
      Title: grpcio
      DownloadURL: https://github.com/grpc/grpc/archive/refs/tags/v1.62.0.tar.gz
  bazaar:
    register: 'yes'
    prim: SCAS-642718
    community_link: https://github.com/grpc/grpc/
    community_name: https://github.com/grpc/grpc/
    community_url: https://github.com/grpc/grpc/
    component_comment: ''
    component_highlevel_description: ''
    component_name: grpcio
    component_platform: linux
    component_programing_language: ''
    component_version: '1.62.0'
    licenses: []
    src_download_link: https://github.com/grpc/grpc/archive/refs/tags/v1.62.0.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: grpcio
    target_sw: linux
    vendor: pip
    version: '1.62.0'
    web_url: https://grpc.io
  licenses:
  - Apache-2.0
  - BSD-2-Clause
  - BSD-3-Clause
  - GPL-1.0-or-later
  - GPL-2.0-only
  - ISC
  - Info-ZIP
  - MIT
  - MPL-2.0
  - OpenSSL
  - Zlib
  - apache-2.0 WITH llvm-exception
  - mit-old-style
  - mit-old-style-no-advert
  - openssl-nokia-psk-contribution
  - openssl-ssleay
  name: grpcio
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 1.62.0
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: ''
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: h5py+3.10.0
  additional_info:
    fossa-attribution:
      Description: The h5py package provides both a high- and low-level interface
        to the HDF5library from Python. The low-level interface is intended to be
        a completewrapping of the HDF5 API, while the high-level component supports  access
        toHDF5 files, datasets and groups using established Python and NumPy concepts.A
        strong emphasis on automatic conversion between Python (Numpy) datatypes anddata
        structures and their HDF5 equivalents vastly simplifies the process ofreading
        and writing data from Python.Wheels are provided for several popular platforms,
        with an included copy ofthe HDF5 library (usually the latest version when
        h5py is released).You can also `build h5py from source<https://docs.h5py.org/en/stable/build.html#source-installation>`_with
        any HDF5 stable release from version 1.10.4 onwards, although naturally newHDF5
        versions released after this version of h5py may not work.Odd-numbered minor
        versions of HDF5 (e.g. 1.13) are experimental, and may notbe supported.
      Package: h5py
      Source: pip
      Version: '3.10.0'
      Hash: ''
      licenses:
      - BSD-2-Clause
      - BSD-3-Clause
      - GPL-2.0-or-later
      - PSF-2.0
      - Python-2.0
      Title: h5py
      DownloadURL: https://files.pythonhosted.org/packages/37/fc/0b1825077a1c4c79a13984c59997e4b36702962df0bca420698f77b70b10/h5py-3.10.0.tar.gz
  bazaar:
    register: 'no'
    prim: 12/CTX1022515
    community_link: https://github.com/h5py/h5py
    community_name: https://github.com/h5py/h5py
    community_url: https://github.com/h5py/h5py
    component_comment: ''
    component_highlevel_description: HDF5 for Python -- The h5py package is a Pythonic
      interface to the HDF5 binary data format.
    component_name: h5py
    component_platform: linux
    component_programing_language: Python
    component_version: '3.10.0'
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://github.com/h5py/h5py/archive/3.10.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1082916&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: h5py
    target_sw: linux
    vendor: pip
    version: '3.10.0'
    web_url: https://pypi.org/project/h5py/3.10.0/
  licenses:
  - BSD-2-Clause
  - BSD-3-Clause
  - GPL-2.0-or-later
  - PSF-2.0
  - Python-2.0
  name: h5py
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '3.10.0'
  mimer:
    linking: Static
    product_number: CTX1022515
    product_version_label: '3.10.0'
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: huggingface-hub+0.21.3
  additional_info:
    fossa-attribution:
      Description: "# `huggingface_hub`<a href=\"https://github.com/psf/black\"><img
        alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"></a><a
        href=\"https://codecov.io/gh/huggingface/huggingface_hub\"><img alt=\"Code
        coverage\" src=\"https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL\"></a><a
        href=\"https://github.com/huggingface/huggingface_hub/releases\"><img alt=\"GitHub
        release\" src=\"https://img.shields.io/github/release/huggingface/huggingface_hub.svg\"></a><a
        href=\"https://github.com/huggingface/huggingface_hub\"><img alt=\"Documentation\"
        src=\"https://img.shields.io/pypi/pyversions/huggingface_hub.svg\"></a><a
        href=\"https://huggingface.co/docs/huggingface_hub/index\"><img alt=\"Documentation\"
        src=\"https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc\"></a>##
        Welcome to the huggingface_hub libraryThe `huggingface_hub` is a client library
        to interact with the Hugging Face Hub. The Hugging Face Hub is a platform
        with over 35K models, 4K datasets, and 2K demos in which people can easily
        collaborate in their ML workflows. The Hub works as a central place where
        anyone can share, explore, discover, and experiment with open-source Machine
        Learning.With `huggingface_hub`, you can easily download and upload models,
        datasets, and Spaces. You can extract useful information from the Hub, and
        do much more. Some example use cases:* Downloading and caching files from
        a Hub repository.* Creating repositories and uploading an updated model every
        few epochs.* Extract metadata from all models that match certain criteria
        (e.g. models for `text-classification`).* List all files from a specific repository.Read
        all about it in [the library documentation](https://huggingface.co/docs/huggingface_hub).<br>##
        Integrating to the Hub.We're partnering with cool open source ML libraries
        to provide free model hosting and versioning. You can find the existing integrations
        [here](https://huggingface.co/docs/hub/libraries).The advantages are:- Free
        model or dataset hosting for libraries and their users.- Built-in file versioning,
        even with very large files, thanks to a git-based approach.- Hosted inference
        API for all models publicly available.- In-browser widgets to play with the
        uploaded models.- Anyone can upload a new model for your library, they just
        need to add the corresponding tag for the model to be discoverable.- Fast
        downloads! We use Cloudfront (a CDN) to geo-replicate downloads so they're
        blazing fast from anywhere on the globe.- Usage stats and more features to
        come.If you would like to integrate your library, feel free to open an issue
        to begin the discussion. We wrote a [step-by-step guide](https://huggingface.co/docs/hub/adding-a-library)
        with \u2764\uFE0F showing how to do this integration.<br>## Feedback (feature
        requests, bugs, etc.) is super welcome \U0001F499\U0001F49A\U0001F49B\U0001F49C\u2665\uFE0F\U0001F9E1"
      Package: huggingface-hub
      Source: pip
      Version: '0.21.3'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: huggingface-hub
      DownloadURL: https://files.pythonhosted.org/packages/47/8f/cf6683de320cf3873850ba48b7383db96958fe435b8e227db92119f6d867/huggingface_hub-0.21.3-py3-none-any.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/huggingface/huggingface_hub
    community_name: https://github.com/huggingface/huggingface_hub
    community_url: https://github.com/huggingface/huggingface_hub
    component_comment: ''
    component_highlevel_description: ''
    component_name: huggingface-hub
    component_platform: linux
    component_programing_language: ''
    component_version: v0.21.3
    licenses: []
    src_download_link: https://github.com/huggingface/huggingface_hub/archive/refs/tags/v0.21.3.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: huggingface-hub
    target_sw: linux
    vendor: pip
    version: 0.21.3
    web_url: https://github.com/huggingface/huggingface_hub
  licenses:
  - Apache-2.0
  name: huggingface-hub
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 0.21.3
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: ''
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: humanfriendly+10.0
  additional_info:
    fossa-attribution:
      Description: "humanfriendly: Human friendly input/output in Python====================================================..
        image:: https://travis-ci.org/xolox/python-humanfriendly.svg?branch=master
        \  :target: https://travis-ci.org/xolox/python-humanfriendly.. image:: https://coveralls.io/repos/github/xolox/python-humanfriendly/badge.svg?branch=master
        \  :target: https://coveralls.io/github/xolox/python-humanfriendly?branch=masterThe
        functions and classes in the `humanfriendly` package can be used to maketext
        interfaces more user friendly. Some example features:- Parsing and formatting
        numbers, file sizes, pathnames and timespans in  simple, human friendly formats.-
        Easy to use timers for long running operations, with human friendly  formatting
        of the resulting timespans.- Prompting the user to select a choice from a
        list of options by typing the  option's number or a unique substring of the
        option.- Terminal interaction including text styling (ANSI escape sequences),
        user  friendly rendering of usage messages and querying the terminal for its
        \ size.The `humanfriendly` package is currently tested on Python 2.7, 3.5+
        and PyPy(2.7) on Linux and macOS. While the intention is to support Windows
        as well,you may encounter some rough edges... contents::   :local:Getting
        started---------------It's very simple to start using the `humanfriendly`
        package::   >>> import humanfriendly   >>> user_input = raw_input(\"Enter
        a readable file size: \")   Enter a readable file size: 16G   >>> num_bytes
        = humanfriendly.parse_size(user_input)   >>> print num_bytes   16000000000
        \  >>> print \"You entered:\", humanfriendly.format_size(num_bytes)   You
        entered: 16 GB   >>> print \"You entered:\", humanfriendly.format_size(num_bytes,
        binary=True)   You entered: 14.9 GiBCommand line------------.. A DRY solution
        to avoid duplication of the `humanfriendly --help' text:.... [[[cog.. from
        humanfriendly.usage import inject_usage.. inject_usage('humanfriendly.cli')..
        ]]]**Usage:** `humanfriendly [OPTIONS]`Human friendly input/output (text formatting)
        on the commandline based on the Python package with the same name.**Supported
        options:**.. csv-table::   :header: Option, Description   :widths: 30, 70
        \  \"``-c``, ``--run-command``\",\"Execute an external command (given as the
        positional arguments) and render   a spinner and timer while the command is
        running. The exit status of the   command is propagated.\"   ``--format-table``,\"Read
        tabular data from standard input (each line is a row and each   whitespace
        separated field is a column), format the data as a table and   print the resulting
        table to standard output. See also the ``--delimiter``   option.\"   \"``-d``,
        ``--delimiter=VALUE``\",\"Change the delimiter used by ``--format-table``
        to ``VALUE`` (a string). By default   all whitespace is treated as a delimiter.\"
        \  \"``-l``, ``--format-length=LENGTH``\",\"Convert a length count (given
        as the integer or float ``LENGTH``) into a human   readable string and print
        that string to standard output.\"   \"``-n``, ``--format-number=VALUE``\",\"Format
        a number (given as the integer or floating point number ``VALUE``) with   thousands
        separators and two decimal places (if needed) and print the   formatted number
        to standard output.\"   \"``-s``, ``--format-size=BYTES``\",\"Convert a byte
        count (given as the integer ``BYTES``) into a human readable   string and
        print that string to standard output.\"   \"``-b``, ``--binary``\",\"Change
        the output of ``-s``, ``--format-size`` to use binary multiples of bytes   (base-2)
        instead of the default decimal multiples of bytes (base-10).\"   \"``-t``,
        ``--format-timespan=SECONDS``\",\"Convert a number of seconds (given as the
        floating point number ``SECONDS``)   into a human readable timespan and print
        that string to standard output.\"   ``--parse-length=VALUE``,\"Parse a human
        readable length (given as the string ``VALUE``) and print the   number of
        metres to standard output.\"   ``--parse-size=VALUE``,\"Parse a human readable
        data size (given as the string ``VALUE``) and print the   number of bytes
        to standard output.\"   ``--demo``,\"Demonstrate changing the style and color
        of the terminal font using ANSI   escape sequences.\"   \"``-h``, ``--help``\",Show
        this message and exit... [[[end]]]A note about size units-----------------------When
        I originally published the `humanfriendly` package I went with binarymultiples
        of bytes (powers of two). It was pointed out several times that thiswas a
        poor choice (see issue `#4`_ and pull requests `#8`_ and `#9`_) and thusthe
        new default became decimal multiples of bytes (powers of ten):+------+---------------+---------------+|
        Unit | Binary value  | Decimal value |+------+---------------+---------------+|
        KB   |          1024 |          1000 ++------+---------------+---------------+|
        MB   |       1048576 |       1000000 |+------+---------------+---------------+|
        GB   |    1073741824 |    1000000000 |+------+---------------+---------------+|
        TB   | 1099511627776 | 1000000000000 |+------+---------------+---------------+|
        etc  |               |               |+------+---------------+---------------+The
        option to use binary multiples of bytes remains by passing the keywordargument
        `binary=True` to the `format_size()`_ and `parse_size()`_ functions.Windows
        support---------------Windows 10 gained native support for ANSI escape sequences
        which means commandslike ``humanfriendly --demo`` should work out of the box
        (if your system isup-to-date enough). If this doesn't work then you can install
        the colorama_package, it will be used automatically once installed.Contact-------The
        latest version of `humanfriendly` is available on PyPI_ and GitHub_. Thedocumentation
        is hosted on `Read the Docs`_ and includes a changelog_. For bugreports please
        create an issue on GitHub_. If you have questions, suggestions,etc. feel free
        to send me an e-mail at `peter@peterodding.com`_.License-------This software
        is licensed under the `MIT license`_.\xA9 2020 Peter Odding... External references:..
        _#4: https://github.com/xolox/python-humanfriendly/issues/4.. _#8: https://github.com/xolox/python-humanfriendly/pull/8..
        _#9: https://github.com/xolox/python-humanfriendly/pull/9.. _changelog: https://humanfriendly.readthedocs.io/en/latest/changelog.html..
        _colorama: https://pypi.org/project/colorama.. _format_size(): https://humanfriendly.readthedocs.io/en/latest/#humanfriendly.format_size..
        _GitHub: https://github.com/xolox/python-humanfriendly.. _MIT license: https://en.wikipedia.org/wiki/MIT_License..
        _parse_size(): https://humanfriendly.readthedocs.io/en/latest/#humanfriendly.parse_size..
        _peter@peterodding.com: peter@peterodding.com.. _PyPI: https://pypi.org/project/humanfriendly..
        _Read the Docs: https://humanfriendly.readthedocs.io"
      Package: humanfriendly
      Source: pip
      Version: '10.0'
      Hash: ''
      licenses:
      - MIT
      Title: humanfriendly
      DownloadURL: https://files.pythonhosted.org/packages/f0/0f/310fb31e39e2d734ccaa2c0fb981ee41f7bd5056ce9bc29b2248bd569169/humanfriendly-10.0-py2.py3-none-any.whl
  bazaar:
    register: 'no'
    prim: 1/CTX1038803
    community_link: https://github.com/xolox/python-humanfriendly/
    community_name: https://github.com/xolox/python-humanfriendly/
    community_url: https://github.com/xolox/python-humanfriendly/
    component_comment: ''
    component_highlevel_description: Human friendly input/output for text interfaces
      using Python
    component_name: humanfriendly
    component_platform: linux
    component_programing_language: Python
    component_version: '10.0'
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://github.com/xolox/python-humanfriendly/archive/refs/tags/10.0.tar.gz
    stako_decision_reason: allowed
    stako: ESW4
    stako_comment: Inactive community
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1062828&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: humanfriendly
    target_sw: linux
    vendor: pip
    version: '10.0'
    web_url: https://humanfriendly.readthedocs.io
  licenses:
  - MIT
  name: humanfriendly
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '10.0'
  mimer:
    linking: Static
    product_number: CTX1038803
    product_version_label: '10.0'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: idna+3.6
  additional_info:
    fossa-attribution:
      Description: "Internationalized Domain Names in Applications (IDNA)=====================================================Support
        for the Internationalized Domain Names inApplications (IDNA) protocol as specified
        in `RFC 5891<https://tools.ietf.org/html/rfc5891>`_. This is the latest version
        ofthe protocol and is sometimes referred to as \u201CIDNA 2008\u201D.This
        library also provides support for Unicode TechnicalStandard 46, `Unicode IDNA
        Compatibility Processing<https://unicode.org/reports/tr46/>`_.This acts as
        a suitable replacement for the \u201Cencodings.idna\u201Dmodule that comes
        with the Python standard library, but whichonly supports the older superseded
        IDNA specification (`RFC 3490<https://tools.ietf.org/html/rfc3490>`_).Basic
        functions are simply executed:.. code-block:: pycon    >>> import idna    >>>
        idna.encode('\u30C9\u30E1\u30A4\u30F3.\u30C6\u30B9\u30C8')    b'xn--eckwd4c7c.xn--zckzah'
        \   >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))    \u30C9\u30E1\u30A4\u30F3.\u30C6\u30B9\u30C8Installation------------This
        package is available for installation from PyPI:.. code-block:: bash    $
        python3 -m pip install idnaUsage-----For typical usage, the ``encode`` and
        ``decode`` functions will take adomain name argument and perform a conversion
        to A-labels or U-labelsrespectively... code-block:: pycon    >>> import idna
        \   >>> idna.encode('\u30C9\u30E1\u30A4\u30F3.\u30C6\u30B9\u30C8')    b'xn--eckwd4c7c.xn--zckzah'
        \   >>> print(idna.decode('xn--eckwd4c7c.xn--zckzah'))    \u30C9\u30E1\u30A4\u30F3.\u30C6\u30B9\u30C8You
        may use the codec encoding and decoding methods using the``idna.codec`` module:..
        code-block:: pycon    >>> import idna.codec    >>> print('\u0434\u043E\u043C\u0435\u043D.\u0438\u0441\u043F\u044B\u0442\u0430\u043D\u0438\u0435'.encode('idna2008'))
        \   b'xn--d1acufc.xn--80akhbyknj4f'    >>> print(b'xn--d1acufc.xn--80akhbyknj4f'.decode('idna2008'))
        \   \u0434\u043E\u043C\u0435\u043D.\u0438\u0441\u043F\u044B\u0442\u0430\u043D\u0438\u0435Conversions
        can be applied at a per-label basis using the ``ulabel`` or``alabel`` functions
        if necessary:.. code-block:: pycon    >>> idna.alabel('\u6D4B\u8BD5')    b'xn--0zwm56d'Compatibility
        Mapping (UTS #46)+++++++++++++++++++++++++++++++As described in `RFC 5895
        <https://tools.ietf.org/html/rfc5895>`_, theIDNA specification does not normalize
        input from different potentialways a user may input a domain name. This functionality,
        known asa \u201Cmapping\u201D, is considered by the specification to be a
        localuser-interface issue distinct from IDNA conversion functionality.This
        library provides one such mapping that was developed by theUnicode Consortium.
        Known as `Unicode IDNA Compatibility Processing<https://unicode.org/reports/tr46/>`_,
        it provides for both a regularmapping for typical applications, as well as
        a transitional mapping tohelp migrate from older IDNA 2003 applications.For
        example, \u201CK\xF6nigsg\xE4\xDFchen\u201D is not a permissible label as
        *LATINCAPITAL LETTER K* is not allowed (nor are capital letters in general).UTS
        46 will convert this into lower case prior to applying the IDNAconversion...
        code-block:: pycon    >>> import idna    >>> idna.encode('K\xF6nigsg\xE4\xDFchen')
        \   ...    idna.core.InvalidCodepoint: Codepoint U+004B at position 1 of 'K\xF6nigsg\xE4\xDFchen'
        not allowed    >>> idna.encode('K\xF6nigsg\xE4\xDFchen', uts46=True)    b'xn--knigsgchen-b4a3dun'
        \   >>> print(idna.decode('xn--knigsgchen-b4a3dun'))    k\xF6nigsg\xE4\xDFchenTransitional
        processing provides conversions to help transition fromthe older 2003 standard
        to the current standard. For example, in theoriginal IDNA specification, the
        *LATIN SMALL LETTER SHARP S* (\xDF) wasconverted into two *LATIN SMALL LETTER
        S* (ss), whereas in the currentIDNA specification this conversion is not performed...
        code-block:: pycon    >>> idna.encode('K\xF6nigsg\xE4\xDFchen', uts46=True,
        transitional=True)    'xn--knigsgsschen-lcb0w'Implementers should use transitional
        processing with caution, only inrare cases where conversion from legacy labels
        to current labels must beperformed (i.e. IDNA implementations that pre-date
        2008). For typicalapplications that just need to convert labels, transitional
        processingis unlikely to be beneficial and could produce unexpected incompatibleresults.``encodings.idna``
        Compatibility++++++++++++++++++++++++++++++++Function calls from the Python
        built-in ``encodings.idna`` module aremapped to their IDNA 2008 equivalents
        using the ``idna.compat`` module.Simply substitute the ``import`` clause in
        your code to refer to the newmodule name.Exceptions----------All errors raised
        during the conversion following the specificationshould raise an exception
        derived from the ``idna.IDNAError`` baseclass.More specific exceptions that
        may be generated as ``idna.IDNABidiError``when the error reflects an illegal
        combination of left-to-right andright-to-left characters in a label; ``idna.InvalidCodepoint``
        whena specific codepoint is an illegal character in an IDN label (i.e.INVALID);
        and ``idna.InvalidCodepointContext`` when the codepoint isillegal based on
        its positional context (i.e. it is CONTEXTO or CONTEXTJbut the contextual
        requirements are not satisfied.)Building and Diagnostics------------------------The
        IDNA and UTS 46 functionality relies upon pre-calculated lookuptables for
        performance. These tables are derived from computing againsteligibility criteria
        in the respective standards. These tables arecomputed using the command-line
        script ``tools/idna-data``.This tool will fetch relevant codepoint data from
        the Unicode repositoryand perform the required calculations to identify eligibility.
        There arethree main modes:* ``idna-data make-libdata``. Generates ``idnadata.py``
        and  ``uts46data.py``, the pre-calculated lookup tables used for IDNA and
        \ UTS 46 conversions. Implementers who wish to track this library against
        \ a different Unicode version may use this tool to manually generate a  different
        version of the ``idnadata.py`` and ``uts46data.py`` files.* ``idna-data make-table``.
        Generate a table of the IDNA disposition  (e.g. PVALID, CONTEXTJ, CONTEXTO)
        in the format found in Appendix  B.1 of RFC 5892 and the pre-computed tables
        published by `IANA  <https://www.iana.org/>`_.* ``idna-data U+0061``. Prints
        debugging output on the various  properties associated with an individual
        Unicode codepoint (in this  case, U+0061), that are used to assess the IDNA
        and UTS 46 status of a  codepoint. This is helpful in debugging or analysis.The
        tool accepts a number of arguments, described using ``idna-data-h``. Most
        notably, the ``--version`` argument allows the specificationof the version
        of Unicode to be used in computing the table data. Forexample, ``idna-data
        --version 9.0.0 make-libdata`` will generatelibrary data against Unicode 9.0.0.Additional
        Notes----------------* **Packages**. The latest tagged release version is
        published in the  `Python Package Index <https://pypi.org/project/idna/>`_.*
        **Version support**. This library supports Python 3.5 and higher.  As this
        library serves as a low-level toolkit for a variety of  applications, many
        of which strive for broad compatibility with older  Python versions, there
        is no rush to remove older interpreter support.  Removing support for older
        versions should be well justified in that the  maintenance burden has become
        too high.* **Python 2**. Python 2 is supported by version 2.x of this library.
        \ While active development of the version 2.x series has ended, notable  issues
        being corrected may be backported to 2.x. Use \"idna<3\" in your  requirements
        file if you need this library for a Python 2 application.* **Testing**. The
        library has a test suite based on each rule of the  IDNA specification, as
        well as tests that are provided as part of the  Unicode Technical Standard
        46, `Unicode IDNA Compatibility Processing  <https://unicode.org/reports/tr46/>`_.*
        **Emoji**. It is an occasional request to support emoji domains in  this library.
        Encoding of symbols like emoji is expressly prohibited by  the technical standard
        IDNA 2008 and emoji domains are broadly phased  out across the domain industry
        due to associated security risks. For  now, applications that need to support
        these non-compliant labels  may wish to consider trying the encode/decode
        operation in this library  first, and then falling back to using `encodings.idna`.
        See `the Github  project <https://github.com/kjd/idna/issues/18>`_ for more
        discussion."
      Package: idna
      Source: pip
      Version: '3.6'
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: idna
      DownloadURL: https://files.pythonhosted.org/packages/bf/3f/ea4b9117521a1e9c50344b909be7886dd00a519552724809bb1f486986c2/idna-3.6.tar.gz
  bazaar:
    register: 'no'
    prim: 14/CAX1058316
    community_link: https://github.com/kjd/idna
    community_name: https://github.com/kjd/idna
    community_url: https://github.com/kjd/idna
    component_comment: ''
    component_highlevel_description: Internationalized Domain Names for Python (IDNA
      2008 and UTS 46)
    component_name: idna
    component_platform: linux
    component_programing_language: Python
    component_version: V3.6
    licenses:
    - FAL1159003/2 (BSD 3-Clause "New" or "Revised" License (BSD-3-Cla)
    src_download_link: https://github.com/kjd/idna/archive/v3.6.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1072146&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: idna
    target_sw: linux
    vendor: pip
    version: '3.6'
    web_url: https://pypi.org/project/idna/3.7/
  licenses:
  - BSD-3-Clause
  name: idna
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '3.6'
  mimer:
    linking: Static
    product_number: CAX1058316
    product_version_label: V3.6
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: importlib-metadata+6.11.0
  additional_info:
    fossa-attribution:
      Description: '.. image:: https://img.shields.io/pypi/v/importlib_metadata.svg   :target:
        https://pypi.org/project/importlib_metadata.. image:: https://img.shields.io/pypi/pyversions/importlib_metadata.svg..
        image:: https://github.com/python/importlib_metadata/actions/workflows/main.yml/badge.svg   :target:
        https://github.com/python/importlib_metadata/actions?query=workflow%3A%22tests%22   :alt:
        tests.. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json    :target:
        https://github.com/astral-sh/ruff    :alt: Ruff.. image:: https://readthedocs.org/projects/importlib-metadata/badge/?version=latest   :target:
        https://importlib-metadata.readthedocs.io/en/latest/?badge=latest.. image::
        https://img.shields.io/badge/skeleton-2024-informational   :target: https://blog.jaraco.com/skeleton..
        image:: https://tidelift.com/badges/package/pypi/importlib-metadata   :target:
        https://tidelift.com/subscription/pkg/pypi-importlib-metadata?utm_source=pypi-importlib-metadata&utm_medium=readmeLibrary
        to access the metadata for a Python package.This package supplies third-party
        access to the functionality of`importlib.metadata <https://docs.python.org/3/library/importlib.metadata.html>`_including
        improvements added to subsequent Python versions.Compatibility=============New
        features are introduced in this third-party library and later mergedinto CPython.
        The following table indicates which versions of this librarywere contributed
        to different versions in the standard library:.. list-table::   :header-rows:
        1   * - importlib_metadata     - stdlib   * - 7.0     - 3.13   * - 6.5     -
        3.12   * - 4.13     - 3.11   * - 4.6     - 3.10   * - 1.4     - 3.8Usage=====See
        the `online documentation <https://importlib-metadata.readthedocs.io/>`_for
        usage details.`Finder authors<https://docs.python.org/3/reference/import.html#finders-and-loaders>`_
        canalso add support for custom package installers.  See the above documentationfor
        details.Caveats=======This project primarily supports third-party packages
        installed by PyPAtools (or other conforming packages). It does not support:-
        Packages in the stdlib.- Packages installed without metadata.Project details===============
        * Project home: https://github.com/python/importlib_metadata * Report bugs
        at: https://github.com/python/importlib_metadata/issues * Code hosting: https://github.com/python/importlib_metadata
        * Documentation: https://importlib-metadata.readthedocs.io/For Enterprise==============Available
        as part of the Tidelift Subscription.This project and the maintainers of thousands
        of other packages are working with Tidelift to deliver one enterprise subscription
        that covers all of the open source you use.`Learn more <https://tidelift.com/subscription/pkg/pypi-importlib-metadata?utm_source=pypi-importlib-metadata&utm_medium=referral&utm_campaign=github>`_.'
      Package: importlib-metadata
      Source: pip
      Version: '6.11.0'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: importlib-metadata
      DownloadURL: https://files.pythonhosted.org/packages/ee/eb/58c2ab27ee628ad801f56d4017fe62afab0293116f6d0b08f1d5bd46e06f/importlib_metadata-6.11.0.tar.gz
  bazaar:
    register: 'no'
    prim: 56/CTX1026316
    community_link: https://github.com/python/importlib_metadata
    community_name: https://github.com/python/importlib_metadata
    community_url: https://github.com/python/importlib_metadata
    component_comment: ''
    component_highlevel_description: Backport of the importlib.metadata module
    component_name: importlib_metadata
    component_platform: linux
    component_programing_language: Python
    component_version: V6.11.0
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/python/importlib_metadata/archive/v6.11.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1076002&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: importlib-metadata
    target_sw: linux
    vendor: pip
    version: '6.11.0'
    web_url: https://github.com/python/importlib_metadata
  licenses:
  - Apache-2.0
  name: importlib-metadata
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '6.11.0'
  mimer:
    linking: Static
    product_number: CTX1026316
    product_version_label: v6.11.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: Jinja2+3.1.3
  additional_info:
    fossa-attribution:
      Description: 'Jinja=====Jinja is a fast, expressive, extensible templating engine.
        Specialplaceholders in the template allow writing code similar to Pythonsyntax.
        Then the template is passed data to render the final document.It includes:-   Template
        inheritance and inclusion.-   Define and import macros within templates.-   HTML
        templates can use autoescaping to prevent XSS from untrusted    user input.-   A
        sandboxed environment can safely render untrusted templates.-   AsyncIO support
        for generating templates and calling async    functions.-   I18N support with
        Babel.-   Templates are compiled to optimized Python code just-in-time and    cached,
        or can be compiled ahead-of-time.-   Exceptions point to the correct line
        in templates to make debugging    easier.-   Extensible filters, tests, functions,
        and even syntax.Jinja''s philosophy is that while application logic belongs
        in Python ifpossible, it shouldn''t make the template designer''s job difficult
        byrestricting functionality too much.Installing----------Install and update
        using `pip`_:.. code-block:: text    $ pip install -U Jinja2.. _pip: https://pip.pypa.io/en/stable/getting-started/In
        A Nutshell-------------.. code-block:: jinja    {% extends "base.html" %}    {%
        block title %}Members{% endblock %}    {% block content %}      <ul>      {%
        for user in users %}        <li><a href="{{ user.url }}">{{ user.username
        }}</a></li>      {% endfor %}      </ul>    {% endblock %}Donate------The
        Pallets organization develops and supports Jinja and other popularpackages.
        In order to grow the community of contributors and users, andallow the maintainers
        to devote more time to the projects, `pleasedonate today`_... _please donate
        today: https://palletsprojects.com/donateLinks------   Documentation: https://jinja.palletsprojects.com/-   Changes:
        https://jinja.palletsprojects.com/changes/-   PyPI Releases: https://pypi.org/project/Jinja2/-   Source
        Code: https://github.com/pallets/jinja/-   Issue Tracker: https://github.com/pallets/jinja/issues/-   Chat:
        https://discord.gg/pallets'
      Package: Jinja2
      Source: pip
      Version: '3.1.3'
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: Jinja2
      DownloadURL: https://files.pythonhosted.org/packages/b2/5e/3a21abf3cd467d7876045335e681d276ac32492febe6d98ad89562d1a7e1/Jinja2-3.1.3.tar.gz
  bazaar:
    register: 'no'
    prim: 25/CAX1054904
    community_link: https://github.com/pallets/jinja
    community_name: https://github.com/pallets/jinja
    community_url: https://github.com/pallets/jinja
    component_comment: ''
    component_highlevel_description: Jinja2 is a template engine written in pure Python.
      It provides a Django inspired non-XML syntax but supports inline expressions
      and an optional sandboxed environment.
    component_name: jinja
    component_platform: linux
    component_programing_language: Python
    component_version: '3.1.3'
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://github.com/pallets/jinja/archive/3.1.3.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1074820&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: 'NO'
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: Jinja2
    target_sw: linux
    vendor: pip
    version: '3.1.3'
    web_url: https://palletsprojects.com/p/jinja/
  licenses:
  - BSD-3-Clause
  name: Jinja2
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '3.1.3'
  mimer:
    linking: Static
    product_number: CAX1054904
    product_version_label: '3.1.3'
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: keras+2.15.0
  additional_info:
    fossa-attribution:
      Description: '# Keras 3: Deep Learning for HumansKeras 3 is a multi-backend
        deep learning framework, with support for JAX, TensorFlow, and PyTorch.Effortlessly
        build and train models for computer vision, natural language processing, audio
        processing,timeseries forecasting, recommender systems, etc.- **Accelerated
        model development**: Ship deep learning solutions faster thanks to the high-level
        UX of Kerasand the availability of easy-to-debug runtimes like PyTorch or
        JAX eager execution.- **State-of-the-art performance**: By picking the backend
        that is the fastest for your model architecture (often JAX!),leverage speedups
        ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).-
        **Datacenter-scale training**: Scale confidently from your laptop to large
        clusters of GPUs or TPUs.Join nearly three million developers, from burgeoning
        startups to global enterprises, in harnessing the power of Keras 3.## Installation###
        Install with pipKeras 3 is available on PyPI as `keras`. Note that Keras 2
        remains available as the `tf-keras` package.1. Install `keras`:```pip install
        keras --upgrade```2. Install backend package(s).To use `keras`, you should
        also install the backend of choice: `tensorflow`, `jax`, or `torch`.Note that
        `tensorflow` is required for using certain Keras 3 features: certain preprocessing
        layersas well as `tf.data` pipelines.### Local installation#### Minimal installationKeras
        3 is compatible with Linux and MacOS systems. For Windows users, we recommend
        using WSL2 to run Keras.To install a local development version:1. Install
        dependencies:```pip install -r requirements.txt```2. Run installation command
        from the root directory.```python pip_build.py --install```3. Run API generation
        script when creating PRs that update `keras_export` public APIs:```./shell/api_gen.sh```####
        Adding GPU supportThe `requirements.txt` file will install a CPU-only version
        of TensorFlow, JAX, and PyTorch. For GPU support, we alsoprovide a separate
        `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These
        install all CUDAdependencies via `pip` and expect a NVIDIA driver to be pre-installed.
        We recommend a clean python environment for eachbackend to avoid CUDA version
        mismatches. As an example, here is how to create a Jax GPU environment with
        `conda`:```shellconda create -y -n keras-jax python=3.10conda activate keras-jaxpip
        install -r requirements-jax-cuda.txtpython pip_build.py --install```## Configuring
        your backendYou can export the environment variable `KERAS_BACKEND` or you
        can edit your local config file at `~/.keras/keras.json`to configure your
        backend. Available backend options are: `"tensorflow"`, `"jax"`, `"torch"`.
        Example:```export KERAS_BACKEND="jax"```In Colab, you can do:```pythonimport
        osos.environ["KERAS_BACKEND"] = "jax"import keras```**Note:** The backend
        must be configured before importing `keras`, and the backend cannot be changed
        after the package has been imported.## Backwards compatibilityKeras 3 is intended
        to work as a drop-in replacement for `tf.keras` (when using the TensorFlow
        backend). Just take yourexisting `tf.keras` code, make sure that your calls
        to `model.save()` are using the up-to-date `.keras` format, and you''redone.If
        your `tf.keras` model does not include custom components, you can start running
        it on top of JAX or PyTorch immediately.If it does include custom components
        (e.g. custom layers or a custom `train_step()`), it is usually possible to
        convert itto a backend-agnostic implementation in just a few minutes.In addition,
        Keras models can consume datasets in any format, regardless of the backend
        you''re using:you can train your models with your existing `tf.data.Dataset`
        pipelines or PyTorch `DataLoaders`.## Why use Keras 3?- Run your high-level
        Keras workflows on top of any framework -- benefiting at will from the advantages
        of each framework,e.g. the scalability and performance of JAX or the production
        ecosystem options of TensorFlow.- Write custom components (e.g. layers, models,
        metrics) that you can use in low-level workflows in any framework.    - You
        can take a Keras model and train it in a training loop written from scratch
        in native TF, JAX, or PyTorch.    - You can take a Keras model and use it
        as part of a PyTorch-native `Module` or as part of a JAX-native model function.-
        Make your ML code future-proof by avoiding framework lock-in.- As a PyTorch
        user: get access to power and usability of Keras, at last!- As a JAX user:
        get access to a fully-featured, battle-tested, well-documented modeling and
        training library.Read more in the [Keras 3 release announcement](https://keras.io/keras_3/).'
      Package: keras
      Source: pip
      Version: '2.15.0'
      Hash: ''
      licenses:
      - Apache-2.0
      - CC-BY-SA-3.0
      Title: keras
      DownloadURL: https://files.pythonhosted.org/packages/fc/a7/0d4490de967a67f68a538cc9cdb259bff971c4b5787f7765dc7c8f118f71/keras-2.15.0-py3-none-any.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/keras-team/keras
    community_name: https://github.com/keras-team/keras
    community_url: https://github.com/keras-team/keras
    component_comment: ''
    component_highlevel_description: ''
    component_name: keras
    component_platform: linux
    component_programing_language: ''
    component_version: v2.15.0
    licenses: []
    src_download_link: https://github.com/keras-team/keras/archive/refs/tags/v2.15.0.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: keras
    target_sw: linux
    vendor: pip
    version: '2.15.0'
    web_url: https://github.com/keras-team/keras
  licenses:
  - Apache-2.0
  - CC-BY-SA-3.0
  name: keras
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '2.15.0'
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: ''
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: libclang+16.0.6
  additional_info:
    fossa-attribution:
      Description: 'libclang-for-pip================[![PyPI](https://img.shields.io/pypi/v/libclang)](https://pypi.org/project/libclang)![Python](https://img.shields.io/pypi/pyversions/libclang)![Downloads](https://img.shields.io/pypi/dw/libclang)[![License](https://img.shields.io/pypi/l/libclang)](https://github.com/sighingnow/libclang/blob/master/LICENSE.TXT)[![Arch:
        x86\_64](https://img.shields.io/badge/arch-x86__64-brightgreen)](https://pypi.org/project/libclang/#files)[![Arch:
        aarch64](https://img.shields.io/badge/arch-aarch64-yellowgreen)](https://pypi.org/project/libclang/#files)[![Arch:
        arm](https://img.shields.io/badge/arch-arm-orange)](https://pypi.org/project/libclang/#files)[![Linux](https://github.com/sighingnow/libclang/workflows/libclang-linux-amd64/badge.svg)](https://github.com/sighingnow/libclang/actions/workflows/libclang-linux-amd64.yml)[![Linux
        Arm](https://github.com/sighingnow/libclang/workflows/libclang-linux-arm/badge.svg)](https://github.com/sighingnow/libclang/actions/workflows/libclang-linux-arm.yml)[![Linux
        AArch64](https://github.com/sighingnow/libclang/workflows/libclang-linux-aarch64/badge.svg)](https://github.com/sighingnow/libclang/actions/workflows/libclang-linux-aarch64.yml)[![Linux
        Alpine](https://github.com/sighingnow/libclang/workflows/libclang-alpine-amd64/badge.svg)](https://github.com/sighingnow/libclang/actions/workflows/libclang-alpine-amd64.yml)[![MacOS
        Intel](https://github.com/sighingnow/libclang/workflows/libclang-macosx-amd64/badge.svg)](https://github.com/sighingnow/libclang/actions/workflows/libclang-macosx-amd64.yml)[![MacOS
        M1](https://img.shields.io/cirrus/github/sighingnow/libclang?label=libclang-macosx-arm64)](https://cirrus-ci.com/github/sighingnow/libclang)[![Windows](https://github.com/sighingnow/libclang/workflows/libclang-windows-amd64/badge.svg)](https://github.com/sighingnow/libclang/actions/workflows/libclang-windows-amd64.yml)[![Windows
        AArch64](https://github.com/sighingnow/libclang/workflows/libclang-windows-aarch64/badge.svg)](https://github.com/sighingnow/libclang/actions/workflows/libclang-windows-aarch64.yml)The
        repository contains code taken from [the LLVM project][1], to make it easier
        to installclang''s python bindings.The repository copies necessary Python
        binding files from LLVM repo, adds packaging scriptsto make it a valid Python
        package and finally uploads the package to [pypi][2]. To make the libclangavailable
        without installing the LLVM toolkits, this package provides bundled static-linked
        libclangshared library for different platforms, which, should work well on
        OSX, Windows, as well asusual Linux distributions.The aim of this project
        is to make the `clang.cindex` (aka., Clang Python Bindings)available for more
        Python users, without setting up the LLVM environment. To install the package,you
        just need to run```bashpip install libclang```Note that the library is named
        `libclang`, the package `clang` on PyPi is another package anddoesn''t bundle
        the prebuilt shared library.Internals---------Update class variable `library_path`
        of `Config` in `cindex.py` as:```python    library_path = os.path.join(os.path.dirname(os.path.realpath(__file__)),
        ''native'')```License-------This repository follows the license agreement
        of the LLVM project, see [Apache-2.0 WITH LLVM-exception](./LICENSE.TXT).[1]:
        https://github.com/llvm/llvm-project/tree/main/clang/bindings/python[2]: https://pypi.org/project/libclang'
      Package: libclang
      Source: pip
      Version: 16.0.6
      Hash: ''
      licenses:
      - Apache-2.0
      - NCSA
      - apache-2.0 WITH llvm-exception
      Title: libclang
      DownloadURL: https://files.pythonhosted.org/packages/c2/01/ec65dffc8c94bd8cafd359a76992f3212a239a80ead05522995c105432b8/libclang-16.0.6.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/sighingnow/libclang
    community_name: https://github.com/sighingnow/libclang
    community_url: https://github.com/sighingnow/libclang
    component_comment: ''
    component_highlevel_description: ''
    component_name: libclang
    component_platform: linux
    component_programing_language: ''
    component_version: llvm-16.0.6
    licenses: []
    src_download_link: https://github.com/sighingnow/libclang/archive/refs/tags/llvm-16.0.6.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: libclang
    target_sw: linux
    vendor: pip
    version: 16.0.6
    web_url: https://github.com/sighingnow/libclang
  licenses:
  - Apache-2.0
  - NCSA
  - apache-2.0 WITH llvm-exception
  name: libclang
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 16.0.6
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 16.0.6
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: Markdown+3.5.2
  additional_info:
    fossa-attribution:
      Description: '[Python-Markdown][]===================[![Build Status][build-button]][build][![Coverage
        Status][codecov-button]][codecov][![Latest Version][mdversion-button]][md-pypi][![Python
        Versions][pyversion-button]][md-pypi][![BSD License][bsdlicense-button]][bsdlicense][![Code
        of Conduct][codeofconduct-button]][Code of Conduct][build-button]: https://github.com/Python-Markdown/markdown/workflows/CI/badge.svg?event=push[build]:
        https://github.com/Python-Markdown/markdown/actions?query=workflow%3ACI+event%3Apush[codecov-button]:
        https://codecov.io/gh/Python-Markdown/markdown/branch/master/graph/badge.svg[codecov]:
        https://codecov.io/gh/Python-Markdown/markdown[mdversion-button]: https://img.shields.io/pypi/v/Markdown.svg[md-pypi]:
        https://pypi.org/project/Markdown/[pyversion-button]: https://img.shields.io/pypi/pyversions/Markdown.svg[bsdlicense-button]:
        https://img.shields.io/badge/license-BSD-yellow.svg[bsdlicense]: https://opensource.org/licenses/BSD-3-Clause[codeofconduct-button]:
        https://img.shields.io/badge/code%20of%20conduct-contributor%20covenant-green.svg?style=flat-square[Code
        of Conduct]: https://github.com/Python-Markdown/markdown/blob/master/CODE_OF_CONDUCT.mdThis
        is a Python implementation of John Gruber''s [Markdown][].It is almost completely
        compliant with the reference implementation,though there are a few known issues.
        See [Features][] for informationon what exactly is supported and what is not.
        Additional features aresupported by the [Available Extensions][].[Python-Markdown]:
        https://Python-Markdown.github.io/[Markdown]: https://daringfireball.net/projects/markdown/[Features]:
        https://Python-Markdown.github.io#Features[Available Extensions]: https://Python-Markdown.github.io/extensionsDocumentation-------------```bashpip
        install markdown``````pythonimport markdownhtml = markdown.markdown(your_text_string)```For
        more advanced [installation] and [usage] documentation, see the `docs/` directoryof
        the distribution or the project website at <https://Python-Markdown.github.io/>.[installation]:
        https://python-markdown.github.io/install/[usage]: https://python-markdown.github.io/reference/See
        the change log at <https://python-markdown.github.io/changelog/>.Support-------You
        may report bugs, ask for help, and discuss various other issues on the [bug
        tracker][].[bug tracker]: https://github.com/Python-Markdown/markdown/issuesCode
        of Conduct---------------Everyone interacting in the Python-Markdown project''s
        code bases, issue trackers,and mailing lists is expected to follow the [Code
        of Conduct].'
      Package: Markdown
      Source: pip
      Version: 3.5.2
      Hash: ''
      licenses:
      - BSD-2-Clause
      - BSD-3-Clause
      - PIL
      Title: Markdown
      DownloadURL: https://files.pythonhosted.org/packages/11/28/c5441a6642681d92de56063fa7984df56f783d3f1eba518dc3e7a253b606/Markdown-3.5.2.tar.gz
  bazaar:
    register: 'no'
    prim: 19/CAX1057063
    community_link: https://github.com/Python-Markdown/markdown
    community_name: https://github.com/Python-Markdown/markdown
    community_url: https://github.com/Python-Markdown/markdown
    component_comment: ''
    component_highlevel_description: "A Python implementation of John Gruber\u2019s
      Markdown with Extension support."
    component_name: markdown
    component_platform: linux
    component_programing_language: Python
    component_version: 3.5.2
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://github.com/Python-Markdown/markdown/archive/3.5.2.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078376&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: Markdown
    target_sw: linux
    vendor: pip
    version: 3.5.2
    web_url: https://pypi.org/project/Markdown/3.6/
  licenses:
  - BSD-2-Clause
  - BSD-3-Clause
  - PIL
  name: Markdown
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 3.5.2
  mimer:
    linking: Static
    product_number: CAX1057063
    product_version_label: 3.5.2
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: MarkupSafe+2.1.5
  additional_info:
    fossa-attribution:
      Description: 'MarkupSafe==========MarkupSafe implements a text object that escapes
        characters so it issafe to use in HTML and XML. Characters that have special
        meanings arereplaced so that they display as the actual characters. This mitigatesinjection
        attacks, meaning untrusted user input can safely be displayedon a page.Installing----------Install
        and update using `pip`_:.. code-block:: text    pip install -U MarkupSafe..
        _pip: https://pip.pypa.io/en/stable/getting-started/Examples--------.. code-block::
        pycon    >>> from markupsafe import Markup, escape    >>> # escape replaces
        special characters and wraps in Markup    >>> escape("<script>alert(document.cookie);</script>")    Markup(''&lt;script&gt;alert(document.cookie);&lt;/script&gt;'')    >>>
        # wrap in Markup to mark text "safe" and prevent escaping    >>> Markup("<strong>Hello</strong>")    Markup(''<strong>hello</strong>'')    >>>
        escape(Markup("<strong>Hello</strong>"))    Markup(''<strong>hello</strong>'')    >>>
        # Markup is a str subclass    >>> # methods and operators escape their arguments    >>>
        template = Markup("Hello <em>{name}</em>")    >>> template.format(name=''"World"'')    Markup(''Hello
        <em>&#34;World&#34;</em>'')Donate------The Pallets organization develops and
        supports MarkupSafe and otherpopular packages. In order to grow the community
        of contributors andusers, and allow the maintainers to devote more time to
        the projects,`please donate today`_... _please donate today: https://palletsprojects.com/donateLinks------   Documentation:
        https://markupsafe.palletsprojects.com/-   Changes: https://markupsafe.palletsprojects.com/changes/-   PyPI
        Releases: https://pypi.org/project/MarkupSafe/-   Source Code: https://github.com/pallets/markupsafe/-   Issue
        Tracker: https://github.com/pallets/markupsafe/issues/-   Chat: https://discord.gg/pallets'
      Package: MarkupSafe
      Source: pip
      Version: 2.1.5
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: MarkupSafe
      DownloadURL: https://files.pythonhosted.org/packages/87/5b/aae44c6655f3801e81aa3eef09dbbf012431987ba564d7231722f68df02d/MarkupSafe-2.1.5.tar.gz
  bazaar:
    register: 'no'
    prim: 14/CAX1056401
    community_link: https://pypi.org/project/MarkupSafe/
    community_name: https://pypi.org/project/MarkupSafe/
    community_url: https://pypi.org/project/MarkupSafe/
    component_comment: ''
    component_highlevel_description: Implements a XML/HTML/XHTML Markup safe string
      for Python
    component_name: markupsafe
    component_platform: linux
    component_programing_language: Python
    component_version: 2.1.5
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/87/5b/aae44c6655f3801e81aa3eef09dbbf012431987ba564d7231722f68df02d/MarkupSafe-2.1.5.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1077792&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: MarkupSafe
    target_sw: linux
    vendor: pip
    version: 2.1.5
    web_url: https://palletsprojects.com/p/markupsafe/
  licenses:
  - BSD-3-Clause
  name: MarkupSafe
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 2.1.5
  mimer:
    linking: Static
    product_number: CAX1056401
    product_version_label: 2.1.5
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: ml-dtypes+0.2.0
  additional_info:
    fossa-attribution:
      Description: '# ml_dtypes[![Unittests](https://github.com/jax-ml/ml_dtypes/actions/workflows/test.yml/badge.svg)](https://github.com/jax-ml/ml_dtypes/actions/workflows/test.yml)[![Wheel
        Build](https://github.com/jax-ml/ml_dtypes/actions/workflows/wheels.yml/badge.svg)](https://github.com/jax-ml/ml_dtypes/actions/workflows/wheels.yml)[![PyPI
        version](https://badge.fury.io/py/ml_dtypes.svg)](https://badge.fury.io/py/ml_dtypes)`ml_dtypes`
        is a stand-alone implementation of several NumPy dtype extensions used in
        machine learning libraries, including:- [`bfloat16`](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format):  an
        alternative to the standard [`float16`](https://en.wikipedia.org/wiki/Half-precision_floating-point_format)
        format- `float8_*`: several experimental 8-bit floating point representations  including:  *
        `float8_e4m3b11fnuz`  * `float8_e4m3fn`  * `float8_e4m3fnuz`  * `float8_e5m2`  *
        `float8_e5m2fnuz`- `int4` and `uint4`: low precision integer types.See below
        for specifications of these number formats.## InstallationThe `ml_dtypes`
        package is tested with Python versions 3.9-3.12, and can be installedwith
        the following command:```pip install ml_dtypes```To test your installation,
        you can run the following:```pip install absl-py pytestpytest --pyargs ml_dtypes```To
        build from source, clone the repository and run:```git submodule initgit submodule
        updatepip install .```## Example Usage```python>>> from ml_dtypes import bfloat16>>>
        import numpy as np>>> np.zeros(4, dtype=bfloat16)array([0, 0, 0, 0], dtype=bfloat16)```Importing
        `ml_dtypes` also registers the data types with numpy, so that they maybe referred
        to by their string name:```python>>> np.dtype(''bfloat16'')dtype(bfloat16)>>>
        np.dtype(''float8_e5m2'')dtype(float8_e5m2)```## Specifications of implemented
        floating point formats### `bfloat16`A `bfloat16` number is a single-precision
        float truncated at 16 bits.Exponent: 8, Mantissa: 7, exponent bias: 127. IEEE
        754, with NaN and inf.### `float8_e4m3b11fnuz`Exponent: 4, Mantissa: 3, bias:
        11.Extended range: no inf, NaN represented by 0b1000''0000.### `float8_e4m3fn`Exponent:
        4, Mantissa: 3, bias: 7.Extended range: no inf, NaN represented by 0bS111''1111.The
        `fn` suffix is for consistency with the corresponding LLVM/MLIR type, signaling
        this type is not consistent with IEEE-754.  The `f` indicates it is finite
        values only. The `n` indicates it includes NaNs, but only at the outer range.###
        `float8_e4m3fnuz`8-bit floating point with 3 bit mantissa.An 8-bit floating
        point type with 1 sign bit, 4 bits exponent and 3 bits mantissa. The suffix
        `fnuz` is consistent with LLVM/MLIR naming and is derived from the differences
        to IEEE floating point conventions. `F` is for "finite" (no infinities), `N`
        for with special NaN encoding, `UZ` for unsigned zero.This type has the following
        characteristics: * bit encoding: S1E4M3 - `0bSEEEEMMM` * exponent bias: 8
        * infinities: Not supported * NaNs: Supported with sign bit set to 1, exponent
        bits and mantissa bits set to all 0s - `0b10000000` * denormals when exponent
        is 0### `float8_e5m2`Exponent: 5, Mantissa: 2, bias: 15. IEEE 754, with NaN
        and inf.### `float8_e5m2fnuz`8-bit floating point with 2 bit mantissa.An 8-bit
        floating point type with 1 sign bit, 5 bits exponent and 2 bits mantissa.
        The suffix `fnuz` is consistent with LLVM/MLIR naming and is derived from
        the differences to IEEE floating point conventions. `F` is for "finite" (no
        infinities), `N` for with special NaN encoding, `UZ` for unsigned zero.This
        type has the following characteristics: * bit encoding: S1E5M2 - `0bSEEEEEMM`
        * exponent bias: 16 * infinities: Not supported * NaNs: Supported with sign
        bit set to 1, exponent bits and mantissa bits set to all 0s - `0b10000000`
        * denormals when exponent is 0## `int4` and `uint4`4-bit integer types, where
        each element is represented unpacked (i.e., padded upto a byte in memory).NumPy
        does not support types smaller than a single byte. For example, thedistance
        between adjacent elements in an array (`.strides`) is expressed inbytes. Relaxing
        this restriction would be a considerable engineering project.The `int4` and
        `uint4` types therefore use an unpacked representation, whereeach element
        of the array is padded up to a byte in memory. The lower four bitsof each
        byte contain the representation of the number, whereas the upper fourbits
        are ignored.## Quirks of low-precision ArithmeticIf you''re exploring the
        use of low-precision dtypes in your code, you should becareful to anticipate
        when the precision loss might lead to surprising results.One example is the
        behavior of aggregations like `sum`; consider this `bfloat16`summation in
        NumPy (run with version 1.24.2):```python>>> from ml_dtypes import bfloat16>>>
        import numpy as np>>> rng = np.random.default_rng(seed=0)>>> vals = rng.uniform(size=10000).astype(bfloat16)>>>
        vals.sum()256```The true sum should be close to 5000, but numpy returns exactly
        256: this isbecause `bfloat16` does not have the precision to increment `256`
        by values less than`1`:```python>>> bfloat16(256) + bfloat16(1)256```After
        256, the next representable value in bfloat16 is 258:```python>>> np.nextafter(bfloat16(256),
        bfloat16(np.inf))258```For better results you can specify that the accumulation
        should happen in ahigher-precision type like `float32`:```python>>> vals.sum(dtype=''float32'').astype(bfloat16)4992```In
        contrast to NumPy, projects like [JAX](http://jax.readthedocs.io/) which supportlow-precision
        arithmetic more natively will often do these kinds of higher-precisionaccumulations
        automatically:```python>>> import jax.numpy as jnp>>> jnp.array(vals).sum()Array(4992,
        dtype=bfloat16)```## License*This is not an officially supported Google product.*The
        `ml_dtypes` source code is licensed under the Apache 2.0 license(see [LICENSE](LICENSE)).
        Pre-compiled wheels are built with the[EIGEN](https://eigen.tuxfamily.org/)
        project, which is released under theMPL 2.0 license (see [LICENSE.eigen](LICENSE.eigen)).'
      Package: ml-dtypes
      Source: pip
      Version: 0.2.0
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-3-Clause
      - MPL-2.0
      - proprietary-license
      Title: ml-dtypes
      DownloadURL: https://github.com/jax-ml/ml_dtypes/archive/refs/tags/v0.2.0.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/jax-ml/ml_dtypes
    community_name: https://github.com/jax-ml/ml_dtypes
    community_url: https://github.com/jax-ml/ml_dtypes
    component_comment: ''
    component_highlevel_description: ''
    component_name: ml-dtypes
    component_platform: linux
    component_programing_language: ''
    component_version: v0.2.0
    licenses: []
    src_download_link: https://github.com/jax-ml/ml_dtypes/archive/refs/tags/v0.2.0.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: ml-dtypes
    target_sw: linux
    vendor: pip
    version: 0.2.0
    web_url: https://pypi.org/project/ml-dtypes/0.4.0/
  licenses:
  - Apache-2.0
  - BSD-3-Clause
  - MPL-2.0
  - proprietary-license
  name: ml-dtypes
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 0.2.0
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 0.2.0
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: mpmath+1.3.0
  additional_info:
    fossa-attribution:
      Description: 'mpmath======|pypi version| |Build status| |Code coverage status|
        |Zenodo Badge|.. |pypi version| image:: https://img.shields.io/pypi/v/mpmath.svg   :target:
        https://pypi.python.org/pypi/mpmath.. |Build status| image:: https://github.com/fredrik-johansson/mpmath/workflows/test/badge.svg   :target:
        https://github.com/fredrik-johansson/mpmath/actions?workflow=test.. |Code
        coverage status| image:: https://codecov.io/gh/fredrik-johansson/mpmath/branch/master/graph/badge.svg   :target:
        https://codecov.io/gh/fredrik-johansson/mpmath.. |Zenodo Badge| image:: https://zenodo.org/badge/2934512.svg   :target:
        https://zenodo.org/badge/latestdoi/2934512A Python library for arbitrary-precision
        floating-point arithmetic.Website: http://mpmath.org/Main author: Fredrik
        Johansson <fredrik.johansson@gmail.com>Mpmath is free software released under
        the New BSD License (see theLICENSE file for details)0. History and credits----------------------The
        following people (among others) have contributed major patchesor new features
        to mpmath:* Pearu Peterson <pearu.peterson@gmail.com>* Mario Pernici <mario.pernici@mi.infn.it>*
        Ondrej Certik <ondrej@certik.cz>* Vinzent Steinberg <vinzent.steinberg@gmail.cm>*
        Nimish Telang <ntelang@gmail.com>* Mike Taschuk <mtaschuk@ece.ualberta.ca>*
        Case Van Horsen <casevh@gmail.com>* Jorn Baayen <jorn.baayen@gmail.com>* Chris
        Smith <smichr@gmail.com>* Juan Arias de Reyna <arias@us.es>* Ioannis Tziakos
        <itziakos@gmail.com>* Aaron Meurer <asmeurer@gmail.com>* Stefan Krastanov
        <krastanov.stefan@gmail.com>* Ken Allen <ken.allen@sbcglobal.net>* Timo Hartmann
        <thartmann15@gmail.com>* Sergey B Kirpichev <skirpichev@gmail.com>* Kris Kuhlman
        <kristopher.kuhlman@gmail.com>* Paul Masson <paulmasson@analyticphysics.com>*
        Michael Kagalenko <michael.kagalenko@gmail.com>* Jonathan Warner <warnerjon12@gmail.com>*
        Max Gaukler <max.gaukler@fau.de>* Guillermo Navas-Palencia <g.navas.palencia@gmail.com>*
        Nike Dattani <nike@hpqc.org>Numerous other people have contributed by reporting
        bugs,requesting new features, or suggesting improvements to thedocumentation.For
        a detailed changelog, including individual contributions,see the CHANGES file.Fredrik''s
        work on mpmath during summer 2008 was sponsored by Googleas part of the Google
        Summer of Code program.Fredrik''s work on mpmath during summer 2009 was sponsored
        by theAmerican Institute of Mathematics under the support of the National
        ScienceFoundation Grant No. 0757627 (FRG: L-functions and Modular Forms).Any
        opinions, findings, and conclusions or recommendations expressed in thismaterial
        are those of the author(s) and do not necessarily reflect theviews of the
        sponsors.Credit also goes to:* The authors of the GMP library and the Python
        wrapper  gmpy, enabling mpmath to become much faster at  high precision* The
        authors of MPFR, pari/gp, MPFUN, and other arbitrary-  precision libraries,
        whose documentation has been helpful  for implementing many of the algorithms
        in mpmath* Wikipedia contributors; Abramowitz & Stegun; Gradshteyn & Ryzhik;  Wolfram
        Research for MathWorld and the Wolfram Functions site.  These are the main
        references used for special functions  implementations.* George Brandl for
        developing the Sphinx documentation tool  used to build mpmath''s documentationRelease
        history:* Version 1.3.0 released on March 7, 2023* Version 1.2.0 released
        on February 1, 2021* Version 1.1.0 released on December 11, 2018* Version
        1.0.0 released on September 27, 2017* Version 0.19 released on June 10, 2014*
        Version 0.18 released on December 31, 2013* Version 0.17 released on February
        1, 2011* Version 0.16 released on September 24, 2010* Version 0.15 released
        on June 6, 2010* Version 0.14 released on February 5, 2010* Version 0.13 released
        on August 13, 2009* Version 0.12 released on June 9, 2009* Version 0.11 released
        on January 26, 2009* Version 0.10 released on October 15, 2008* Version 0.9
        released on August 23, 2008* Version 0.8 released on April 20, 2008* Version
        0.7 released on March 12, 2008* Version 0.6 released on January 13, 2008*
        Version 0.5 released on November 24, 2007* Version 0.4 released on November
        3, 2007* Version 0.3 released on October 5, 2007* Version 0.2 released on
        October 2, 2007* Version 0.1 released on September 27, 20071. Download & installation--------------------------Mpmath
        requires Python 2.7 or 3.5 (or later versions). It has been testedwith CPython
        2.7, 3.5 through 3.7 and for PyPy.The latest release of mpmath can be downloaded
        from the mpmathwebsite and from https://github.com/fredrik-johansson/mpmath/releasesIt
        should also be available in the Python Package Index athttps://pypi.python.org/pypi/mpmathTo
        install latest release of Mpmath with pip, simply run``pip install mpmath``Or
        unpack the mpmath archive and run``python setup.py install``Mpmath can also
        be installed using``python -m easy_install mpmath``The latest development
        code is available fromhttps://github.com/fredrik-johansson/mpmathSee the main
        documentation for more detailed instructions.2. Running tests----------------The
        unit tests in mpmath/tests/ can be run via the scriptruntests.py, but it is
        recommended to run them with py.test(https://pytest.org/), especiallyto generate
        more useful reports in case there are failures.You may also want to check
        out the demo scripts in the demodirectory.The master branch is automatically
        tested by Travis CI.3. Documentation----------------Documentation in reStructuredText
        format is available in thedoc directory included with the source package.
        These filesare human-readable, but can be compiled to prettier HTML usingthe
        build.py script (requires Sphinx, http://sphinx.pocoo.org/).See setup.txt
        in the documentation for more information.The most recent documentation is
        also available in HTML format:http://mpmath.org/doc/current/4. Known problems-----------------Mpmath
        is a work in progress. Major issues include:* Some functions may return incorrect
        values when given extremely  large arguments or arguments very close to singularities.*
        Directed rounding works for arithmetic operations. It is implemented  heuristically
        for other operations, and their results may be off by one  or two units in
        the last place (even if otherwise accurate).* Some IEEE 754 features are not
        available. Inifinities and NaN are  partially supported; denormal rounding
        is currently not available  at all.* The interface for switching precision
        and rounding is not finalized.  The current method is not threadsafe.5. Help
        and bug reports-----------------------General questions and comments can be
        sent to the mpmath mailinglist,mpmath@googlegroups.comYou can also report
        bugs and send patches to the mpmath issue tracker,https://github.com/fredrik-johansson/mpmath/issues'
      Package: mpmath
      Source: pip
      Version: 1.3.0
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: mpmath
      DownloadURL: https://files.pythonhosted.org/packages/e0/47/dd32fa426cc72114383ac549964eecb20ecfd886d1e5ccf5340b55b02f57/mpmath-1.3.0.tar.gz
  bazaar:
    register: 'no'
    prim: 2/CTX1035649
    community_link: https://pypi.org/project/mpmath/
    community_name: https://pypi.org/project/mpmath/
    community_url: https://pypi.org/project/mpmath/
    component_comment: ''
    component_highlevel_description: A Python library for arbitrary-precision floating-point
      arithmetic.
    component_name: mpmath
    component_platform: linux
    component_programing_language: Python
    component_version: 1.3.0
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/e0/47/dd32fa426cc72114383ac549964eecb20ecfd886d1e5ccf5340b55b02f57/mpmath-1.3.0.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Low community activity.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1062811&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: France
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: mpmath
    target_sw: linux
    vendor: pip
    version: 1.3.0
    web_url: http://mpmath.org/
  licenses:
  - BSD-3-Clause
  name: mpmath
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 1.3.0
  mimer:
    linking: Static
    product_number: CTX1035649
    product_version_label: 1.3.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: multidict+6.0.5
  additional_info:
    fossa-attribution:
      Description: '=========multidict=========.. image:: https://github.com/aio-libs/multidict/workflows/CI/badge.svg   :target:
        https://github.com/aio-libs/multidict/actions?query=workflow%3ACI   :alt:
        GitHub status for master branch.. image:: https://codecov.io/gh/aio-libs/multidict/branch/master/graph/badge.svg   :target:
        https://codecov.io/gh/aio-libs/multidict   :alt: Coverage metrics.. image::
        https://img.shields.io/pypi/v/multidict.svg   :target: https://pypi.org/project/multidict   :alt:
        PyPI.. image:: https://readthedocs.org/projects/multidict/badge/?version=latest   :target:
        http://multidict.aio-libs.org/en/latest/?badge=latest   :alt: Documentation..
        image:: https://img.shields.io/pypi/pyversions/multidict.svg   :target: https://pypi.org/project/multidict   :alt:
        Python versions.. image:: https://badges.gitter.im/Join%20Chat.svg   :target:
        https://gitter.im/aio-libs/Lobby   :alt: Chat on GitterMultidict is dict-like
        collection of *key-value pairs* where keymight occur more than once in the
        container.Introduction------------*HTTP Headers* and *URL query string* require
        specific data structure:*multidict*. It behaves mostly like a regular ``dict``
        but it may haveseveral *values* for the same *key* and *preserves insertion
        ordering*.The *key* is ``str`` (or ``istr`` for case-insensitive dictionaries).``multidict``
        has four multidict classes:``MultiDict``, ``MultiDictProxy``, ``CIMultiDict``and
        ``CIMultiDictProxy``.Immutable proxies (``MultiDictProxy`` and``CIMultiDictProxy``)
        provide a dynamic view for theproxied multidict, the view reflects underlying
        collection changes. Theyimplement the ``collections.abc.Mapping`` interface.Regular
        mutable (``MultiDict`` and ``CIMultiDict``) classesimplement ``collections.abc.MutableMapping``
        and allows them to changetheir own content.*Case insensitive* (``CIMultiDict``
        and``CIMultiDictProxy``) assume the *keys* are caseinsensitive, e.g.::   >>>
        dct = CIMultiDict(key=''val'')   >>> ''Key'' in dct   True   >>> dct[''Key'']   ''val''*Keys*
        should be ``str`` or ``istr`` instances.The library has optional C Extensions
        for speed.License-------Apache 2Library Installation--------------------..
        code-block:: bash   $ pip install multidictThe library is Python 3 only!PyPI
        contains binary wheels for Linux, Windows and MacOS.  If you want to install``multidict``
        on another operating system (or *Alpine Linux* inside a Docker) thetarball
        will be used to compile the library from source.  It requires a C compiler
        andPython headers to be installed.To skip the compilation, please use the
        `MULTIDICT_NO_EXTENSIONS` environment variable,e.g.:.. code-block:: bash   $
        MULTIDICT_NO_EXTENSIONS=1 pip install multidictPlease note, the pure Python
        (uncompiled) version is about 20-50 times slower depending onthe usage scenario!!!Changelog---------See
        `RTD page <http://multidict.aio-libs.org/en/latest/changes>`_.'
      Package: multidict
      Source: pip
      Version: 6.0.5
      Hash: ''
      licenses:
      - Apache-2.0
      Title: multidict
      DownloadURL: https://files.pythonhosted.org/packages/f9/79/722ca999a3a09a63b35aac12ec27dfa8e5bb3a38b0f857f7a1a209a88836/multidict-6.0.5.tar.gz
  bazaar:
    register: 'no'
    prim: 9/CTX1028735
    community_link: https://github.com/aio-libs/multidict
    community_name: https://github.com/aio-libs/multidict
    community_url: https://github.com/aio-libs/multidict
    component_comment: ''
    component_highlevel_description: The multidict implementation
    component_name: multidict
    component_platform: linux
    component_programing_language: Python
    component_version: 6.0.5
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/aio-libs/multidict/archive/v6.0.5.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078400&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: multidict
    target_sw: linux
    vendor: pip
    version: 6.0.5
    web_url: https://github.com/aio-libs/multidict
  licenses:
  - Apache-2.0
  name: multidict
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 6.0.5
  mimer:
    linking: Static
    product_number: CTX1028735
    product_version_label: v6.0.5
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: multiprocess+0.70.15
  additional_info:
    fossa-attribution:
      Description: '-----------------------------------------------------------------multiprocess:
        better multiprocessing and multithreading in Python-----------------------------------------------------------------About
        Multiprocess==================``multiprocess`` is a fork of ``multiprocessing``.
        ``multiprocess`` extends ``multiprocessing`` to provide enhanced serialization,
        using `dill`. ``multiprocess`` leverages ``multiprocessing`` to support the
        spawning of processes using the API of the Python standard library''s ``threading``
        module. ``multiprocessing`` has been distributed as part of the standard library
        since Python 2.6.``multiprocess`` is part of ``pathos``,  a Python framework
        for heterogeneous computing.``multiprocess`` is in active development, so
        any user feedback, bug reports, comments,or suggestions are highly appreciated.  A
        list of issues is located at https://github.com/uqfoundation/multiprocess/issues,
        with a legacy list maintained at https://uqfoundation.github.io/project/pathos/query.Major
        Features==============``multiprocess`` enables:    - objects to be transferred
        between processes using pipes or multi-producer/multi-consumer queues    -
        objects to be shared between processes using a server process or (for simple
        data) shared memory``multiprocess`` provides:    - equivalents of all the
        synchronization primitives in ``threading``    - a ``Pool`` class to facilitate
        submitting tasks to worker processes    - enhanced serialization, using ``dill``Current
        Release===============The latest released version of ``multiprocess`` is available
        from:    https://pypi.org/project/multiprocess``multiprocess`` is distributed
        under a 3-clause BSD license, and is a fork of ``multiprocessing``.Development
        Version===================You can get the latest development version with
        all the shiny new features at:    https://github.com/uqfoundationIf you have
        a new contribution, please submit a pull request.Installation============``multiprocess``
        can be installed with ``pip``::    $ pip install multiprocessFor Python 2,
        a C compiler is required to build the included extension module from source.
        Python 3 and binary installs do not require a C compiler.Requirements============``multiprocess``
        requires:    - ``python`` (or ``pypy``), **>=3.8**    - ``setuptools``, **>=42**    -
        ``dill``, **>=0.3.8**Basic Usage===========The ``multiprocess.Process`` class
        follows the API of ``threading.Thread``.For example ::    from multiprocess
        import Process, Queue    def f(q):        q.put(''hello world'')    if __name__
        == ''__main__'':        q = Queue()        p = Process(target=f, args=[q])        p.start()        print
        (q.get())        p.join()Synchronization primitives like locks, semaphores
        and conditions areavailable, for example ::    >>> from multiprocess import
        Condition    >>> c = Condition()    >>> print (c)    <Condition(<RLock(None,
        0)>), 0>    >>> c.acquire()    True    >>> print (c)    <Condition(<RLock(MainProcess,
        1)>), 0>One can also use a manager to create shared objects either in sharedmemory
        or in a server process, for example ::    >>> from multiprocess import Manager    >>>
        manager = Manager()    >>> l = manager.list(range(10))    >>> l.reverse()    >>>
        print (l)    [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]    >>> print (repr(l))    <Proxy[list]
        object at 0x00E1B3B0>Tasks can be offloaded to a pool of worker processes
        in various ways,for example ::    >>> from multiprocess import Pool    >>>
        def f(x): return x*x    ...    >>> p = Pool(4)    >>> result = p.map_async(f,
        range(10))    >>> print (result.get(timeout=1))    [0, 1, 4, 9, 16, 25, 36,
        49, 64, 81]When ``dill`` is installed, serialization is extended to most objects,for
        example ::    >>> from multiprocess import Pool    >>> p = Pool(4)    >>>
        print (p.map(lambda x: (lambda y:y**2)(x) + x, xrange(10)))    [0, 2, 6, 12,
        20, 30, 42, 56, 72, 90]More Information================Probably the best way
        to get started is to look at the documentation athttp://multiprocess.rtfd.io.
        Also see ``multiprocess.tests`` for scripts thatdemonstrate how ``multiprocess``
        can be used to leverge multiple processesto execute Python in parallel. You
        can run the test suite with``python -m multiprocess.tests``. As ``multiprocess``
        conforms to the``multiprocessing`` interface, the examples and documentation
        found athttp://docs.python.org/library/multiprocessing.html also apply to``multiprocess``
        if one will ``import multiprocessing as multiprocess``.See https://github.com/uqfoundation/multiprocess/tree/master/py3.12/examplesfor
        a set of examples that demonstrate some basic use cases and benchmarkingfor
        running Python code in parallel. Please feel free to submit a ticket ongithub,
        or ask a question on stackoverflow (**@Mike McKerns**). If you wouldlike to
        share how you use ``multiprocess`` in your work, please send an email(to **mmckerns
        at uqfoundation dot org**).Citation========If you use ``multiprocess`` to
        do research that leads to publication, we ask that youacknowledge use of ``multiprocess``
        by citing the following in your publication::    M.M. McKerns, L. Strand,
        T. Sullivan, A. Fang, M.A.G. Aivazis,    "Building a framework for predictive
        science", Proceedings of    the 10th Python in Science Conference, 2011;    http://arxiv.org/pdf/1202.1056    Michael
        McKerns and Michael Aivazis,    "pathos: a framework for heterogeneous computing",
        2010- ;    https://uqfoundation.github.io/project/pathosPlease see https://uqfoundation.github.io/project/pathos
        orhttp://arxiv.org/pdf/1202.1056 for further information.'
      Package: multiprocess
      Source: pip
      Version: 0.70.15
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: multiprocess
      DownloadURL: https://files.pythonhosted.org/packages/68/e0/a77ca96e772e13c828fa52f3ad370d413bef194aeaf78b7c6611870ad815/multiprocess-0.70.15.tar.gz
  bazaar:
    register: 'no'
    prim: 4/CTX1033019
    community_link: https://pypi.org/project/multiprocess
    community_name: https://pypi.org/project/multiprocess
    community_url: https://pypi.org/project/multiprocess
    component_comment: ''
    component_highlevel_description: better multiprocessing and multithreading in
      python
    component_name: multiprocess
    component_platform: linux
    component_programing_language: Python
    component_version: 0.70.15
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/68/e0/a77ca96e772e13c828fa52f3ad370d413bef194aeaf78b7c6611870ad815/multiprocess-0.70.15.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1054761&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: multiprocess
    target_sw: linux
    vendor: pip
    version: 0.70.15
    web_url: https://github.com/uqfoundation/multiprocess
  licenses:
  - BSD-3-Clause
  name: multiprocess
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 0.70.15
  mimer:
    linking: Static
    product_number: CTX1033019
    product_version_label: 0.70.15
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: networkx+3.2.1
  additional_info:
    fossa-attribution:
      Description: 'NetworkX========.. image:: https://github.com/networkx/networkx/workflows/test/badge.svg?branch=main  :target:
        https://github.com/networkx/networkx/actions?query=workflow%3A%22test%22..
        image:: https://codecov.io/gh/networkx/networkx/branch/main/graph/badge.svg   :target:
        https://app.codecov.io/gh/networkx/networkx/branch/main   .. image:: https://img.shields.io/github/labels/networkx/networkx/Good%20First%20Issue?color=green&label=Contribute%20&style=flat-square   :target:
        https://github.com/networkx/networkx/issues?q=is%3Aopen+is%3Aissue+label%3A%22Good+First+Issue%22   NetworkX
        is a Python package for the creation, manipulation,and study of the structure,
        dynamics, and functionsof complex networks.- **Website (including documentation):**
        https://networkx.org- **Mailing list:** https://groups.google.com/forum/#!forum/networkx-discuss-
        **Source:** https://github.com/networkx/networkx- **Bug reports:** https://github.com/networkx/networkx/issues-
        **Report a security vulnerability:** https://tidelift.com/security- **Tutorial:**
        https://networkx.org/documentation/latest/tutorial.html- **GitHub Discussions:**
        https://github.com/networkx/networkx/discussionsSimple example--------------Find
        the shortest path between two nodes in an undirected graph:.. code:: pycon    >>>
        import networkx as nx    >>> G = nx.Graph()    >>> G.add_edge("A", "B", weight=4)    >>>
        G.add_edge("B", "D", weight=2)    >>> G.add_edge("A", "C", weight=3)    >>>
        G.add_edge("C", "D", weight=4)    >>> nx.shortest_path(G, "A", "D", weight="weight")    [''A'',
        ''B'', ''D'']Install-------Install the latest version of NetworkX::    $ pip
        install networkxInstall with all optional dependencies::    $ pip install
        networkx[all]For additional details, please see `INSTALL.rst`.Bugs----Please
        report any bugs that you find `here <https://github.com/networkx/networkx/issues>`_.Or,
        even better, fork the repository on `GitHub <https://github.com/networkx/networkx>`_and
        create a pull request (PR). We welcome all changes, big or small, and wewill
        help you make the PR if you are new to `git` (just ask on the issue and/orsee
        `CONTRIBUTING.rst`).License-------Released under the 3-Clause BSD license
        (see `LICENSE.txt`)::   Copyright (C) 2004-2024 NetworkX Developers   Aric
        Hagberg <hagberg@lanl.gov>   Dan Schult <dschult@colgate.edu>   Pieter Swart
        <swart@lanl.gov>'
      Package: networkx
      Source: pip
      Version: 3.2.1
      Hash: ''
      licenses: []
      Title: networkx
  bazaar:
    register: 'no'
    prim: 1/CTX1040640
    community_link: https://pypi.org/project/networkx/
    community_name: https://pypi.org/project/networkx/
    community_url: https://pypi.org/project/networkx/
    component_comment: ''
    component_highlevel_description: Python package for creating and manipulating
      graphs and networks
    component_name: networkx
    component_platform: linux
    component_programing_language: Python
    component_version: 3.2.1
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/c4/80/a84676339aaae2f1cfdf9f418701dd634aef9cc76f708ef55c36ff39c3ca/networkx-3.2.1.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Low community activity
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078641&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: networkx
    target_sw: linux
    vendor: pip
    version: 3.2.1
    web_url: https://pypi.org/project/networkx/3.3/
  licenses: []
  name: networkx
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 3.2.1
  mimer:
    linking: Static
    product_number: CTX1040640
    product_version_label: 3.2.1
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: numpy+1.26.4
  additional_info:
    fossa-attribution:
      Description: "<h1 align=\"center\"><img src=\"https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg\"
        width=\"300\"></h1><br>[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)[![PyPI
        Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](https://pypi.org/project/numpy/)[![Conda
        Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](https://anaconda.org/conda-forge/numpy)[![Stack
        Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](https://stackoverflow.com/questions/tagged/numpy)[![Nature
        Paper](https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue)](https://doi.org/10.1038/s41586-020-2649-2)[![OpenSSF
        Scorecard](https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge)](https://api.securityscorecards.dev/projects/github.com/numpy/numpy)NumPy
        is the fundamental package for scientific computing with Python.- **Website:**
        https://www.numpy.org- **Documentation:** https://numpy.org/doc- **Mailing
        list:** https://mail.python.org/mailman/listinfo/numpy-discussion- **Source
        code:** https://github.com/numpy/numpy- **Contributing:** https://www.numpy.org/devdocs/dev/index.html-
        **Bug reports:** https://github.com/numpy/numpy/issues- **Report a security
        vulnerability:** https://tidelift.com/docs/securityIt provides:- a powerful
        N-dimensional array object- sophisticated (broadcasting) functions- tools
        for integrating C/C++ and Fortran code- useful linear algebra, Fourier transform,
        and random number capabilitiesTesting:NumPy requires `pytest` and `hypothesis`.
        \ Tests can then be run after installation with:    python -c \"import numpy,
        sys; sys.exit(numpy.test() is False)\"Code of Conduct----------------------NumPy
        is a community-driven open source project developed by a diverse group of[contributors](https://numpy.org/teams/).
        The NumPy leadership has made a strongcommitment to creating an open, inclusive,
        and positive community. Please read the[NumPy Code of Conduct](https://numpy.org/code-of-conduct/)
        for guidance on how to interactwith others in a way that makes our community
        thrive.Call for Contributions----------------------The NumPy project welcomes
        your expertise and enthusiasm!Small improvements or fixes are always appreciated.
        If you are considering larger contributionsto the source code, please contact
        us through the [mailinglist](https://mail.python.org/mailman/listinfo/numpy-discussion)
        first.Writing code isn\u2019t the only way to contribute to NumPy. You can
        also:- review pull requests- help us stay on top of new and old issues- develop
        tutorials, presentations, and other educational materials- maintain and improve
        [our website](https://github.com/numpy/numpy.org)- develop graphic design
        for our brand assets and promotional materials- translate website content-
        help with outreach and onboard new contributors- write grant proposals and
        help with other fundraising effortsFor more information about the ways you
        can contribute to NumPy, visit [our website](https://numpy.org/contribute/).
        If you\u2019re unsure where to start or how your skills fit in, reach out!
        You canask on the mailing list or here, on GitHub, by opening a new issue
        or leaving acomment on a relevant issue that is already open.Our preferred
        channels of communication are all public, but if you\u2019d like tospeak to
        us in private first, contact our community coordinators atnumpy-team@googlegroups.com
        or on Slack (write numpy-team@googlegroups.com foran invitation).We also have
        a biweekly community call, details of which are announced on themailing list.
        You are very welcome to join.If you are new to contributing to open source,
        [thisguide](https://opensource.guide/how-to-contribute/) helps explain why,
        what,and how to successfully get involved."
      Package: numpy
      Source: pip
      Version: 1.26.4
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-2-Clause
      - BSD-3-Clause
      - BSL-1.0
      - CC-BY-4.0
      - CC-BY-SA-3.0
      - CC-BY-SA-4.0
      - CC0-1.0
      - FSFAP
      - GPL-1.0-or-later
      - GPL-3.0-only
      - GPL-3.0-or-later
      - GPL-3.0-with-GCC-exception
      - LGPL-2.0-or-later
      - MIT
      - NCSA
      - OPL-1.0
      - SunPro
      - Zlib
      - bsd-3-clause-open-mpi
      - lgpl-2.1-nokia-qt
      - openpub
      - public-domain
      - unknown
      - zsh
      Title: numpy
      DownloadURL: https://files.pythonhosted.org/packages/65/6e/09db70a523a96d25e115e71cc56a6f9031e7b8cd166c1ac8438307c14058/numpy-1.26.4.tar.gz
  bazaar:
    register: 'no'
    prim: 73/CAX1057111
    community_link: https://pypi.org/project/numpy/
    community_name: https://pypi.org/project/numpy/
    community_url: https://pypi.org/project/numpy/
    component_comment: ''
    component_highlevel_description: The fundamental package for scientific computing
      with Python.
    component_name: numpy
    component_platform: linux
    component_programing_language: Python
    component_version: 1.26.4
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/65/6e/09db70a523a96d25e115e71cc56a6f9031e7b8cd166c1ac8438307c14058/numpy-1.26.4.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1077793&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: numpy
    target_sw: linux
    vendor: pip
    version: 1.26.4
    web_url: https://numpy.org
  licenses:
  - Apache-2.0
  - BSD-2-Clause
  - BSD-3-Clause
  - BSL-1.0
  - CC-BY-4.0
  - CC-BY-SA-3.0
  - CC-BY-SA-4.0
  - CC0-1.0
  - FSFAP
  - GPL-1.0-or-later
  - GPL-3.0-only
  - GPL-3.0-or-later
  - GPL-3.0-with-GCC-exception
  - LGPL-2.0-or-later
  - MIT
  - NCSA
  - OPL-1.0
  - SunPro
  - Zlib
  - bsd-3-clause-open-mpi
  - lgpl-2.1-nokia-qt
  - openpub
  - public-domain
  - unknown
  - zsh
  name: numpy
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 1.26.4
  mimer:
    linking: Static
    product_number: CAX1057111
    product_version_label: v1.26.4
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-cublas-cu12+12.1.3.1
  additional_info:
    fossa-attribution:
      Description: CUBLAS native runtime libraries
      Package: nvidia-cublas-cu12
      Source: pip
      Version: 12.1.3.1
      Hash: ''
      licenses: []
      Title: nvidia-cublas-cu12
      DownloadURL: https://files.pythonhosted.org/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-cublas-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 12.1.3.1
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-cublas-cu12
    target_sw: linux
    vendor: pip
    version: 12.1.3.1
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-cublas-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 12.1.3.1
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 12.1.3.1
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-cuda-cupti-cu12+12.1.105
  additional_info:
    fossa-attribution:
      Description: Provides libraries to enable third party tools using GPU profiling
        APIs.
      Package: nvidia-cuda-cupti-cu12
      Source: pip
      Version: 12.1.105
      Hash: ''
      licenses:
      - BSD-3-Clause
      - MIT
      - NCSA
      - nvidia-gov
      Title: nvidia-cuda-cupti-cu12
      DownloadURL: https://files.pythonhosted.org/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-cuda-cupti-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 12.1.105
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-cuda-cupti-cu12
    target_sw: linux
    vendor: pip
    version: 12.1.105
    web_url: https://developer.nvidia.com/cuda-zone
  licenses:
  - BSD-3-Clause
  - MIT
  - NCSA
  - nvidia-gov
  name: nvidia-cuda-cupti-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 12.1.105
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 12.1.105
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-cuda-nvrtc-cu12+12.1.105
  additional_info:
    fossa-attribution:
      Description: NVRTC native runtime libraries
      Package: nvidia-cuda-nvrtc-cu12
      Source: pip
      Version: 12.1.105
      Hash: ''
      licenses: []
      Title: nvidia-cuda-nvrtc-cu12
      DownloadURL: https://files.pythonhosted.org/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-cuda-nvrtc-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 12.1.105
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-cuda-nvrtc-cu12
    target_sw: linux
    vendor: pip
    version: 12.1.105
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-cuda-nvrtc-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 12.1.105
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 12.1.105
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-cuda-runtime-cu12+12.1.105
  additional_info:
    fossa-attribution:
      Description: CUDA Runtime native Libraries
      Package: nvidia-cuda-runtime-cu12
      Source: pip
      Version: 12.1.105
      Hash: ''
      licenses: []
      Title: nvidia-cuda-runtime-cu12
      DownloadURL: https://files.pythonhosted.org/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-cuda-runtime-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 12.1.105
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-cuda-runtime-cu12
    target_sw: linux
    vendor: pip
    version: 12.1.105
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-cuda-runtime-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 12.1.105
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 12.1.105
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-cudnn-cu12+8.9.2.26
  additional_info:
    fossa-attribution:
      Description: cuDNN runtime libraries containing primitives for deep neural networks.
      Package: nvidia-cudnn-cu12
      Source: pip
      Version: 8.9.2.26
      Hash: ''
      licenses: []
      Title: nvidia-cudnn-cu12
      DownloadURL: https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-cudnn-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 8.9.2.26
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-cudnn-cu12
    target_sw: linux
    vendor: pip
    version: 8.9.2.26
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-cudnn-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 8.9.2.26
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 8.9.2.26
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-cufft-cu12+11.0.2.54
  additional_info:
    fossa-attribution:
      Description: CUFFT native runtime libraries
      Package: nvidia-cufft-cu12
      Source: pip
      Version: 11.0.2.54
      Hash: ''
      licenses: []
      Title: nvidia-cufft-cu12
      DownloadURL: https://files.pythonhosted.org/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-cufft-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 11.0.2.54
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-cufft-cu12
    target_sw: linux
    vendor: pip
    version: 11.0.2.54
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-cufft-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 11.0.2.54
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 11.0.2.54
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-curand-cu12+10.3.2.106
  additional_info:
    fossa-attribution:
      Description: CURAND native runtime libraries
      Package: nvidia-curand-cu12
      Source: pip
      Version: 10.3.2.106
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: nvidia-curand-cu12
      DownloadURL: https://files.pythonhosted.org/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-curand-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 10.3.2.106
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-curand-cu12
    target_sw: linux
    vendor: pip
    version: 10.3.2.106
    web_url: https://developer.nvidia.com/cuda-zone
  licenses:
  - BSD-3-Clause
  name: nvidia-curand-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 10.3.2.106
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 10.3.2.106
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-cusolver-cu12+11.4.5.107
  additional_info:
    fossa-attribution:
      Description: CUDA solver native runtime libraries
      Package: nvidia-cusolver-cu12
      Source: pip
      Version: 11.4.5.107
      Hash: ''
      licenses: []
      Title: nvidia-cusolver-cu12
      DownloadURL: https://files.pythonhosted.org/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-cusolver-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 11.4.5.107
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-cusolver-cu12
    target_sw: linux
    vendor: pip
    version: 11.4.5.107
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-cusolver-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 11.4.5.107
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 11.4.5.107
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-cusparse-cu12+12.1.0.106
  additional_info:
    fossa-attribution:
      Description: CUSPARSE native runtime libraries
      Package: nvidia-cusparse-cu12
      Source: pip
      Version: 12.1.0.106
      Hash: ''
      licenses: []
      Title: nvidia-cusparse-cu12
      DownloadURL: https://files.pythonhosted.org/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-cusparse-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 12.1.0.106
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-cusparse-cu12
    target_sw: linux
    vendor: pip
    version: 12.1.0.106
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-cusparse-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 12.1.0.106
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 12.1.0.106
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-nccl-cu12+2.19.3
  additional_info:
    fossa-attribution:
      Description: NCCL (pronounced "Nickel") is a stand-alone library of standard
        collective communication routines for GPUs, implementing all-reduce, all-gather,
        reduce, broadcast, and reduce-scatter. It has been optimized to achieve high
        bandwidth on any platform using PCIe, NVLink, NVswitch, as well as networking
        using InfiniBand Verbs or TCP/IP sockets.
      Package: nvidia-nccl-cu12
      Source: pip
      Version: 2.19.3
      Hash: ''
      licenses: []
      Title: nvidia-nccl-cu12
      DownloadURL: https://github.com/NVIDIA/nccl/archive/refs/tags/v2.19.3-1.zip
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/NVIDIA/nccl
    community_name: https://github.com/NVIDIA/nccl
    community_url: https://github.com/NVIDIA/nccl
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-nccl-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 2.19.3
    licenses: []
    src_download_link: https://github.com/NVIDIA/nccl/archive/refs/tags/v2.19.3-1.zip
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-nccl-cu12
    target_sw: linux
    vendor: pip
    version: 2.19.3
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-nccl-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 2.19.3
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 2.19.3
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-nvjitlink-cu12+12.3.101
  additional_info:
    fossa-attribution:
      Description: NVIDIA compiler library for JIT LTO functionality.
      Package: nvidia-nvjitlink-cu12
      Source: pip
      Version: 12.3.101
      Hash: ''
      licenses: []
      Title: nvidia-nvjitlink-cu12
      DownloadURL: https://files.pythonhosted.org/packages/1e/07/bf730d44c2fe1b676ad9cc2be5f5f861eb5d153fb6951987a2d6a96379a9/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://developer.nvidia.com/cuda-zone
    community_name: https://developer.nvidia.com/cuda-zone
    community_url: https://developer.nvidia.com/cuda-zone
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-nvjitlink-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 12.3.101
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/1e/07/bf730d44c2fe1b676ad9cc2be5f5f861eb5d153fb6951987a2d6a96379a9/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-nvjitlink-cu12
    target_sw: linux
    vendor: pip
    version: 12.3.101
    web_url: https://developer.nvidia.com/cuda-zone
  licenses: []
  name: nvidia-nvjitlink-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 12.3.101
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 12.3.101
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: nvidia-nvtx-cu12+12.1.105
  additional_info:
    fossa-attribution:
      Description: The fossa information is fetched from  FOSSA service. Don't Edit!
      Package: nvidia-nvtx-cu12
      Source: pip
      Version: 12.1.105
      Hash: ''
      licenses:
      - nvidia-gov
      Title: nvidia-nvtx-cu12
      DownloadURL: https://files.pythonhosted.org/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: MANDATORY_FOR_BAZAAR/SCAS_REGISTRATION
    community_name: MANDATORY_FOR_BAZAAR/SCAS_REGISTRATION
    community_url: MANDATORY_FOR_BAZAAR/SCAS_REGISTRATION
    component_comment: ''
    component_highlevel_description: ''
    component_name: nvidia-nvtx-cu12
    component_platform: linux
    component_programing_language: ''
    component_version: 12.1.105
    licenses: []
    src_download_link: https://files.pythonhosted.org/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: nvidia-nvtx-cu12
    target_sw: linux
    vendor: pip
    version: 12.1.105
    web_url: ''
  licenses:
  - nvidia-gov
  name: nvidia-nvtx-cu12
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 12.1.105
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 12.1.105
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: oauthlib+3.2.2
  additional_info:
    fossa-attribution:
      Description: 'OAuthLib - Python Framework for OAuth1 & OAuth2===============================================*A
        generic, spec-compliant, thorough implementation of the OAuth request-signinglogic
        for Python 3.6+.*.. image:: https://app.travis-ci.com/oauthlib/oauthlib.svg?branch=master  :target:
        https://app.travis-ci.com/oauthlib/oauthlib  :alt: Travis.. image:: https://coveralls.io/repos/oauthlib/oauthlib/badge.svg?branch=master  :target:
        https://coveralls.io/r/oauthlib/oauthlib  :alt: Coveralls.. image:: https://img.shields.io/pypi/pyversions/oauthlib.svg  :target:
        https://pypi.org/project/oauthlib/  :alt: Download from PyPI.. image:: https://img.shields.io/pypi/l/oauthlib.svg  :target:
        https://pypi.org/project/oauthlib/  :alt: License.. image:: https://app.fossa.io/api/projects/git%2Bgithub.com%2Foauthlib%2Foauthlib.svg?type=shield   :target:
        https://app.fossa.io/projects/git%2Bgithub.com%2Foauthlib%2Foauthlib?ref=badge_shield   :alt:
        FOSSA Status.. image:: https://img.shields.io/readthedocs/oauthlib.svg  :target:
        https://oauthlib.readthedocs.io/en/latest/index.html  :alt: Read the Docs..
        image:: https://badges.gitter.im/oauthlib/oauthlib.svg  :target: https://gitter.im/oauthlib/Lobby  :alt:
        Chat on Gitter.. image:: https://raw.githubusercontent.com/oauthlib/oauthlib/8d71b161fd145d11c40d55c9ab66ac134a303253/docs/logo/oauthlib-banner-700x192.png  :target:
        https://github.com/oauthlib/oauthlib/  :alt: OAuth + Python = OAuthlib Python
        FrameworkOAuth often seems complicated and difficult-to-implement. There are
        severalprominent libraries for handling OAuth requests, but they all suffer
        from one orboth of the following:1. They predate the `OAuth 1.0 spec`_, AKA
        RFC 5849.2. They predate the `OAuth 2.0 spec`_, AKA RFC 6749.3. They assume
        the usage of a specific HTTP request library... _`OAuth 1.0 spec`: https://tools.ietf.org/html/rfc5849..
        _`OAuth 2.0 spec`: https://tools.ietf.org/html/rfc6749OAuthLib is a framework
        which implements the logic of OAuth1 or OAuth2 withoutassuming a specific
        HTTP request object or web framework. Use it to graft OAuthclient support
        onto your favorite HTTP library, or provide support onto yourfavourite web
        framework. If you''re a maintainer of such a library, write a thinveneer on
        top of OAuthLib and get OAuth support for very little effort.Documentation--------------Full
        documentation is available on `Read the Docs`_. All contributions are verywelcome!
        The documentation is still quite sparse, please open an issue for whatyou''d
        like to know, or discuss it in our `Gitter community`_, or even better, send
        apull request!.. _`Gitter community`: https://gitter.im/oauthlib/Lobby.. _`Read
        the Docs`: https://oauthlib.readthedocs.io/en/latest/index.htmlInterested
        in making OAuth requests?------------------------------------Then you might
        be more interested in using `requests`_ which has OAuthLibpowered OAuth support
        provided by the `requests-oauthlib`_ library... _`requests`: https://github.com/requests/requests..
        _`requests-oauthlib`: https://github.com/requests/requests-oauthlibWhich web
        frameworks are supported?-----------------------------------The following
        packages provide OAuth support using OAuthLib.- For Django there is `django-oauth-toolkit`_,
        which includes `Django REST framework`_ support.- For Flask there is `flask-oauthlib`_
        and `Flask-Dance`_.- For Pyramid there is `pyramid-oauthlib`_.- For Bottle
        there is `bottle-oauthlib`_.If you have written an OAuthLib package that supports
        your favorite framework,please open a Pull Request, updating the documentation...
        _`django-oauth-toolkit`: https://github.com/evonove/django-oauth-toolkit..
        _`flask-oauthlib`: https://github.com/lepture/flask-oauthlib.. _`Django REST
        framework`: http://django-rest-framework.org.. _`Flask-Dance`: https://github.com/singingwolfboy/flask-dance..
        _`pyramid-oauthlib`: https://github.com/tilgovi/pyramid-oauthlib.. _`bottle-oauthlib`:
        https://github.com/thomsonreuters/bottle-oauthlibUsing OAuthLib? Please get
        in touch!------------------------------------Patching OAuth support onto an
        http request framework? Creating an OAuthprovider extension for a web framework?
        Simply using OAuthLib to Get Things Doneor to learn?No matter which we''d
        love to hear from you in our `Gitter community`_ or if you haveanything in
        particular you would like to have, change or comment on don''thesitate for
        a second to send a pull request or open an issue. We might be quitebusy and
        therefore slow to reply but we love feedback!Chances are you have run into
        something annoying that you wish there wasdocumentation for, if you wish to
        gain eternal fame and glory, and a drink if wehave the pleasure to run into
        each other, please send a docs pull request =).. _`Gitter community`: https://gitter.im/oauthlib/LobbyLicense-------OAuthLib
        is yours to use and abuse according to the terms of the BSD license.Check
        the LICENSE file for full details.Credits-------OAuthLib has been started
        and maintained several years by Idan Gazit and otheramazing `AUTHORS`_. Thanks
        to their wonderful work, the open-source `community`_creation has been possible
        and the project can stay active and reactive to usersrequests... _`AUTHORS`:
        https://github.com/oauthlib/oauthlib/blob/master/AUTHORS.. _`community`: https://github.com/oauthlib/Changelog---------*OAuthLib
        is in active development, with the core of both OAuth1 and OAuth2completed,
        for providers as well as clients.* See `supported features`_ fordetails...
        _`supported features`: https://oauthlib.readthedocs.io/en/latest/feature_matrix.htmlFor
        a full changelog see ``CHANGELOG.rst``.'
      Package: oauthlib
      Source: pip
      Version: 3.2.2
      Hash: ''
      licenses:
      - BSD-2-Clause
      - BSD-3-Clause
      Title: oauthlib
      DownloadURL: https://files.pythonhosted.org/packages/6d/fa/fbf4001037904031639e6bfbfc02badfc7e12f137a8afa254df6c4c8a670/oauthlib-3.2.2.tar.gz
  bazaar:
    register: 'no'
    prim: 11/CAX1057905
    community_link: https://pypi.org/project/oauthlib
    community_name: https://pypi.org/project/oauthlib
    community_url: https://pypi.org/project/oauthlib
    component_comment: ''
    component_highlevel_description: A generic, spec-compliant, thorough implementation
      of the OAuth request-signing logic
    component_name: oauthlib, Python
    component_platform: linux
    component_programing_language: Python
    component_version: 3.2.2
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/6d/fa/fbf4001037904031639e6bfbfc02badfc7e12f137a8afa254df6c4c8a670/oauthlib-3.2.2.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1027074&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: oauthlib
    target_sw: linux
    vendor: pip
    version: 3.2.2
    web_url: https://github.com/oauthlib/oauthlib
  licenses:
  - BSD-2-Clause
  - BSD-3-Clause
  name: oauthlib
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 3.2.2
  mimer:
    linking: Static
    product_number: CAX1057905
    product_version_label: 3.2.2
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: onnx+1.15.0
  additional_info:
    fossa-attribution:
      Description: '<!--Copyright (c) ONNX Project Contributors--><!--- SPDX-License-Identifier:
        Apache-2.0 --><p align="center"><img width="40%" src="https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png"
        /></p>[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)[![Build
        Status](https://dev.azure.com/onnx-pipelines/onnx/_apis/build/status/Windows-CI?branchName=main&label=Windows)](https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=5&branchName=main)[![Build
        Status](https://dev.azure.com/onnx-pipelines/onnx/_apis/build/status/Linux-CI?branchName=main&label=Linux)](https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=7&branchName=main)[![Build
        Status](https://dev.azure.com/onnx-pipelines/onnx/_apis/build/status/MacOS-CI?branchName=main&label=MacOS)](https://dev.azure.com/onnx-pipelines/onnx/_build/latest?definitionId=6&branchName=main)[![CII
        Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)[![OpenSSF
        Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)[![REUSE
        compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[Open
        Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that
        empowers AI developersto choose the right tools as their project evolves.
        ONNX provides an open source format for AI models, both deep learning and
        traditional ML. It defines an extensible computation graph model, as well
        as definitions of built-in operators and standarddata types. Currently we
        focus on the capabilities needed for inferencing (scoring).ONNX is [widely
        supported](http://onnx.ai/supported-tools) and can be found in many frameworks,
        tools, and hardware. Enabling interoperability between different frameworks
        and streamlining the path from research to production helps increase the speed
        of innovation in the AI community. We invite the community to join us and
        further evolve ONNX.# Use ONNX* [Documentation of ONNX Python Package](https://onnx.ai/onnx/)*
        [Tutorials for creating ONNX models](https://github.com/onnx/tutorials)* [Pre-trained
        ONNX models](https://github.com/onnx/models)# Learn about the ONNX spec* [Overview](https://github.com/onnx/onnx/blob/main/docs/Overview.md)*
        [ONNX intermediate representation spec](https://github.com/onnx/onnx/blob/main/docs/IR.md)*
        [Versioning principles of the spec](https://github.com/onnx/onnx/blob/main/docs/Versioning.md)*
        [Operators documentation](https://github.com/onnx/onnx/blob/main/docs/Operators.md)*
        [Operators documentation](https://onnx.ai/onnx/operators/index.html) (latest
        release)* [Python API Overview](https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md)#
        Programming utilities for working with ONNX Graphs* [Shape and Type Inference](https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md)*
        [Graph Optimization](https://github.com/onnx/optimizer)* [Opset Version Conversion](https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/version_converter.md)#
        ContributeONNX is a community project and the open governance model is described
        [here](https://github.com/onnx/onnx/blob/main/community/readme.md). We encourage
        you to join the effort and contribute feedback, ideas, and code. You can participate
        in the [Special Interest Groups](https://github.com/onnx/onnx/blob/main/community/sigs.md)
        and [Working Groups](https://github.com/onnx/onnx/blob/main/community/working-groups.md)
        to shape the future of ONNX.Check out our [contribution guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md)
        to get started.If you think some operator should be added to ONNX specification,
        please read[this document](https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md).#
        Community meetingsThe schedules of the regular meetings of the Steering Committee,
        the working groups and the SIGs can be found [here](https://onnx.ai/calendar)Community
        Meetups are held at least once a year. Content from previous community meetups
        are at:* 2020.04.09 <https://wiki.lfaidata.foundation/display/DL/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+April+9>*
        2020.10.14 <https://wiki.lfaidata.foundation/display/DL/LF+AI+Day+-+ONNX+Community+Workshop+-+October+14>*
        2021.03.24 <https://wiki.lfaidata.foundation/pages/viewpage.action?pageId=35160391>*
        2021.10.21 <https://wiki.lfaidata.foundation/pages/viewpage.action?pageId=46989689>*
        2022.06.24 <https://wiki.lfaidata.foundation/display/DL/ONNX+Community+Day+-+June+24>*
        2023.06.28 <https://wiki.lfaidata.foundation/display/DL/ONNX+Community+Day+2023+-+June+28>#
        DiscussWe encourage you to open [Issues](https://github.com/onnx/onnx/issues),
        or use [Slack](https://lfaifoundation.slack.com/) (If you have not joined
        yet, please use this [link](https://join.slack.com/t/lfaifoundation/shared_invite/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA)
        to join the group) for more real-time discussion.# Follow UsStay up to date
        with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)]
        [[Twitter](https://twitter.com/onnxai)]# RoadmapA roadmap process takes place
        every year. More details can be found [here](https://github.com/onnx/steering-committee/tree/main/roadmap)#
        Installation## Official Python packagesONNX released packages are published
        in PyPi.```shpip install onnx  # or pip install onnx[reference] for optional
        reference implementation dependencies```[ONNX weekly packages](https://pypi.org/project/onnx-weekly/)
        are published in PyPI to enable experimentation and early testing.## vcpkg
        packagesonnx is in the maintenance list of [vcpkg](https://github.com/microsoft/vcpkg),
        you can easily use vcpkg to build and install it.```shgit clone https://github.com/microsoft/vcpkg.gitcd
        vcpkg./bootstrap-vcpkg.bat # For powershell./bootstrap-vcpkg.sh # For bash./vcpkg
        install onnx```## Conda packagesA binary build of ONNX is available from [Conda](https://conda.io),
        in [conda-forge](https://conda-forge.org/):```shconda install -c conda-forge
        onnx```## Build ONNX from SourceBefore building from source uninstall any
        existing versions of onnx `pip uninstall onnx`.c++17 or higher C++ compiler
        version is required to build ONNX from source. Still, users can specify their
        own `CMAKE_CXX_STANDARD` version for building ONNX.If you don''t have protobuf
        installed, ONNX will internally download and build protobuf for ONNX build.Or,
        you can manually install [protobuf C/C++ libraries and tools](https://github.com/protocolbuffers/protobuf)
        with specified version before proceeding forward. Then depending on how you
        installed protobuf, you need to set environment variable CMAKE_ARGS to "-DONNX_USE_PROTOBUF_SHARED_LIBS=ON"
        or "-DONNX_USE_PROTOBUF_SHARED_LIBS=OFF".  For example, you may need to run
        the following command:Linux:```shexport CMAKE_ARGS="-DONNX_USE_PROTOBUF_SHARED_LIBS=ON"```Windows:```batset
        CMAKE_ARGS="-DONNX_USE_PROTOBUF_SHARED_LIBS=ON"```The ON/OFF depends on what
        kind of protobuf library you have. Shared libraries are files ending with
        \*.dll/\*.so/\*.dylib. Static libraries are files ending with \*.a/\*.lib.
        This option depends on how you get your protobuf library and how it was built.
        And it is default OFF. You don''t need to run the commands above if you''d
        prefer to use a static protobuf library.### WindowsIf you are building ONNX
        from source, it is recommended that you also build Protobuf locally as a static
        library. The version distributed with conda-forge is a DLL, but ONNX expects
        it to be a static library. Building protobuf locally also lets you control
        the version of protobuf. The tested and recommended version is 3.21.12.The
        instructions in this README assume you are using Visual Studio.  It is recommended
        that you run all the commands from a shell started from "x64 Native Tools
        Command Prompt for VS 2019" and keep the build system generator for cmake
        (e.g., cmake -G "Visual Studio 16 2019") consistent while building protobuf
        as well as ONNX.You can get protobuf by running the following commands:```batgit
        clone https://github.com/protocolbuffers/protobuf.gitcd protobufgit checkout
        v21.12cd cmakecmake -G "Visual Studio 16 2019" -A x64 -DCMAKE_INSTALL_PREFIX=<protobuf_install_dir>
        -Dprotobuf_MSVC_STATIC_RUNTIME=OFF -Dprotobuf_BUILD_SHARED_LIBS=OFF -Dprotobuf_BUILD_TESTS=OFF
        -Dprotobuf_BUILD_EXAMPLES=OFF .msbuild protobuf.sln /m /p:Configuration=Releasemsbuild
        INSTALL.vcxproj /p:Configuration=Release```Then it will be built as a static
        library and installed to <protobuf_install_dir>. Please add the bin directory(which
        contains protoc.exe) to your PATH.```batset PATH=<protobuf_install_dir>/bin;%PATH%```Please
        note: if your protobuf_install_dir contains spaces, **do not** add quotation
        marks around it.Alternative: if you don''t want to change your PATH, you can
        set ONNX_PROTOC_EXECUTABLE instead.```batset CMAKE_ARGS=-DONNX_PROTOC_EXECUTABLE=<full_path_to_protoc.exe>```Then
        you can build ONNX as:```git clone https://github.com/onnx/onnx.gitcd onnxgit
        submodule update --init --recursive# prefer lite protoset CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ONpip
        install -e .```### LinuxFirst, you need to install protobuf. The minimum Protobuf
        compiler (protoc) version required by ONNX is 3.6.1. Please note that old
        protoc versions might not work with `CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON`.Ubuntu
        20.04 (and newer) users may choose to install protobuf via```shapt-get install
        python3-pip python3-dev libprotobuf-dev protobuf-compiler```In this case,
        it is required to add `-DONNX_USE_PROTOBUF_SHARED_LIBS=ON` to CMAKE_ARGS in
        the ONNX build step.A more general way is to build and install it from source.
        See the instructions below for more details.<details>  <summary> Installing
        Protobuf from source </summary>  Debian/Ubuntu:  ```sh    git clone https://github.com/protocolbuffers/protobuf.git    cd
        protobuf    git checkout v21.12    git submodule update --init --recursive    mkdir
        build_source && cd build_source    cmake ../cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF
        -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_POSITION_INDEPENDENT_CODE=ON
        -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release    make -j$(nproc)    make
        install  ```  CentOS/RHEL/Fedora:  ```sh    git clone https://github.com/protocolbuffers/protobuf.git    cd
        protobuf    git checkout v21.12    git submodule update --init --recursive    mkdir
        build_source && cd build_source    cmake ../cmake  -DCMAKE_INSTALL_LIBDIR=lib64
        -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc
        -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release    make
        -j$(nproc)    make install  ```  Here "-DCMAKE_POSITION_INDEPENDENT_CODE=ON"
        is crucial. By default static libraries are built without "-fPIC" flag, they
        are not position independent code. But shared libraries must be position independent
        code. Python C/C++ extensions(like ONNX) are shared libraries. So if a static
        library was not built with "-fPIC", it can''t be linked to such a shared library.  Once
        build is successful, update PATH to include protobuf paths.</details>Then
        you can build ONNX as:```shgit clone https://github.com/onnx/onnx.gitcd onnxgit
        submodule update --init --recursive# Optional: prefer lite protoexport CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ONpip
        install -e .```### Mac```shexport NUM_CORES=`sysctl -n hw.ncpu`brew updatebrew
        install autoconf && brew install automakewget https://github.com/protocolbuffers/protobuf/releases/download/v21.12/protobuf-cpp-3.21.12.tar.gztar
        -xvf protobuf-cpp-3.21.12.tar.gzcd protobuf-3.21.12mkdir build_source && cd
        build_sourcecmake ../cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON
        -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Releasemake -j${NUM_CORES}make
        install```Once build is successful, update PATH to include protobuf paths.Then
        you can build ONNX as:```shgit clone --recursive https://github.com/onnx/onnx.gitcd
        onnx# Optional: prefer lite protoset CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ONpip
        install -e .```## Verify InstallationAfter installation, run```shpython -c
        "import onnx"```to verify it works.## Common Build OptionsFor full list refer
        to CMakeLists.txt### Environment variables* `USE_MSVC_STATIC_RUNTIME` should
        be 1 or 0, not ON or OFF. When set to 1 onnx links statically to runtime library.**Default**:
        `USE_MSVC_STATIC_RUNTIME=0`* `DEBUG` should be 0 or 1. When set to 1 onnx
        is built in debug mode. or debug versions of the dependencies, you need to
        open the [CMakeLists file](https://github.com/onnx/onnx/blob/main/CMakeLists.txt)
        and append a letter `d` at the end of the package name lines. For example,
        `NAMES protobuf-lite` would become `NAMES protobuf-lited`.**Default**: `Debug=0`###
        CMake variables* `ONNX_USE_PROTOBUF_SHARED_LIBS` should be `ON` or `OFF`.**Default**:
        `ONNX_USE_PROTOBUF_SHARED_LIBS=OFF USE_MSVC_STATIC_RUNTIME=0``ONNX_USE_PROTOBUF_SHARED_LIBS`
        determines how onnx links to protobuf libraries.  * When set to `ON` - onnx
        will dynamically link to protobuf shared libs, PROTOBUF_USE_DLLS will be defined
        as described [here](https://github.com/protocolbuffers/protobuf/blob/main/cmake/README.md#dlls-vs-static-linking),
        Protobuf_USE_STATIC_LIBS will be set to `OFF` and `USE_MSVC_STATIC_RUNTIME`
        must be 0.  * When set to `OFF` - onnx will link statically to protobuf, and
        Protobuf_USE_STATIC_LIBS will be set to `ON` (to force the use of the static
        libraries) and `USE_MSVC_STATIC_RUNTIME` can be `0` or `1`.* `ONNX_USE_LITE_PROTO`
        should be `ON` or `OFF`. When set to `ON` onnx uses lite protobuf instead
        of full protobuf.**Default**: `ONNX_USE_LITE_PROTO=OFF`* `ONNX_WERROR` should
        be `ON` or `OFF`. When set to `ON` warnings are treated as errors.**Default**:
        `ONNX_WERROR=OFF` in local builds, `ON` in CI and release pipelines.## Common
        Errors* Note: the `import onnx` command does not work from the source checkout
        directory; in this case you''ll see `ModuleNotFoundError: No module named
        ''onnx.onnx_cpp2py_export''`. Change into another directory to fix this error.*
        If you run into any issues while building Protobuf as a static library, please
        ensure that shared Protobuf libraries, like libprotobuf, are not installed
        on your device or in the conda environment. If these shared libraries exist,
        either remove them to build Protobuf from source as a static library, or skip
        the Protobuf build from source to use the shared version directly.* If you
        run into any issues while building ONNX from source, and your error message
        reads, `Could not find pythonXX.lib`, ensure that you have consistent Python
        versions for common commands, such as `python` and `pip`. Clean all existing
        build files and rebuild ONNX again.# TestingONNX uses [pytest](https://docs.pytest.org)
        as test driver. In order to run tests, you will first need to install `pytest`:```shpip
        install pytest nbval```After installing pytest, use the following command
        to run tests.```shpytest```# DevelopmentCheck out the [contributor guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md)
        for instructions.# License[Apache License v2.0](LICENSE)# Code of Conduct[ONNX
        Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)'
      Package: onnx
      Source: pip
      Version: 1.15.0
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-3-Clause
      - CC0-1.0
      - MIT
      - NCSA
      Title: onnx
      DownloadURL: https://github.com/onnx/onnx/archive/refs/tags/v1.15.0.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/onnx/onnx
    community_name: https://github.com/onnx/onnx
    community_url: https://github.com/onnx/onnx
    component_comment: ''
    component_highlevel_description: ''
    component_name: onnx
    component_platform: linux
    component_programing_language: ''
    component_version: v1.15.0
    licenses: []
    src_download_link: https://github.com/onnx/onnx/archive/refs/tags/v1.15.0.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: onnx
    target_sw: linux
    vendor: pip
    version: 1.15.0
    web_url: https://pypi.org/project/onnx/1.16.0/
  licenses:
  - Apache-2.0
  - BSD-3-Clause
  - CC0-1.0
  - MIT
  - NCSA
  name: onnx
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 1.15.0
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 1.15.0
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: onnxruntime+1.17.1
  additional_info:
    fossa-attribution:
      Description: 'ONNX Runtime============ONNX Runtime is a performance-focused
        scoring engine for Open Neural Network Exchange (ONNX) models.For more information
        on ONNX Runtime, please see `aka.ms/onnxruntime <https://aka.ms/onnxruntime/>`_
        or the `Github project <https://github.com/microsoft/onnxruntime/>`_.Changes-------1.17.1^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.11.17.0^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.01.16.0^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.16.01.15.0^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.15.01.14.0^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.14.01.13.0^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.13.01.12.0^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.12.01.11.0^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.11.01.10.0^^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.10.01.9.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.9.01.8.2^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.21.8.1^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.11.8.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.01.7.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.7.01.6.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.6.01.5.3^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.31.5.2^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.21.5.1^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.11.4.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.4.01.3.1^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.11.3.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.01.2.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.2.01.1.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.1.01.0.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.0.00.5.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.5.00.4.0^^^^^Release
        Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.4.0'
      Package: onnxruntime
      Source: pip
      Version: 1.17.1
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-2-Clause
      - BSD-3-Clause
      - BSL-1.0
      - ISC
      - JSON
      - MIT
      - MPL-2.0
      - NCSA
      - PIL
      - Protobuf
      - Unlicense
      - Zlib
      - bsd-3-clause-open-mpi
      - cc-pd
      - curl
      - proprietary-license
      Title: onnxruntime
      DownloadURL: https://github.com/microsoft/onnxruntime/archive/refs/tags/v1.17.1.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/microsoft/onnxruntime
    community_name: https://github.com/microsoft/onnxruntime
    community_url: https://github.com/microsoft/onnxruntime
    component_comment: ''
    component_highlevel_description: ''
    component_name: onnxruntime
    component_platform: linux
    component_programing_language: ''
    component_version: v1.17.1
    licenses: []
    src_download_link: https://github.com/microsoft/onnxruntime/archive/refs/tags/v1.17.1.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: onnxruntime
    target_sw: linux
    vendor: pip
    version: 1.17.1
    web_url: https://onnxruntime.ai
  licenses:
  - Apache-2.0
  - BSD-2-Clause
  - BSD-3-Clause
  - BSL-1.0
  - ISC
  - JSON
  - MIT
  - MPL-2.0
  - NCSA
  - PIL
  - Protobuf
  - Unlicense
  - Zlib
  - bsd-3-clause-open-mpi
  - cc-pd
  - curl
  - proprietary-license
  name: onnxruntime
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 1.17.1
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 1.17.1
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: opt-einsum+3.3.0
  additional_info:
    fossa-attribution:
      Description: '[![Build Status](https://travis-ci.org/dgasmith/opt_einsum.svg?branch=master)](https://travis-ci.org/dgasmith/opt_einsum)[![codecov](https://codecov.io/gh/dgasmith/opt_einsum/branch/master/graph/badge.svg)](https://codecov.io/gh/dgasmith/opt_einsum)[![Anaconda-Server
        Badge](https://anaconda.org/conda-forge/opt_einsum/badges/version.svg)](https://anaconda.org/conda-forge/opt_einsum)[![PyPI](https://img.shields.io/pypi/v/opt_einsum.svg)](https://pypi.org/project/opt-einsum/#description)[![PyPIStats](https://img.shields.io/pypi/dm/opt_einsum)](https://pypistats.org/packages/opt-einsum)[![Documentation
        Status](https://readthedocs.org/projects/optimized-einsum/badge/?version=latest)](http://optimized-einsum.readthedocs.io/en/latest/?badge=latest)[![DOI](http://joss.theoj.org/papers/10.21105/joss.00753/status.svg)](https://doi.org/10.21105/joss.00753)Optimized
        Einsum: A tensor contraction order optimizer======================================================Optimized
        einsum can significantly reduce the overall execution time of einsum-like
        expressions (e.g.,[`np.einsum`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html),[`dask.array.einsum`](https://docs.dask.org/en/latest/array-api.html#dask.array.einsum),[`pytorch.einsum`](https://pytorch.org/docs/stable/torch.html#torch.einsum),[`tensorflow.einsum`](https://www.tensorflow.org/api_docs/python/tf/einsum),)by
        optimizing the expression''s contraction order and dispatching manyoperations
        to canonical BLAS, cuBLAS, or other specialized routines. Optimizedeinsum
        is agnostic to the backend and can handle NumPy, Dask, PyTorch,Tensorflow,
        CuPy, Sparse, Theano, JAX, and Autograd arrays as well as potentiallyany library
        which conforms to a standard API. See the[**documentation**](http://optimized-einsum.readthedocs.io)
        for moreinformation.## Example usageThe [`opt_einsum.contract`](https://optimized-einsum.readthedocs.io/en/latest/autosummary/opt_einsum.contract.html#opt-einsum-contract)function
        can often act as a drop-in replacement for `einsum`functions without futher
        changes to the code while providing superior performance.Here, a tensor contraction
        is preformed with and without optimization:```pythonimport numpy as npfrom
        opt_einsum import contractN = 10C = np.random.rand(N, N)I = np.random.rand(N,
        N, N, N)%timeit np.einsum(''pi,qj,ijkl,rk,sl->pqrs'', C, C, I, C, C)1 loops,
        best of 3: 934 ms per loop%timeit contract(''pi,qj,ijkl,rk,sl->pqrs'', C,
        C, I, C, C)1000 loops, best of 3: 324 us per loop```In this particular example,
        we see a ~3000x performance improvement which isnot uncommon when compared
        against unoptimized contractions. See the [backendexamples](https://optimized-einsum.readthedocs.io/en/latest/backends.html)for
        more information on using other backends.## FeaturesThe algorithms found in
        this repository often power the `einsum` optimizationsin many of the above
        projects. For example, the optimization of `np.einsum`has been passed upstream
        and most of the same features that can be found inthis repository can be enabled
        with `np.einsum(..., optimize=True)`. However,this repository often has more
        up to date algorithms for complex contractions.The following capabilities
        are enabled by `opt_einsum`:* Inspect [detailed information](http://optimized-einsum.readthedocs.io/en/latest/path_finding.html)
        about the path chosen.* Perform contractions with [numerous backends](http://optimized-einsum.readthedocs.io/en/latest/backends.html),
        including on the GPU and with libraries such as [TensorFlow](https://www.tensorflow.org)
        and [PyTorch](https://pytorch.org).* Generate [reusable expressions](http://optimized-einsum.readthedocs.io/en/latest/reusing_paths.html),
        potentially with [constant tensors](http://optimized-einsum.readthedocs.io/en/latest/reusing_paths.html#specifying-constants),
        that can be compiled for greater performance.* Use an arbitrary number of
        indices to find contractions for [hundreds or even thousands of tensors](http://optimized-einsum.readthedocs.io/en/latest/ex_large_expr_with_greedy.html).*
        Share [intermediate computations](http://optimized-einsum.readthedocs.io/en/latest/sharing_intermediates.html)
        among multiple contractions.* Compute gradients of tensor contractions using
        [autograd](https://github.com/HIPS/autograd) or [jax](https://github.com/google/jax)Please
        see the [documentation](http://optimized-einsum.readthedocs.io/en/latest/?badge=latest)
        for more features!## Installation`opt_einsum` can either be installed via
        `pip install opt_einsum` or from conda `conda install opt_einsum -c conda-forge`.
        See the installation [documenation](http://optimized-einsum.readthedocs.io/en/latest/install.html)
        for further methods.## CitationIf this code has benefited your research, please
        support us by citing:Daniel G. A. Smith and Johnnie Gray, opt_einsum - A Python
        package for optimizing contraction order for einsum-like expressions. *Journal
        of Open Source Software*, **2018**, 3(26), 753DOI: https://doi.org/10.21105/joss.00753##
        ContributingAll contributions, bug reports, bug fixes, documentation improvements,
        enhancements, and ideas are welcome.A detailed overview on how to contribute
        can be found in the [contributing guide](https://github.com/dgasmith/opt_einsum/blob/master/.github/CONTRIBUTING.md).'
      Package: opt-einsum
      Source: pip
      Version: 3.3.0
      Hash: ''
      licenses:
      - MIT
      Title: opt-einsum
      DownloadURL: https://files.pythonhosted.org/packages/7d/bf/9257e53a0e7715bc1127e15063e831f076723c6cd60985333a1c18878fb8/opt_einsum-3.3.0.tar.gz
  bazaar:
    register: 'no'
    prim: 1/CTX1029654
    community_link: https://github.com/dgasmith/opt_einsum
    community_name: https://github.com/dgasmith/opt_einsum
    community_url: https://github.com/dgasmith/opt_einsum
    component_comment: ''
    component_highlevel_description: Optimizing einsum functions in NumPy, Tensorflow, Dask, and more with contraction order optimization.
    component_name: opt-einsum
    component_platform: linux
    component_programing_language: ''
    component_version: 3.3.0
    licenses: []
    src_download_link: https://github.com/dgasmith/opt_einsum/archive/v3.3.0.zip
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=942980&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: opt-einsum
    target_sw: linux
    vendor: pip
    version: 3.3.0
    web_url: https://github.com/dgasmith/opt_einsum
  licenses:
  - MIT
  name: opt-einsum
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 3.3.0
  mimer:
    linking: Static
    product_number: ''
    product_version_label: v3.3.0
    selected_licenses:
    - MIT
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: optimum+1.17.1
  additional_info:
    fossa-attribution:
      Description: "[![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test_onnxruntime.yml)#
        Hugging Face Optimum\U0001F917 Optimum is an extension of \U0001F917 Transformers
        and Diffusers, providing a set of optimization tools enabling maximum efficiency
        to train and run models on targeted hardware, while keeping things easy to
        use.## Installation\U0001F917 Optimum can be installed using `pip` as follows:```bashpython
        -m pip install optimum```If you'd like to use the accelerator-specific features
        of \U0001F917 Optimum, you can install the required dependencies according
        to the table below:| Accelerator                                                                                                            |
        Installation                                      ||:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------||
        [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview)                                                                           |
        `pip install --upgrade-strategy eager optimum[onnxruntime]`       || [Intel
        Neural Compressor](https://huggingface.co/docs/optimum/intel/index)       |
        `pip install --upgrade-strategy eager optimum[neural-compressor]`|| [OpenVINO](https://huggingface.co/docs/optimum/intel/index)
        \                                                                | `pip install
        --upgrade-strategy eager optimum[openvino,nncf]`    || [AMD Instinct GPUs
        and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index)                     |
        `pip install --upgrade-strategy eager optimum[amd]`              || [Habana
        Gaudi Processor (HPU)](https://huggingface.co/docs/optimum/habana/index)                                                            |
        `pip install --upgrade-strategy eager optimum[habana]`           || [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index)
        \                                                                                  |
        `pip install --upgrade-strategy eager optimum[furiosa]`          |The `--upgrade-strategy
        eager` option is needed to ensure the different packages are upgraded to the
        latest possible version.To install from source:```bashpython -m pip install
        git+https://github.com/huggingface/optimum.git```For the accelerator-specific
        features, append `optimum[accelerator_type]` to the above command:```bashpython
        -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git```##
        Accelerated Inference\U0001F917 Optimum provides multiple tools to export
        and run optimized models on various ecosystems:- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model)
        / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)-
        TensorFlow Lite- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference)-
        Habana first-gen Gaudi / Gaudi2, more details [here](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference)The
        [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations
        can be done both programmatically and with a command line.### Features summary|
        Features                           | [ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview)|
        [Neural Compressor](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)|
        [OpenVINO](https://huggingface.co/docs/optimum/main/en/intel/inference)| [TensorFlow
        Lite](https://huggingface.co/docs/optimum/main/en/exporters/tflite/overview)||:----------------------------------:|:------------------:|:------------------:|:------------------:|:------------------:||
        Graph optimization                 | :heavy_check_mark: | N/A                |
        :heavy_check_mark: | N/A                || Post-training dynamic quantization
        | :heavy_check_mark: | :heavy_check_mark: | N/A                | :heavy_check_mark:
        || Post-training static quantization  | :heavy_check_mark: | :heavy_check_mark:
        | :heavy_check_mark: | :heavy_check_mark: || Quantization Aware Training (QAT)
        \ | N/A                | :heavy_check_mark: | :heavy_check_mark: | N/A                ||
        FP16 (half precision)              | :heavy_check_mark: | N/A                |
        :heavy_check_mark: | :heavy_check_mark: || Pruning                            |
        N/A                | :heavy_check_mark: | :heavy_check_mark: | N/A                ||
        Knowledge Distillation             | N/A                | :heavy_check_mark:
        | :heavy_check_mark: | N/A                |### OpenVINOBefore you begin, make
        sure you have all the necessary libraries installed :```bashpip install --upgrade-strategy
        eager optimum[openvino,nncf]```It is possible to export \U0001F917 Transformers
        and Diffusers models to the OpenVINO format easily:```bashoptimum-cli export
        openvino --model distilbert-base-uncased-finetuned-sst-2-english distilbert_sst2_ov```If
        you add `--int8`, the weights will be quantized to INT8. Static quantization
        can also be applied on the activations using [NNCF](https://github.com/openvinotoolkit/nncf),
        more information can be found in the [documentation](https://huggingface.co/docs/optimum/main/en/intel/optimization_ov).To
        load a model and run inference with OpenVINO Runtime, you can just replace
        your `AutoModelForXxx` class with the corresponding `OVModelForXxx` class.
        To load a PyTorch checkpoint and convert it to the OpenVINO format on-the-fly,
        you can set `export=True` when loading your model.```diff- from transformers
        import AutoModelForSequenceClassification+ from optimum.intel import OVModelForSequenceClassification
        \ from transformers import AutoTokenizer, pipeline  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"
        \ tokenizer = AutoTokenizer.from_pretrained(model_id)- model = AutoModelForSequenceClassification.from_pretrained(model_id)+
        model = OVModelForSequenceClassification.from_pretrained(\"distilbert_sst2_ov\")
        \ classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)
        \ results = classifier(\"He's a dreadful magician.\")```You can find more
        examples in the [documentation](https://huggingface.co/docs/optimum/intel/inference)
        and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino).###
        Neural CompressorBefore you begin, make sure you have all the necessary libraries
        installed :```bashpip install --upgrade-strategy eager optimum[neural-compressor]```Dynamic
        quantization can be applied on your model:```bashoptimum-cli inc quantize
        --model distilbert-base-cased-distilled-squad --output ./quantized_distilbert```To
        load a model quantized with Intel Neural Compressor, hosted locally or on
        the \U0001F917 hub, you can do as follows :```pythonfrom optimum.intel import
        INCModelForSequenceClassificationmodel_id = \"Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic\"model
        = INCModelForSequenceClassification.from_pretrained(model_id)```You can find
        more examples in the [documentation](https://huggingface.co/docs/optimum/intel/optimization_inc)
        and in the [examples](https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor).###
        ONNX + ONNX RuntimeBefore you begin, make sure you have all the necessary
        libraries installed :```bashpip install optimum[exporters,onnxruntime]```It
        is possible to export \U0001F917 Transformers and Diffusers models to the
        [ONNX](https://onnx.ai/) format and perform graph optimization as well as
        quantization easily:```plainoptimum-cli export onnx -m deepset/roberta-base-squad2
        --optimize O2 roberta_base_qa_onnx```The model can then be quantized using
        `onnxruntime`:```bashoptimum-cli onnxruntime quantize \\  --avx512 \\  --onnx_model
        roberta_base_qa_onnx \\  -o quantized_roberta_base_qa_onnx```These commands
        will export `deepset/roberta-base-squad2` and perform [O2 graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization#optimization-configuration)
        on the exported model, and finally quantize it with the [avx512 configuration](https://huggingface.co/docs/optimum/main/en/onnxruntime/package_reference/configuration#optimum.onnxruntime.AutoQuantizationConfig.avx512).For
        more information on the ONNX export, please check the [documentation](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model).####
        Run the exported model using ONNX RuntimeOnce the model is exported to the
        ONNX format, we provide Python classes enabling you to run the exported ONNX
        model in a seemless manner using [ONNX Runtime](https://onnxruntime.ai/) in
        the backend:```diff- from transformers import AutoModelForQuestionAnswering+
        from optimum.onnxruntime import ORTModelForQuestionAnswering  from transformers
        import AutoTokenizer, pipeline  model_id = \"deepset/roberta-base-squad2\"
        \ tokenizer = AutoTokenizer.from_pretrained(model_id)- model = AutoModelForQuestionAnswering.from_pretrained(model_id)+
        model = ORTModelForQuestionAnswering.from_pretrained(\"roberta_base_qa_onnx\")
        \ qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)
        \ question = \"What's Optimum?\"  context = \"Optimum is an awesome library
        everyone should use!\"  results = qa_pipe(question=question, context=context)```More
        details on how to run ONNX models with `ORTModelForXXX` classes [here](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models).###
        TensorFlow LiteBefore you begin, make sure you have all the necessary libraries
        installed :```bashpip install optimum[exporters-tf]```Just as for ONNX, it
        is possible to export models to [TensorFlow Lite](https://www.tensorflow.org/lite)
        and quantize them:```plainoptimum-cli export tflite \\  -m deepset/roberta-base-squad2
        \\  --sequence_length 384  \\  --quantize int8-dynamic roberta_tflite_model```##
        Accelerated training\U0001F917 Optimum provides wrappers around the original
        \U0001F917 Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
        to enable training on powerful hardware easily.We support many providers:-
        Habana's Gaudi processors- ONNX Runtime (optimized for GPUs)### HabanaBefore
        you begin, make sure you have all the necessary libraries installed :```bashpip
        install --upgrade-strategy eager optimum[habana]``````diff- from transformers
        import Trainer, TrainingArguments+ from optimum.habana import GaudiTrainer,
        GaudiTrainingArguments  # Download a pretrained model from the Hub  model
        = AutoModelForXxx.from_pretrained(\"bert-base-uncased\")  # Define the training
        arguments- training_args = TrainingArguments(+ training_args = GaudiTrainingArguments(
        \     output_dir=\"path/to/save/folder/\",+     use_habana=True,+     use_lazy_mode=True,+
        \    gaudi_config_name=\"Habana/bert-base-uncased\",      ...  )  # Initialize
        the trainer- trainer = Trainer(+ trainer = GaudiTrainer(      model=model,
        \     args=training_args,      train_dataset=train_dataset,      ...  )  #
        Use Habana Gaudi processor for training!  trainer.train()```You can find more
        examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart)
        and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).###
        ONNX Runtime```diff- from transformers import Trainer, TrainingArguments+
        from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments  # Download
        a pretrained model from the Hub  model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")
        \ # Define the training arguments- training_args = TrainingArguments(+ training_args
        = ORTTrainingArguments(      output_dir=\"path/to/save/folder/\",      optim=\"adamw_ort_fused\",
        \     ...  )  # Create a ONNX Runtime Trainer- trainer = Trainer(+ trainer
        = ORTTrainer(      model=model,      args=training_args,      train_dataset=train_dataset,
        \     ...  )  # Use ONNX Runtime for training!  trainer.train()```You can
        find more examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer)
        and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training)."
      Package: optimum
      Source: pip
      Version: 1.17.1
      Hash: ''
      licenses:
      - Apache-2.0
      Title: optimum
      DownloadURL: https://github.com/huggingface/optimum/archive/refs/tags/v1.17.1.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/huggingface/optimum
    community_name: https://github.com/huggingface/optimum
    community_url: https://github.com/huggingface/optimum
    component_comment: ''
    component_highlevel_description: ''
    component_name: optimum
    component_platform: linux
    component_programing_language: ''
    component_version: v1.17.1
    licenses: []
    src_download_link: https://github.com/huggingface/optimum/archive/refs/tags/v1.17.1.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: optimum
    target_sw: linux
    vendor: pip
    version: 1.17.1
    web_url: https://github.com/huggingface/optimum
  licenses:
  - Apache-2.0
  name: optimum
  primary:
  - this
  subcomponent: false
  type: FOSS
  versions:
  - 1.17.1
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 1.17.1
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'True'
- ID: packaging+23.2
  additional_info:
    fossa-attribution:
      Description: 'packaging=========.. start-introReusable core utilities for various
        Python Packaging`interoperability specifications <https://packaging.python.org/specifications/>`_.This
        library provides utilities that implement the interoperabilityspecifications
        which have clearly one correct behaviour (eg: :pep:`440`)or benefit greatly
        from having a single shared implementation (eg: :pep:`425`)... end-introThe
        ``packaging`` project includes the following: version handling, specifiers,markers,
        requirements, tags, utilities.Documentation-------------The `documentation`_
        provides information and the API for the following:- Version Handling- Specifiers-
        Markers- Requirements- Tags- UtilitiesInstallation------------Use ``pip``
        to install these utilities::    pip install packagingThe ``packaging`` library
        uses calendar-based versioning (``YY.N``).Discussion----------If you run into
        bugs, you can file them in our `issue tracker`_.You can also join ``#pypa``
        on Freenode to ask questions or get involved... _`documentation`: https://packaging.pypa.io/..
        _`issue tracker`: https://github.com/pypa/packaging/issuesCode of Conduct---------------Everyone
        interacting in the packaging project''s codebases, issue trackers, chatrooms,
        and mailing lists is expected to follow the `PSF Code of Conduct`_... _PSF
        Code of Conduct: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.mdContributing------------The
        ``CONTRIBUTING.rst`` file outlines how to contribute to this project aswell
        as how to report a potential security issue. The documentation for thisproject
        also covers information about `project development`_ and `security`_... _`project
        development`: https://packaging.pypa.io/en/latest/development/.. _`security`:
        https://packaging.pypa.io/en/latest/security/Project History---------------Please
        review the ``CHANGELOG.rst`` file or the `Changelog documentation`_ forrecent
        changes and project history... _`Changelog documentation`: https://packaging.pypa.io/en/latest/changelog/'
      Package: packaging
      Source: pip
      Version: '23.2'
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-2-Clause
      - BSD-3-Clause
      Title: packaging
      DownloadURL: https://files.pythonhosted.org/packages/fb/2b/9b9c33ffed44ee921d0967086d653047286054117d584f1b1a7c22ceaf7b/packaging-23.2.tar.gz
  bazaar:
    register: 'no'
    prim: 20/CTX1020778
    community_link: https://github.com/pypa/packaging
    community_name: https://github.com/pypa/packaging
    community_url: https://github.com/pypa/packaging
    component_comment: ''
    component_highlevel_description: Core utilities for Python packages
    component_name: packaging
    component_platform: linux
    component_programing_language: Python
    component_version: '23.2'
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/pypa/packaging/archive/23.2.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1064089&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: packaging
    target_sw: linux
    vendor: pip
    version: '23.2'
    web_url: https://pypi.org/project/packaging/24.0/
  licenses:
  - Apache-2.0
  - BSD-2-Clause
  - BSD-3-Clause
  name: packaging
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '23.2'
  mimer:
    linking: Static
    product_number: CTX1020778
    product_version_label: '23.2'
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: pandas+2.2.1
  additional_info:
    fossa-attribution:
      Description: "<div align=\"center\">  <img src=\"https://pandas.pydata.org/static/img/pandas.svg\"><br></div>-----------------#
        pandas: powerful Python data analysis toolkit[![PyPI Latest Release](https://img.shields.io/pypi/v/pandas.svg)](https://pypi.org/project/pandas/)[![Conda
        Latest Release](https://anaconda.org/conda-forge/pandas/badges/version.svg)](https://anaconda.org/anaconda/pandas/)[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3509134.svg)](https://doi.org/10.5281/zenodo.3509134)[![Package
        Status](https://img.shields.io/pypi/status/pandas.svg)](https://pypi.org/project/pandas/)[![License](https://img.shields.io/pypi/l/pandas.svg)](https://github.com/pandas-dev/pandas/blob/master/LICENSE)[![Azure
        Build Status](https://dev.azure.com/pandas-dev/pandas/_apis/build/status/pandas-dev.pandas?branch=master)](https://dev.azure.com/pandas-dev/pandas/_build/latest?definitionId=1&branch=master)[![Coverage](https://codecov.io/github/pandas-dev/pandas/coverage.svg?branch=master)](https://codecov.io/gh/pandas-dev/pandas)[![Downloads](https://static.pepy.tech/personalized-badge/pandas?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads%20per%20month)](https://pepy.tech/project/pandas)[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/pydata/pandas)[![Powered
        by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)[![Code
        style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[![Imports:
        isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)##
        What is it?**pandas** is a Python package that provides fast, flexible, and
        expressive datastructures designed to make working with \"relational\" or
        \"labeled\" data botheasy and intuitive. It aims to be the fundamental high-level
        building block fordoing practical, **real world** data analysis in Python.
        Additionally, it hasthe broader goal of becoming **the most powerful and flexible
        open source dataanalysis / manipulation tool available in any language**.
        It is already well onits way towards this goal.## Main FeaturesHere are just
        a few of the things that pandas does well:  - Easy handling of [**missing
        data**][missing-data] (represented as    `NaN`, `NA`, or `NaT`) in floating
        point as well as non-floating point data  - Size mutability: columns can be
        [**inserted and    deleted**][insertion-deletion] from DataFrame and higher
        dimensional    objects  - Automatic and explicit [**data alignment**][alignment]:
        objects can    be explicitly aligned to a set of labels, or the user can simply
        \   ignore the labels and let `Series`, `DataFrame`, etc. automatically    align
        the data for you in computations  - Powerful, flexible [**group by**][groupby]
        functionality to perform    split-apply-combine operations on data sets, for
        both aggregating    and transforming data  - Make it [**easy to convert**][conversion]
        ragged,    differently-indexed data in other Python and NumPy data structures
        \   into DataFrame objects  - Intelligent label-based [**slicing**][slicing],
        [**fancy    indexing**][fancy-indexing], and [**subsetting**][subsetting]
        of    large data sets  - Intuitive [**merging**][merging] and [**joining**][joining]
        data    sets  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table]
        of    data sets  - [**Hierarchical**][mi] labeling of axes (possible to have
        multiple    labels per tick)  - Robust IO tools for loading data from [**flat
        files**][flat-files]    (CSV and delimited), [**Excel files**][excel], [**databases**][db],
        \   and saving/loading data from the ultrafast [**HDF5 format**][hdfstore]
        \ - [**Time series**][timeseries]-specific functionality: date range    generation
        and frequency conversion, moving window statistics,    date shifting and lagging
        \  [missing-data]: https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html
        \  [insertion-deletion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#column-selection-addition-deletion
        \  [alignment]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html?highlight=alignment#intro-to-data-structures
        \  [groupby]: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#group-by-split-apply-combine
        \  [conversion]: https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe
        \  [slicing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges
        \  [fancy-indexing]: https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced
        \  [subsetting]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing
        \  [merging]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging
        \  [joining]: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#joining-on-index
        \  [reshape]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
        \  [pivot-table]: https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html
        \  [mi]: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#hierarchical-indexing-multiindex
        \  [flat-files]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#csv-text-files
        \  [excel]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#excel-files
        \  [db]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#sql-queries
        \  [hdfstore]: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#hdf5-pytables
        \  [timeseries]: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#time-series-date-functionality##
        Where to get itThe source code is currently hosted on GitHub at:https://github.com/pandas-dev/pandasBinary
        installers for the latest released version are available at the [PythonPackage
        Index (PyPI)](https://pypi.org/project/pandas) and on [Conda](https://docs.conda.io/en/latest/).```sh#
        condaconda install pandas``````sh# or PyPIpip install pandas```## Dependencies-
        [NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level
        mathematical functions to operate on these arrays](https://www.numpy.org)-
        [python-dateutil - Provides powerful extensions to the standard datetime module](https://dateutil.readthedocs.io/en/stable/index.html)-
        [pytz - Brings the Olson tz database into Python which allows accurate and
        cross platform timezone calculations](https://github.com/stub42/pytz)See the
        [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies)
        for minimum supported versions of required, recommended and optional dependencies.##
        Installation from sourcesTo install pandas from source you need [Cython](https://cython.org/)
        in addition to the normaldependencies above. Cython can be installed from
        PyPI:```shpip install cython```In the `pandas` directory (same one where you
        found this file aftercloning the git repo), execute:```shpython setup.py install```or
        for installing in [development mode](https://pip.pypa.io/en/latest/cli/pip_install/#install-editable):```shpython
        -m pip install -e . --no-build-isolation --no-use-pep517```If you have `make`,
        you can also use `make develop` to run the same command.or alternatively```shpython
        setup.py develop```See the full instructions for [installing from source](https://pandas.pydata.org/pandas-docs/stable/install.html#installing-from-source).##
        License[BSD 3](LICENSE)## DocumentationThe official documentation is hosted
        on PyData.org: https://pandas.pydata.org/pandas-docs/stable## BackgroundWork
        on ``pandas`` started at [AQR](https://www.aqr.com/) (a quantitative hedge
        fund) in 2008 andhas been under active development since then.## Getting HelpFor
        usage questions, the best place to go to is [StackOverflow](https://stackoverflow.com/questions/tagged/pandas).Further,
        general questions and discussions can also take place on the [pydata mailing
        list](https://groups.google.com/forum/?fromgroups#!forum/pydata).## Discussion
        and DevelopmentMost development discussions take place on GitHub in this repo.
        Further, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev)
        can also be used for specialized discussions or design issues, and a [Gitter
        channel](https://gitter.im/pydata/pandas) is available for quick development
        related questions.## Contributing to pandas [![Open Source Helpers](https://www.codetriage.com/pandas-dev/pandas/badges/users.svg)](https://www.codetriage.com/pandas-dev/pandas)All
        contributions, bug reports, bug fixes, documentation improvements, enhancements,
        and ideas are welcome.A detailed overview on how to contribute can be found
        in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.If
        you are simply looking to start working with the pandas codebase, navigate
        to the [GitHub \"issues\" tab](https://github.com/pandas-dev/pandas/issues)
        and start looking through interesting issues. There are a number of issues
        listed under [Docs](https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open)
        and [good first issue](https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open)
        where you could start out.You can also triage issues which may include reproducing
        bug reports, or asking for vital information such as version numbers or reproduction
        instructions. If you would like to start triaging issues, one easy way to
        get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).Or
        maybe through using pandas you have an idea of your own or are looking for
        something in the documentation and thinking \u2018this can be improved\u2019...you
        can do something about it!Feel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata)
        or on [Gitter](https://gitter.im/pydata/pandas).As contributors and maintainers
        to this project, you are expected to abide by pandas' code of conduct. More
        information can be found at: [Contributor Code of Conduct](https://github.com/pandas-dev/pandas/blob/master/.github/CODE_OF_CONDUCT.md)"
      Package: pandas
      Source: pip
      Version: 2.2.1
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: pandas
      DownloadURL: https://files.pythonhosted.org/packages/3d/59/2afa81b9fb300c90531803c0fd43ff4548074fa3e8d0f747ef63b3b5e77a/pandas-2.2.1.tar.gz
  bazaar:
    register: 'no'
    prim: 53/CAX1057900
    community_link: https://github.com/pandas-dev/pandas
    community_name: https://github.com/pandas-dev/pandas
    community_url: https://github.com/pandas-dev/pandas
    component_comment: ''
    component_highlevel_description: Flexible and powerful data analysis / manipulation
      library for Python, providing labeled data structures similar to R data.frame
      objects, statistical functions, and much more.
    component_name: pandas
    component_platform: linux
    component_programing_language: Python
    component_version: V2.2.1
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://github.com/pandas-dev/pandas/archive/v2.2.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1080444&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: pandas
    target_sw: linux
    vendor: pip
    version: 2.2.1
    web_url: https://pandas.pydata.org
  licenses:
  - BSD-3-Clause
  name: pandas
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 2.2.1
  mimer:
    linking: Static
    product_number: CAX1057900
    product_version_label: V2.2.1
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: pillow+10.2.0
  additional_info:
    fossa-attribution:
      Description: <p align="center">    <img width="248" height="250" src="https://raw.githubusercontent.com/python-pillow/pillow-logo/main/pillow-logo-248x250.png"
        alt="Pillow logo"></p># Pillow## Python Imaging Library (Fork)Pillow is the
        friendly PIL fork by [Jeffrey A. Clark andcontributors](https://github.com/python-pillow/Pillow/graphs/contributors).PIL
        is the Python Imaging Library by Fredrik Lundh and contributors.As of 2019,
        Pillow development is[supported by Tidelift](https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=readme&utm_campaign=enterprise).<table>    <tr>        <th>docs</th>        <td>            <a
        href="https://pillow.readthedocs.io/?badge=latest"><img                alt="Documentation
        Status"                src="https://readthedocs.org/projects/pillow/badge/?version=latest"></a>        </td>    </tr>    <tr>        <th>tests</th>        <td>            <a
        href="https://github.com/python-pillow/Pillow/actions/workflows/lint.yml"><img                alt="GitHub
        Actions build status (Lint)"                src="https://github.com/python-pillow/Pillow/workflows/Lint/badge.svg"></a>            <a
        href="https://github.com/python-pillow/Pillow/actions/workflows/test.yml"><img                alt="GitHub
        Actions build status (Test Linux and macOS)"                src="https://github.com/python-pillow/Pillow/workflows/Test/badge.svg"></a>            <a
        href="https://github.com/python-pillow/Pillow/actions/workflows/test-windows.yml"><img                alt="GitHub
        Actions build status (Test Windows)"                src="https://github.com/python-pillow/Pillow/workflows/Test%20Windows/badge.svg"></a>            <a
        href="https://github.com/python-pillow/Pillow/actions/workflows/test-mingw.yml"><img                alt="GitHub
        Actions build status (Test MinGW)"                src="https://github.com/python-pillow/Pillow/workflows/Test%20MinGW/badge.svg"></a>            <a
        href="https://github.com/python-pillow/Pillow/actions/workflows/test-cygwin.yml"><img                alt="GitHub
        Actions build status (Test Cygwin)"                src="https://github.com/python-pillow/Pillow/workflows/Test%20Cygwin/badge.svg"></a>            <a
        href="https://github.com/python-pillow/Pillow/actions/workflows/test-docker.yml"><img                alt="GitHub
        Actions build status (Test Docker)"                src="https://github.com/python-pillow/Pillow/workflows/Test%20Docker/badge.svg"></a>            <a
        href="https://ci.appveyor.com/project/python-pillow/Pillow"><img                alt="AppVeyor
        CI build status (Windows)"                src="https://img.shields.io/appveyor/build/python-pillow/Pillow/main.svg?label=Windows%20build"></a>            <a
        href="https://github.com/python-pillow/Pillow/actions/workflows/wheels.yml"><img                alt="GitHub
        Actions build status (Wheels)"                src="https://github.com/python-pillow/Pillow/workflows/Wheels/badge.svg"></a>            <a
        href="https://app.codecov.io/gh/python-pillow/Pillow"><img                alt="Code
        coverage"                src="https://codecov.io/gh/python-pillow/Pillow/branch/main/graph/badge.svg"></a>            <a
        href="https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:pillow"><img                alt="Fuzzing
        Status"                src="https://oss-fuzz-build-logs.storage.googleapis.com/badges/pillow.svg"></a>        </td>    </tr>    <tr>        <th>package</th>        <td>            <a
        href="https://zenodo.org/badge/latestdoi/17549/python-pillow/Pillow"><img                alt="Zenodo"                src="https://zenodo.org/badge/17549/python-pillow/Pillow.svg"></a>            <a
        href="https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=badge"><img                alt="Tidelift"                src="https://tidelift.com/badges/package/pypi/pillow?style=flat"></a>            <a
        href="https://pypi.org/project/pillow/"><img                alt="Newest PyPI
        version"                src="https://img.shields.io/pypi/v/pillow.svg"></a>            <a
        href="https://pypi.org/project/pillow/"><img                alt="Number of
        PyPI downloads"                src="https://img.shields.io/pypi/dm/pillow.svg"></a>            <a
        href="https://www.bestpractices.dev/projects/6331"><img                alt="OpenSSF
        Best Practices"                src="https://www.bestpractices.dev/projects/6331/badge"></a>        </td>    </tr>    <tr>        <th>social</th>        <td>            <a
        href="https://gitter.im/python-pillow/Pillow?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge"><img                alt="Join
        the chat at https://gitter.im/python-pillow/Pillow"                src="https://badges.gitter.im/python-pillow/Pillow.svg"></a>            <a
        href="https://fosstodon.org/@pillow"><img                alt="Follow on https://fosstodon.org/@pillow"                src="https://img.shields.io/badge/publish-on%20Mastodon-595aff.svg"                rel="me"></a>        </td>    </tr></table>##
        OverviewThe Python Imaging Library adds image processing capabilities to your
        Python interpreter.This library provides extensive file format support, an
        efficient internal representation, and fairly powerful image processing capabilities.The
        core image library is designed for fast access to data stored in a few basic
        pixel formats. It should provide a solid foundation for a general image processing
        tool.## More Information- [Documentation](https://pillow.readthedocs.io/)  -
        [Installation](https://pillow.readthedocs.io/en/latest/installation.html)  -
        [Handbook](https://pillow.readthedocs.io/en/latest/handbook/index.html)- [Contribute](https://github.com/python-pillow/Pillow/blob/main/.github/CONTRIBUTING.md)  -
        [Issues](https://github.com/python-pillow/Pillow/issues)  - [Pull requests](https://github.com/python-pillow/Pillow/pulls)-
        [Release notes](https://pillow.readthedocs.io/en/stable/releasenotes/index.html)-
        [Changelog](https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst)  -
        [Pre-fork](https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst#pre-fork)##
        Report a VulnerabilityTo report a security vulnerability, please follow the
        procedure described in the [Tidelift security policy](https://tidelift.com/docs/security).
      Package: pillow
      Source: pip
      Version: 10.2.0
      Hash: ''
      licenses:
      - Apache-2.0
      - BitstreamVera
      - CC0-1.0
      - HPND
      - MIT
      - OFL-1.1
      - PIL
      - TCL
      - mit-modern
      Title: pillow
      DownloadURL: https://files.pythonhosted.org/packages/f8/3e/32cbd0129a28686621434cbf17bb64bf1458bfb838f1f668262fefce145c/pillow-10.2.0.tar.gz
  bazaar:
    register: 'no'
    prim: 34/CAX1058542
    community_link: https://github.com/python-pillow/Pillow
    community_name: https://github.com/python-pillow/Pillow
    community_url: https://github.com/python-pillow/Pillow
    component_comment: ''
    component_highlevel_description: Python Imaging Library (Fork)
    component_name: Pillow
    component_platform: linux
    component_programing_language: Python
    component_version: 10.2.0
    licenses:
    - FAL1159104/1 (Historical Permission Notice and Disclaimer (HPND))
    src_download_link: https://github.com/python-pillow/Pillow/archive/10.2.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078406&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: pillow
    target_sw: linux
    vendor: pip
    version: 10.2.0
    web_url: https://pypi.org/project/pillow/10.3.0/
  licenses:
  - Apache-2.0
  - BitstreamVera
  - CC0-1.0
  - HPND
  - MIT
  - OFL-1.1
  - PIL
  - TCL
  - mit-modern
  name: pillow
  primary:
  - this
  subcomponent: false
  type: FOSS
  versions:
  - 10.2.0
  mimer:
    linking: Static
    product_number: CAX1058542
    product_version_label: 10.2.0
    selected_licenses:
    - HPND
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'True'
- ID: protobuf+4.25.3
  additional_info:
    fossa-attribution:
      Description: Protocol Buffers are Google's data interchange format
      Package: protobuf
      Source: pip
      Version: 4.25.3
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: protobuf
      DownloadURL: https://github.com/protocolbuffers/protobuf/archive/v4.25.3.tar.gz
  bazaar:
    register: 'no'
    prim: 129/CAX1054389
    community_link: https://github.com/protocolbuffers/protobuf
    community_name: https://github.com/protocolbuffers/protobuf
    community_url: https://github.com/protocolbuffers/protobuf
    component_comment: ''
    component_highlevel_description: ''
    component_name: protobuf
    component_platform: linux
    component_programing_language: ''
    component_version: V4.25.3
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://github.com/protocolbuffers/protobuf/archive/v4.25.3.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1080445&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: protobuf
    target_sw: linux
    vendor: pip
    version: 4.25.3
    web_url: https://developers.google.com/protocol-buffers/
  licenses:
  - BSD-3-Clause
  name: protobuf
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 4.25.3
  mimer:
    linking: Static
    product_number: CAX1054389
    product_version_label: v4.25.3
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: pyarrow+15.0.0
  additional_info:
    fossa-attribution:
      Description: '<!---  Licensed to the Apache Software Foundation (ASF) under
        one  or more contributor license agreements.  See the NOTICE file  distributed
        with this work for additional information  regarding copyright ownership.  The
        ASF licenses this file  to you under the Apache License, Version 2.0 (the  "License");
        you may not use this file except in compliance  with the License.  You may
        obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless
        required by applicable law or agreed to in writing,  software distributed
        under the License is distributed on an  "AS IS" BASIS, WITHOUT WARRANTIES
        OR CONDITIONS OF ANY  KIND, either express or implied.  See the License for
        the  specific language governing permissions and limitations  under the License.-->##
        Python library for Apache Arrow[![pypi](https://img.shields.io/pypi/v/pyarrow.svg)](https://pypi.org/project/pyarrow/)
        [![conda-forge](https://img.shields.io/conda/vn/conda-forge/pyarrow.svg)](https://anaconda.org/conda-forge/pyarrow)This
        library provides a Python API for functionality provided by the Arrow C++libraries,
        along with tools for Arrow integration and interoperability withpandas, NumPy,
        and other software in the Python ecosystem.## InstallingAcross platforms,
        you can install a recent version of pyarrow with the condapackage manager:```shellconda
        install pyarrow -c conda-forge```On Linux, macOS, and Windows, you can also
        install binary wheels from PyPI withpip:```shellpip install pyarrow```If you
        encounter any issues importing the pip wheels on Windows, you may needto install
        the [Visual C++ Redistributable for Visual Studio 2015][6].## DevelopmentSee
        [Python Development][2] in the documentation subproject.### Building the documentationSee
        [documentation build instructions][1] in the documentation subproject.[1]:
        https://github.com/apache/arrow/blob/main/docs/source/developers/documentation.rst[2]:
        https://github.com/apache/arrow/blob/main/docs/source/developers/python.rst[3]:
        https://github.com/pandas-dev/pandas[5]: https://arrow.apache.org/docs/latest/python/benchmarks.html[6]:
        https://www.microsoft.com/en-us/download/details.aspx?id=48145'
      Package: pyarrow
      Source: pip
      Version: 15.0.0
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-3-Clause
      Title: pyarrow
      DownloadURL: https://github.com/apache/arrow/archive/go/v15.0.0.tar.gz
  bazaar:
    register: 'no'
    prim: 29/CTX1023558
    community_link: https://github.com/apache/arrow
    community_name: https://github.com/apache/arrow
    community_url: https://github.com/apache/arrow
    component_comment: ''
    component_highlevel_description: Apache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast.
    component_name: arrow
    component_platform: linux
    component_programing_language: ''
    component_version: V15.0.0
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/apache/arrow/archive/go/v15.0.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078412&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: ''
    programming_language: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: pyarrow
    target_sw: linux
    vendor: pip
    version: 15.0.0
    web_url: https://arrow.apache.org/
  licenses:
  - Apache-2.0
  - BSD-3-Clause
  name: pyarrow
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 15.0.0
  mimer:
    linking: Static
    product_number: CTX1023558
    product_version_label: go/v15.0.0
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: pyasn1+0.5.1
  additional_info:
    fossa-attribution:
      Description: 'ASN.1 library for Python------------------------[![PyPI](https://img.shields.io/pypi/v/pyasn1.svg?maxAge=2592000)](https://pypi.org/project/pyasn1)[![Python
        Versions](https://img.shields.io/pypi/pyversions/pyasn1.svg)](https://pypi.org/project/pyasn1/)[![Build
        status](https://github.com/pyasn1/pyasn1/actions/workflows/main.yml/badge.svg)](https://github.com/pyasn1/pyasn1/actions/workflows/main.yml)[![Coverage
        Status](https://img.shields.io/codecov/c/github/pyasn1/pyasn1.svg)](https://codecov.io/github/pyasn1/pyasn1)[![GitHub
        license](https://img.shields.io/badge/license-BSD-blue.svg)](https://raw.githubusercontent.com/pyasn1/pyasn1/master/LICENSE.txt)This
        is a free and open source implementation of ASN.1 types and codecsas a Python
        package. It has been first written to support particularprotocol (SNMP) but
        then generalized to be suitable for a wide rangeof protocols based on[ASN.1
        specification](https://www.itu.int/rec/dologin_pub.asp?lang=e&id=T-REC-X.208-198811-W!!PDF-E&type=items).**NOTE:**
        The package is now maintained by *Christian Heimes* and*Simon Pichugin* in
        project https://github.com/pyasn1/pyasn1.Features--------* Generic implementation
        of ASN.1 types (X.208)* Standards compliant BER/CER/DER codecs* Can operate
        on streams of serialized data* Dumps/loads ASN.1 structures from Python types*
        100% Python, works with Python 3.8+* MT-safe* Contributed ASN.1 compiler [Asn1ate](https://github.com/kimgr/asn1ate)Why
        using pyasn1----------------ASN.1 solves the data serialisation problem. This
        solution wasdesigned long ago by the wise Ancients. Back then, they did nothave
        the luxury of wasting bits. That is why ASN.1 is designedto serialise data
        structures of unbounded complexity intosomething compact and efficient when
        it comes to processingthe data.That probably explains why many network protocols
        and file formatsstill rely on the 30+ years old technology. Including a number
        ofhigh-profile Internet protocols and file formats.Quite a number of books
        cover the topic of ASN.1. [Communication between heterogeneous systems](http://www.oss.com/asn1/dubuisson.html)by
        Olivier Dubuisson is one of those high quality books freely available on the
        Internet.The pyasn1 package is designed to help Python programmers tacklingnetwork
        protocols and file formats at the comfort of their Pythonprompt. The tool
        struggles to capture all aspects of a rathercomplicated ASN.1 system and to
        represent it on the Python terms.How to use pyasn1-----------------With pyasn1
        you can build Python objects from ASN.1 data structures.For example, the following
        ASN.1 data structure:```bashRecord ::= SEQUENCE {  id        INTEGER,  room  [0]
        INTEGER OPTIONAL,  house [1] INTEGER DEFAULT 0}```Could be expressed in pyasn1
        like this:```pythonclass Record(Sequence):    componentType = NamedTypes(        NamedType(''id'',
        Integer()),        OptionalNamedType(            ''room'', Integer().subtype(                implicitTag=Tag(tagClassContext,
        tagFormatSimple, 0)            )        ),        DefaultedNamedType(            ''house'',
        Integer(0).subtype(                implicitTag=Tag(tagClassContext, tagFormatSimple,
        1)            )        )    )```It is in the spirit of ASN.1 to take abstract
        data description and turn it into a programming language specific form.Once
        you have your ASN.1 data structure expressed in Python, youcan use it along
        the lines of similar Python type (e.g. ASN.1`SET` is similar to Python `dict`,
        `SET OF` to `list`):```python>>> record = Record()>>> record[''id''] = 123>>>
        record[''room''] = 321>>> str(record)Record: id=123 room=321>>>```Part of
        the power of ASN.1 comes from its serialisation features. Youcan serialise
        your data structure and send it over the network.```python>>> from pyasn1.codec.der.encoder
        import encode>>> substrate = encode(record)>>> hexdump(substrate)00000: 30
        07 02 01 7B 80 02 01 41```Conversely, you can turn serialised ASN.1 content,
        as received fromnetwork or read from a file, into a Python object which you
        canintrospect, modify, encode and send back.```python>>> from pyasn1.codec.der.decoder
        import decode>>> received_record, rest_of_substrate = decode(substrate, asn1Spec=Record())>>>>>>
        for field in received_record:>>>    print(''{} is {}''.format(field, received_record[field]))id
        is 123room is 321house is 0>>>>>> record == received_recordTrue>>> received_record.update(room=123)>>>
        substrate = encode(received_record)>>> hexdump(substrate)00000: 30 06 02 01
        7B 80 01 7B```The pyasn1 classes struggle to emulate their Python prototypes
        (e.g. int,list, dict etc.). But ASN.1 types exhibit more complicated behaviour.To
        make life easier for a Pythonista, they can turn their pyasn1classes into
        Python built-ins:```python>>> from pyasn1.codec.native.encoder import encode>>>
        encode(record){''id'': 123, ''room'': 321, ''house'': 0}```Or vice-versa --
        you can initialize an ASN.1 structure from a tree ofPython objects:```python>>>
        from pyasn1.codec.native.decoder import decode>>> record = decode({''id'':
        123, ''room'': 321, ''house'': 0}, asn1Spec=Record())>>> str(record)Record:
        id=123 room=321>>>```With ASN.1 design, serialisation codecs are decoupled
        from data objects,so you could turn every single ASN.1 object into many different
        serialised forms. As of this moment, pyasn1 supports BER, DER, CER andPython
        built-ins codecs. The extremely compact PER encoding is expectedto be introduced
        in the upcoming pyasn1 release.More information on pyasn1 APIs can be found
        in the[documentation](https://pyasn1.readthedocs.io/en/latest/pyasn1/contents.html),compiled
        ASN.1 modules for different protocols and file formatscould be found in the
        pyasn1-modules [repo](https://github.com/pyasn1/pyasn1-modules).How to get
        pyasn1-----------------The pyasn1 package is distributed under terms and conditions
        of 2-clauseBSD [license](https://pyasn1.readthedocs.io/en/latest/license.html).
        Source code is freelyavailable as a GitHub [repo](https://github.com/pyasn1/pyasn1).You
        could `pip install pyasn1` or download it from [PyPI](https://pypi.org/project/pyasn1).If
        something does not work as expected, [open an issue](https://github.com/epyasn1/pyasn1/issues)
        at GitHub orpost your question [on Stack Overflow](https://stackoverflow.com/questions/ask)or
        try browsing pyasn1 [mailing list archives](https://sourceforge.net/p/pyasn1/mailman/pyasn1-users/).Copyright
        (c) 2005-2020, [Ilya Etingof](mailto:etingof@gmail.com).All rights reserved.'
      Package: pyasn1
      Source: pip
      Version: 0.5.1
      Hash: ''
      licenses:
      - BSD-2-Clause
      Title: pyasn1
      DownloadURL: https://files.pythonhosted.org/packages/ce/dc/996e5446a94627fe8192735c20300ca51535397e31e7097a3cc80ccf78b7/pyasn1-0.5.1.tar.gz
  bazaar:
    register: 'no'
    prim: 17/CAX1057052
    community_link: https://pypi.org/project/pyasn1/
    community_name: https://pypi.org/project/pyasn1/
    community_url: https://pypi.org/project/pyasn1/
    component_comment: ''
    component_highlevel_description: This is a free and open source implementation
      of ASN.1 types and codecs as a Python package. It has been first written to
      support particular protocol (SNMP) but then generalized to be suitable for a
      wide range of protocols based on ASN.1 specification.
    component_name: ASN.1 library for Python
    component_platform: linux
    component_programing_language: Python
    component_version: 0.5.1
    licenses:
    - FAL1159003/1 (BSD 2-Clause "Simplified" License (BSD-2-Clause))
    src_download_link: https://files.pythonhosted.org/packages/ce/dc/996e5446a94627fe8192735c20300ca51535397e31e7097a3cc80ccf78b7/pyasn1-0.5.1.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Low activity community.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1074973&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Czech Republic
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: pyasn1
    target_sw: linux
    vendor: pip
    version: 0.5.1
    web_url: https://github.com/pyasn1/pyasn1
  licenses:
  - BSD-2-Clause
  name: pyasn1
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 0.5.1
  mimer:
    linking: Static
    product_number: CAX1057052
    product_version_label: 0.5.1
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: pyasn1-modules+0.3.0
  additional_info:
    fossa-attribution:
      Description: ASN.1 modules for Python------------------------[![PyPI](https://img.shields.io/pypi/v/pyasn1-modules.svg?maxAge=2592000)](https://pypi.org/project/pyasn1-modules)[![Python
        Versions](https://img.shields.io/pypi/pyversions/pyasn1-modules.svg)](https://pypi.org/project/pyasn1-modules/)[![Build
        status](https://github.com/pyasn1/pyasn1-modules/actions/workflows/main.yml/badge.svg)](https://github.com/pyasn1/pyasn1-modules/actions/workflows/main.yml)[![Coverage
        Status](https://img.shields.io/codecov/c/github/pyasn1/pyasn1-modules.svg)](https://codecov.io/github/pyasn1/pyasn1-modules)[![GitHub
        license](https://img.shields.io/badge/license-BSD-blue.svg)](https://raw.githubusercontent.com/pyasn1/pyasn1-modules/master/LICENSE.txt)The
        `pyasn1-modules` package contains a collection of[ASN.1](https://www.itu.int/rec/dologin_pub.asp?lang=e&id=T-REC-X.208-198811-W!!PDF-E&type=items)data
        structures expressed as Python classes based on [pyasn1](https://github.com/pyasn1/pyasn1)data
        model.If ASN.1 module you need is not present in this collection, try using[Asn1ate](https://github.com/kimgr/asn1ate)
        tool that compiles ASN.1 documentsinto pyasn1 code.**NOTE:** The package is
        now maintained by *Christian Heimes* and*Simon Pichugin* in project https://github.com/pyasn1/pyasn1-modules.Feedback--------If
        something does not work as expected, [open an issue](https://github.com/pyasn1/pyasn1-modules/issues)
        at GitHubor post your question [on Stack Overflow](https://stackoverflow.com/questions/ask)
        New modules contributions are welcome via GitHub pull requests.Copyright (c)
        2005-2020, [Ilya Etingof](mailto:etingof@gmail.com).All rights reserved.
      Package: pyasn1-modules
      Source: pip
      Version: 0.3.0
      Hash: ''
      licenses:
      - BSD-2-Clause
      Title: pyasn1-modules
      DownloadURL: https://files.pythonhosted.org/packages/3b/e4/7dec823b1b5603c5b3c51e942d5d9e65efd6ff946e713a325ed4146d070f/pyasn1_modules-0.3.0.tar.gz
  bazaar:
    register: 'no'
    prim: 9/CTX1022396
    community_link: https://github.com/pyasn1/pyasn1-modules
    community_name: https://github.com/pyasn1/pyasn1-modules
    community_url: https://github.com/pyasn1/pyasn1-modules
    component_comment: ''
    component_highlevel_description: A collection of ASN.1 modules expressed in form of pyasn1 classes. Includes protocols PDUs definition (SNMP, LDAP etc.) and various data structures (X.509, PKCS etc.).
    component_name: ASN.1 modules for Python
    component_platform: linux
    component_programing_language: ''
    component_version: V0.3.0
    licenses:
    - FAL1159003/1 (BSD 2-Clause "Simplified" License (BSD-2-Clause))
    src_download_link: https://github.com/pyasn1/pyasn1-modules/archive/refs/tags/v0.3.0.zip
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: No frequent releases.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1045466&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: pyasn1-modules
    target_sw: linux
    vendor: pip
    version: 0.3.0
    web_url: https://github.com/pyasn1/pyasn1-modules
  licenses:
  - BSD-2-Clause
  name: pyasn1-modules
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 0.3.0
  mimer:
    linking: Static
    product_number: CTX1022396
    product_version_label: v0.3.0
    selected_licenses:
    - BSD-2-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: pydantic+1.10.14
  additional_info:
    fossa-attribution:
      Description: "# Pydantic[![CI](https://img.shields.io/github/actions/workflow/status/pydantic/pydantic/ci.yml?branch=main&logo=github&label=CI)](https://github.com/pydantic/pydantic/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)[![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic)[![pypi](https://img.shields.io/pypi/v/pydantic.svg)](https://pypi.python.org/pypi/pydantic)[![CondaForge](https://img.shields.io/conda/v/conda-forge/pydantic.svg)](https://anaconda.org/conda-forge/pydantic)[![downloads](https://static.pepy.tech/badge/pydantic/month)](https://pepy.tech/project/pydantic)[![versions](https://img.shields.io/pypi/pyversions/pydantic.svg)](https://github.com/pydantic/pydantic)[![license](https://img.shields.io/github/license/pydantic/pydantic.svg)](https://github.com/pydantic/pydantic/blob/main/LICENSE)[![Pydantic
        v2](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json)](https://docs.pydantic.dev/latest/contributing/#badges)Data
        validation using Python type hints.Fast and extensible, Pydantic plays nicely
        with your linters/IDE/brain.Define how data should be in pure, canonical Python
        3.8+; validate it with Pydantic.## Pydantic Company :rocket:We've started
        a company based on the principles that I believe have led to Pydantic's success.Learning
        more from the [Company Announcement](https://blog.pydantic.dev/blog/2023/02/16/company-announcement--pydantic/).##
        Pydantic V1.10 vs. V2Pydantic V2 is a ground-up rewrite that offers many new
        features, performance improvements, and some breaking changes compared to
        Pydantic V1.If you're using Pydantic V1 you may want to look at the[pydantic
        V1.10 Documentation](https://docs.pydantic.dev/) or,[`1.10.X-fixes` git branch](https://github.com/pydantic/pydantic/tree/1.10.X-fixes).
        Pydantic V2 also ships with the latest version of Pydantic V1 built in so
        that you can incrementally upgrade your code base and projects: `from pydantic
        import v1 as pydantic_v1`.## HelpSee [documentation](https://docs.pydantic.dev/)
        for more details.## InstallationInstall using `pip install -U pydantic` or
        `conda install pydantic -c conda-forge`.For more installation options to make
        Pydantic even faster,see the [Install](https://docs.pydantic.dev/install/)
        section in the documentation.## A Simple Example```pyfrom datetime import
        datetimefrom typing import List, Optionalfrom pydantic import BaseModelclass
        User(BaseModel):    id: int    name: str = 'John Doe'    signup_ts: Optional[datetime]
        = None    friends: List[int] = []external_data = {'id': '123', 'signup_ts':
        '2017-06-01 12:22', 'friends': [1, '2', b'3']}user = User(**external_data)print(user)#>
        User id=123 name='John Doe' signup_ts=datetime.datetime(2017, 6, 1, 12, 22)
        friends=[1, 2, 3]print(user.id)#> 123```## ContributingFor guidance on setting
        up a development environment and how to make acontribution to Pydantic, see[Contributing
        to Pydantic](https://docs.pydantic.dev/contributing/).## Reporting a Security
        VulnerabilitySee our [security policy](https://github.com/pydantic/pydantic/security/policy).##
        Changelog## v2.7.0 (2024-04-11)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.7.0)The
        code released in v2.7.0 is practically identical to that of v2.7.0b1.### What's
        Changed#### Packaging* Reorganize `pyproject.toml` sections by [@Viicos](https://github.com/Viicos)
        in [#8899](https://github.com/pydantic/pydantic/pull/8899)* Bump `pydantic-core`
        to `v2.18.1` by [@sydney-runkle](https://github.com/sydney-runkle) in [#9211](https://github.com/pydantic/pydantic/pull/9211)*
        Adopt `jiter` `v0.2.0` by [@samuelcolvin](https://github.com/samuelcolvin)
        in [pydantic/pydantic-core#1250](https://github.com/pydantic/pydantic-core/pull/1250)####
        New Features* Extract attribute docstrings from `FieldInfo.description` by
        [@Viicos](https://github.com/Viicos) in [#6563](https://github.com/pydantic/pydantic/pull/6563)*
        Add a `with_config` decorator to comply with typing spec by [@Viicos](https://github.com/Viicos)
        in [#8611](https://github.com/pydantic/pydantic/pull/8611)* Allow an optional
        separator splitting the value and unit of the result of `ByteSize.human_readable`
        by [@jks15satoshi](https://github.com/jks15satoshi) in [#8706](https://github.com/pydantic/pydantic/pull/8706)*
        Add generic `Secret` base type by [@conradogarciaberrotaran](https://github.com/conradogarciaberrotaran)
        in [#8519](https://github.com/pydantic/pydantic/pull/8519)* Make use of `Sphinx`
        inventories for cross references in docs by [@Viicos](https://github.com/Viicos)
        in [#8682](https://github.com/pydantic/pydantic/pull/8682)* Add environment
        variable to disable plugins by [@geospackle](https://github.com/geospackle)
        in [#8767](https://github.com/pydantic/pydantic/pull/8767)* Add support for
        `deprecated` fields by [@Viicos](https://github.com/Viicos) in [#8237](https://github.com/pydantic/pydantic/pull/8237)*
        Allow `field_serializer('*')` by [@ornariece](https://github.com/ornariece)
        in [#9001](https://github.com/pydantic/pydantic/pull/9001)* Handle a case
        when `model_config` is defined as a model property by [@alexeyt101](https://github.com/alexeyt101)
        in [#9004](https://github.com/pydantic/pydantic/pull/9004)* Update `create_model()`
        to support `typing.Annotated` as input by [@wannieman98](https://github.com/wannieman98)
        in [#8947](https://github.com/pydantic/pydantic/pull/8947)* Add `ClickhouseDsn`
        support by [@solidguy7](https://github.com/solidguy7) in [#9062](https://github.com/pydantic/pydantic/pull/9062)*
        Add support for `re.Pattern[str]` to `pattern` field by [@jag-k](https://github.com/jag-k)
        in [#9053](https://github.com/pydantic/pydantic/pull/9053)* Support for `serialize_as_any`
        runtime setting by [@sydney-runkle](https://github.com/sydney-runkle) in [#8830](https://github.com/pydantic/pydantic/pull/8830)*
        Add support for `typing.Self` by [@Youssefares](https://github.com/Youssefares)
        in [#9023](https://github.com/pydantic/pydantic/pull/9023)* Ability to pass
        `context` to serialization by [@ornariece](https://github.com/ornariece) in
        [#8965](https://github.com/pydantic/pydantic/pull/8965)* Add feedback widget
        to docs with flarelytics integration by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#9129](https://github.com/pydantic/pydantic/pull/9129)* Support for parsing
        partial JSON strings in Python by [@samuelcolvin](https://github.com/samuelcolvin)
        in [pydantic/jiter#66](https://github.com/pydantic/jiter/pull/66)**Finalized
        in v2.7.0, rather than v2.7.0b1:*** Add support for field level number to
        str coercion option by [@NeevCohen](https://github.com/NeevCohen) in [#9137](https://github.com/pydantic/pydantic/pull/9137)*
        Update `warnings` parameter for serialization utilities to allow raising a
        warning by [@Lance-Drane](https://github.com/Lance-Drane) in [#9166](https://github.com/pydantic/pydantic/pull/9166)####
        Changes* Correct docs, logic for `model_construct` behavior with `extra` by
        [@sydney-runkle](https://github.com/sydney-runkle) in [#8807](https://github.com/pydantic/pydantic/pull/8807)*
        Improve error message for improper `RootModel` subclasses by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8857](https://github.com/pydantic/pydantic/pull/8857)* Use `PEP570` syntax
        by [@Viicos](https://github.com/Viicos) in [#8940](https://github.com/pydantic/pydantic/pull/8940)*
        Add `enum` and `type` to the JSON schema for single item literals by [@dmontagu](https://github.com/dmontagu)
        in [#8944](https://github.com/pydantic/pydantic/pull/8944)* Deprecate `update_json_schema`
        internal function by [@sydney-runkle](https://github.com/sydney-runkle) in
        [#9125](https://github.com/pydantic/pydantic/pull/9125)* Serialize duration
        to hour minute second, instead of just seconds by [@kakilangit](https://github.com/kakilangit)
        in [pydantic/speedate#50](https://github.com/pydantic/speedate/pull/50)* Trimming
        str before parsing to int and float by [@hungtsetse](https://github.com/hungtsetse)
        in [pydantic/pydantic-core#1203](https://github.com/pydantic/pydantic-core/pull/1203)####
        Performance* `enum` validator improvements by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#9045](https://github.com/pydantic/pydantic/pull/9045)* Move `enum` validation
        and serialization to Rust by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#9064](https://github.com/pydantic/pydantic/pull/9064)* Improve schema
        generation for nested dataclasses by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#9114](https://github.com/pydantic/pydantic/pull/9114)* Fast path for
        ASCII python string creation in JSON by [@samuelcolvin](https://github.com/samuelcolvin)
        in in [pydantic/jiter#72](https://github.com/pydantic/jiter/pull/72)* SIMD
        integer and string JSON parsing on `aarch64`(**Note:** SIMD on x86 will be
        implemented in a future release) by [@samuelcolvin](https://github.com/samuelcolvin)
        in in [pydantic/jiter#65](https://github.com/pydantic/jiter/pull/65)* Support
        JSON `Cow<str>` from `jiter` by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/pydantic-core#1231](https://github.com/pydantic/pydantic-core/pull/1231)*
        MAJOR performance improvement: update to PyO3 0.21 final by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/pydantic-core#1248](https://github.com/pydantic/pydantic-core/pull/1248)*
        cache Python strings by [@samuelcolvin](https://github.com/samuelcolvin) in
        [pydantic/pydantic-core#1240](https://github.com/pydantic/pydantic-core/pull/1240)####
        Fixes* Fix strict parsing for some `Sequence`s by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8614](https://github.com/pydantic/pydantic/pull/8614)* Add a check on
        the existence of `__qualname__` by [@anci3ntr0ck](https://github.com/anci3ntr0ck)
        in [#8642](https://github.com/pydantic/pydantic/pull/8642)* Handle `__pydantic_extra__`
        annotation being a string or inherited by [@alexmojaki](https://github.com/alexmojaki)
        in [#8659](https://github.com/pydantic/pydantic/pull/8659)* Fix json validation
        for `NameEmail` by [@Holi0317](https://github.com/Holi0317) in [#8650](https://github.com/pydantic/pydantic/pull/8650)*
        Fix type-safety of attribute access in `BaseModel` by [@bluenote10](https://github.com/bluenote10)
        in [#8651](https://github.com/pydantic/pydantic/pull/8651)* Fix bug with `mypy`
        plugin and `no_strict_optional = True` by [@dmontagu](https://github.com/dmontagu)
        in [#8666](https://github.com/pydantic/pydantic/pull/8666)* Fix `ByteSize`
        error `type` change by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8681](https://github.com/pydantic/pydantic/pull/8681)* Fix inheriting
        annotations in dataclasses by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8679](https://github.com/pydantic/pydantic/pull/8679)* Fix regression
        in core schema generation for indirect definition references by [@dmontagu](https://github.com/dmontagu)
        in [#8702](https://github.com/pydantic/pydantic/pull/8702)* Fix unsupported
        types bug with plain validator by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8710](https://github.com/pydantic/pydantic/pull/8710)* Reverting problematic
        fix from 2.6 release, fixing schema building bug by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8718](https://github.com/pydantic/pydantic/pull/8718)* fixes `__pydantic_config__`
        ignored for TypeDict by [@13sin](https://github.com/13sin) in [#8734](https://github.com/pydantic/pydantic/pull/8734)*
        Fix test failures with `pytest v8.0.0` due to `pytest.warns()` starting to
        work inside `pytest.raises()` by [@mgorny](https://github.com/mgorny) in [#8678](https://github.com/pydantic/pydantic/pull/8678)*
        Use `is_valid_field` from 1.x for `mypy` plugin by [@DanielNoord](https://github.com/DanielNoord)
        in [#8738](https://github.com/pydantic/pydantic/pull/8738)* Better-support
        `mypy` strict equality flag by [@dmontagu](https://github.com/dmontagu) in
        [#8799](https://github.com/pydantic/pydantic/pull/8799)* model_json_schema
        export with Annotated types misses 'required' parameters by [@LouisGobert](https://github.com/LouisGobert)
        in [#8793](https://github.com/pydantic/pydantic/pull/8793)* Fix default inclusion
        in `FieldInfo.__repr_args__` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8801](https://github.com/pydantic/pydantic/pull/8801)* Fix resolution
        of forward refs in dataclass base classes that are not present in the subclass
        module namespace by [@matsjoyce-refeyn](https://github.com/matsjoyce-refeyn)
        in [#8751](https://github.com/pydantic/pydantic/pull/8751)* Fix `BaseModel`
        type annotations to be resolvable by `typing.get_type_hints` by [@devmonkey22](https://github.com/devmonkey22)
        in [#7680](https://github.com/pydantic/pydantic/pull/7680)* Fix: allow empty
        string aliases with `AliasGenerator` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8810](https://github.com/pydantic/pydantic/pull/8810)* Fix test along
        with `date` -> `datetime` timezone assumption fix by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8823](https://github.com/pydantic/pydantic/pull/8823)* Fix deprecation
        warning with usage of `ast.Str` by [@Viicos](https://github.com/Viicos) in
        [#8837](https://github.com/pydantic/pydantic/pull/8837)* Add missing `deprecated`
        decorators by [@Viicos](https://github.com/Viicos) in [#8877](https://github.com/pydantic/pydantic/pull/8877)*
        Fix serialization of `NameEmail` if name includes an email address by [@NeevCohen](https://github.com/NeevCohen)
        in [#8860](https://github.com/pydantic/pydantic/pull/8860)* Add information
        about class in error message of schema generation by [@Czaki](https://github.com/Czaki)
        in [#8917](https://github.com/pydantic/pydantic/pull/8917)* Make `TypeAdapter`'s
        typing compatible with special forms by [@adriangb](https://github.com/adriangb)
        in [#8923](https://github.com/pydantic/pydantic/pull/8923)* Fix issue with
        config behavior being baked into the ref schema for `enum`s by [@dmontagu](https://github.com/dmontagu)
        in [#8920](https://github.com/pydantic/pydantic/pull/8920)* More helpful error
        re wrong `model_json_schema` usage by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8928](https://github.com/pydantic/pydantic/pull/8928)* Fix nested discriminated
        union schema gen, pt 2 by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8932](https://github.com/pydantic/pydantic/pull/8932)* Fix schema build
        for nested dataclasses / TypedDicts with discriminators by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8950](https://github.com/pydantic/pydantic/pull/8950)* Remove unnecessary
        logic for definitions schema gen with discriminated unions by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8951](https://github.com/pydantic/pydantic/pull/8951)* Fix handling of
        optionals in `mypy` plugin by [@dmontagu](https://github.com/dmontagu) in
        [#9008](https://github.com/pydantic/pydantic/pull/9008)* Fix `PlainSerializer`
        usage with std type constructor by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#9031](https://github.com/pydantic/pydantic/pull/9031)* Remove unnecessary
        warning for config in plugin by [@dmontagu](https://github.com/dmontagu) in
        [#9039](https://github.com/pydantic/pydantic/pull/9039)* Fix default value
        serializing by [@NeevCohen](https://github.com/NeevCohen) in [#9066](https://github.com/pydantic/pydantic/pull/9066)*
        Fix extra fields check in `Model.__getattr__()` by [@NeevCohen](https://github.com/NeevCohen)
        in [#9082](https://github.com/pydantic/pydantic/pull/9082)* Fix `ClassVar`
        forward ref inherited from parent class by [@alexmojaki](https://github.com/alexmojaki)
        in [#9097](https://github.com/pydantic/pydantic/pull/9097)* fix sequence like
        validator with strict `True` by [@andresliszt](https://github.com/andresliszt)
        in [#8977](https://github.com/pydantic/pydantic/pull/8977)* Improve warning
        message when a field name shadows a field in a parent model by [@chan-vince](https://github.com/chan-vince)
        in [#9105](https://github.com/pydantic/pydantic/pull/9105)* Do not warn about
        shadowed fields if they are not redefined in a child class by [@chan-vince](https://github.com/chan-vince)
        in [#9111](https://github.com/pydantic/pydantic/pull/9111)* Fix discriminated
        union bug with unsubstituted type var by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#9124](https://github.com/pydantic/pydantic/pull/9124)* Support serialization
        of `deque` when passed to `Sequence[blah blah blah]` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#9128](https://github.com/pydantic/pydantic/pull/9128)* Init private attributes
        from super-types in `model_post_init` by [@Viicos](https://github.com/Viicos)
        in [#9134](https://github.com/pydantic/pydantic/pull/9134)* fix `model_construct`
        with `validation_alias` by [@ornariece](https://github.com/ornariece) in [#9144](https://github.com/pydantic/pydantic/pull/9144)*
        Ensure json-schema generator handles `Literal` `null` types by [@bruno-f-cruz](https://github.com/bruno-f-cruz)
        in [#9135](https://github.com/pydantic/pydantic/pull/9135)* **Fixed in v2.7.0**:
        Fix allow extra generic by [@dmontagu](https://github.com/dmontagu) in [#9193](https://github.com/pydantic/pydantic/pull/9193)###
        New Contributors* [@hungtsetse](https://github.com/hungtsetse) made their
        first contribution in [#8546](https://github.com/pydantic/pydantic/pull/8546)*
        [@StrawHatDrag0n](https://github.com/StrawHatDrag0n) made their first contribution
        in [#8583](https://github.com/pydantic/pydantic/pull/8583)* [@anci3ntr0ck](https://github.com/anci3ntr0ck)
        made their first contribution in [#8642](https://github.com/pydantic/pydantic/pull/8642)*
        [@Holi0317](https://github.com/Holi0317) made their first contribution in
        [#8650](https://github.com/pydantic/pydantic/pull/8650)* [@bluenote10](https://github.com/bluenote10)
        made their first contribution in [#8651](https://github.com/pydantic/pydantic/pull/8651)*
        [@ADSteele916](https://github.com/ADSteele916) made their first contribution
        in [#8703](https://github.com/pydantic/pydantic/pull/8703)* [@musicinmybrain](https://github.com/musicinmybrain)
        made their first contribution in [#8731](https://github.com/pydantic/pydantic/pull/8731)*
        [@jks15satoshi](https://github.com/jks15satoshi) made their first contribution
        in [#8706](https://github.com/pydantic/pydantic/pull/8706)* [@13sin](https://github.com/13sin)
        made their first contribution in [#8734](https://github.com/pydantic/pydantic/pull/8734)*
        [@DanielNoord](https://github.com/DanielNoord) made their first contribution
        in [#8738](https://github.com/pydantic/pydantic/pull/8738)* [@conradogarciaberrotaran](https://github.com/conradogarciaberrotaran)
        made their first contribution in [#8519](https://github.com/pydantic/pydantic/pull/8519)*
        [@chris-griffin](https://github.com/chris-griffin) made their first contribution
        in [#8775](https://github.com/pydantic/pydantic/pull/8775)* [@LouisGobert](https://github.com/LouisGobert)
        made their first contribution in [#8793](https://github.com/pydantic/pydantic/pull/8793)*
        [@matsjoyce-refeyn](https://github.com/matsjoyce-refeyn) made their first
        contribution in [#8751](https://github.com/pydantic/pydantic/pull/8751)* [@devmonkey22](https://github.com/devmonkey22)
        made their first contribution in [#7680](https://github.com/pydantic/pydantic/pull/7680)*
        [@adamency](https://github.com/adamency) made their first contribution in
        [#8847](https://github.com/pydantic/pydantic/pull/8847)* [@MamfTheKramf](https://github.com/MamfTheKramf)
        made their first contribution in [#8851](https://github.com/pydantic/pydantic/pull/8851)*
        [@ornariece](https://github.com/ornariece) made their first contribution in
        [#9001](https://github.com/pydantic/pydantic/pull/9001)* [@alexeyt101](https://github.com/alexeyt101)
        made their first contribution in [#9004](https://github.com/pydantic/pydantic/pull/9004)*
        [@wannieman98](https://github.com/wannieman98) made their first contribution
        in [#8947](https://github.com/pydantic/pydantic/pull/8947)* [@solidguy7](https://github.com/solidguy7)
        made their first contribution in [#9062](https://github.com/pydantic/pydantic/pull/9062)*
        [@kloczek](https://github.com/kloczek) made their first contribution in [#9047](https://github.com/pydantic/pydantic/pull/9047)*
        [@jag-k](https://github.com/jag-k) made their first contribution in [#9053](https://github.com/pydantic/pydantic/pull/9053)*
        [@priya-gitTest](https://github.com/priya-gitTest) made their first contribution
        in [#9088](https://github.com/pydantic/pydantic/pull/9088)* [@Youssefares](https://github.com/Youssefares)
        made their first contribution in [#9023](https://github.com/pydantic/pydantic/pull/9023)*
        [@chan-vince](https://github.com/chan-vince) made their first contribution
        in [#9105](https://github.com/pydantic/pydantic/pull/9105)* [@bruno-f-cruz](https://github.com/bruno-f-cruz)
        made their first contribution in [#9135](https://github.com/pydantic/pydantic/pull/9135)*
        [@Lance-Drane](https://github.com/Lance-Drane) made their first contribution
        in [#9166](https://github.com/pydantic/pydantic/pull/9166)## v2.7.0b1 (2024-04-03)Pre-release,
        see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.7.0b1)
        for details.## v2.6.4 (2024-03-12)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.4)###
        What's Changed#### Fixes* Fix usage of `AliasGenerator` with `computed_field`
        decorator by [@sydney-runkle](https://github.com/sydney-runkle) in [#8806](https://github.com/pydantic/pydantic/pull/8806)*
        Fix nested discriminated union schema gen, pt 2 by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8932](https://github.com/pydantic/pydantic/pull/8932)* Fix bug with no_strict_optional=True
        caused by API deferral by [@dmontagu](https://github.com/dmontagu) in [#8826](https://github.com/pydantic/pydantic/pull/8826)##
        v2.6.3 (2024-02-27)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.3)###
        What's Changed#### Packaging* Update `pydantic-settings` version in the docs
        by [@hramezani](https://github.com/hramezani) in [#8906](https://github.com/pydantic/pydantic/pull/8906)####
        Fixes* Fix discriminated union schema gen bug by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8904](https://github.com/pydantic/pydantic/pull/8904)## v2.6.2 (2024-02-23)[GitHub
        release](https://github.com/pydantic/pydantic/releases/tag/v2.6.2)### What's
        Changed#### Packaging* Upgrade to `pydantic-core` 2.16.3 by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8879](https://github.com/pydantic/pydantic/pull/8879)#### Fixes* 'YYYY-MM-DD'
        date string coerced to datetime shouldn't infer timezone by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1193](https://github.com/pydantic/pydantic-core/pull/1193)##
        v2.6.1 (2024-02-05)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.1)###
        What's Changed#### Packaging* Upgrade to `pydantic-core` 2.16.2 by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8717](https://github.com/pydantic/pydantic/pull/8717)#### Fixes* Fix
        bug with `mypy` plugin and `no_strict_optional = True` by [@dmontagu](https://github.com/dmontagu)
        in [#8666](https://github.com/pydantic/pydantic/pull/8666)* Fix `ByteSize`
        error `type` change by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8681](https://github.com/pydantic/pydantic/pull/8681)* Fix inheriting
        `Field` annotations in dataclasses by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8679](https://github.com/pydantic/pydantic/pull/8679)* Fix regression
        in core schema generation for indirect definition references by [@dmontagu](https://github.com/dmontagu)
        in [#8702](https://github.com/pydantic/pydantic/pull/8702)* Fix unsupported
        types bug with `PlainValidator` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8710](https://github.com/pydantic/pydantic/pull/8710)* Reverting problematic
        fix from 2.6 release, fixing schema building bug by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8718](https://github.com/pydantic/pydantic/pull/8718)* Fix warning for
        tuple of wrong size in `Union` by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/pydantic-core#1174](https://github.com/pydantic/pydantic-core/pull/1174)*
        Fix `computed_field` JSON serializer `exclude_none` behavior by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1187](https://github.com/pydantic/pydantic-core/pull/1187)##
        v2.6.0 (2024-01-23)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.0)The
        code released in v2.6.0 is practically identical to that of v2.6.0b1.### What's
        Changed#### Packaging* Check for `email-validator` version >= 2.0 by [@commonism](https://github.com/commonism)
        in [#6033](https://github.com/pydantic/pydantic/pull/6033)* Upgrade `ruff``
        target version to Python 3.8 by [@Elkiwa](https://github.com/Elkiwa) in [#8341](https://github.com/pydantic/pydantic/pull/8341)*
        Update to `pydantic-extra-types==2.4.1` by [@yezz123](https://github.com/yezz123)
        in [#8478](https://github.com/pydantic/pydantic/pull/8478)* Update to `pyright==1.1.345`
        by [@Viicos](https://github.com/Viicos) in [#8453](https://github.com/pydantic/pydantic/pull/8453)*
        Update pydantic-core from 2.14.6 to 2.16.1, significant changes from these
        updates are described below, full changelog [here](https://github.com/pydantic/pydantic-core/compare/v2.14.6...v2.16.1)####
        New Features* Add `NatsDsn` by [@ekeew](https://github.com/ekeew) in [#6874](https://github.com/pydantic/pydantic/pull/6874)*
        Add `ConfigDict.ser_json_inf_nan` by [@davidhewitt](https://github.com/davidhewitt)
        in [#8159](https://github.com/pydantic/pydantic/pull/8159)* Add `types.OnErrorOmit`
        by [@adriangb](https://github.com/adriangb) in [#8222](https://github.com/pydantic/pydantic/pull/8222)*
        Support `AliasGenerator` usage by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8282](https://github.com/pydantic/pydantic/pull/8282)* Add Pydantic People
        Page to docs by [@sydney-runkle](https://github.com/sydney-runkle) in [#8345](https://github.com/pydantic/pydantic/pull/8345)*
        Support `yyyy-MM-DD` datetime parsing by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8404](https://github.com/pydantic/pydantic/pull/8404)* Added bits conversions
        to the `ByteSize` class [#8415](https://github.com/pydantic/pydantic/issues/8415)
        by [@luca-matei](https://github.com/luca-matei) in [#8507](https://github.com/pydantic/pydantic/pull/8507)*
        Enable json schema creation with type `ByteSize` by [@geospackle](https://github.com/geospackle)
        in [#8537](https://github.com/pydantic/pydantic/pull/8537)* Add `eval_type_backport`
        to handle union operator and builtin generic subscripting in older Pythons
        by [@alexmojaki](https://github.com/alexmojaki) in [#8209](https://github.com/pydantic/pydantic/pull/8209)*
        Add support for `dataclass` fields `init` by [@dmontagu](https://github.com/dmontagu)
        in [#8552](https://github.com/pydantic/pydantic/pull/8552)* Implement pickling
        for ValidationError by [@davidhewitt](https://github.com/davidhewitt) in
        [pydantic/pydantic-core#1119](https://github.com/pydantic/pydantic-core/pull/1119)*
        Add unified tuple validator that can handle \"variadic\" tuples via PEP-646
        by [@dmontagu](https://github.com/dmontagu) in [pydantic/pydantic-core#865](https://github.com/pydantic/pydantic-core/pull/865)####
        Changes* Drop Python3.7 support by [@hramezani](https://github.com/hramezani)
        in [#7188](https://github.com/pydantic/pydantic/pull/7188)* Drop Python 3.7,
        and PyPy 3.7 and 3.8 by [@davidhewitt](https://github.com/davidhewitt) in
        [pydantic/pydantic-core#1129](https://github.com/pydantic/pydantic-core/pull/1129)*
        Use positional-only `self` in `BaseModel` constructor, so no field name can
        ever conflict with it by [@ariebovenberg](https://github.com/ariebovenberg)
        in [#8072](https://github.com/pydantic/pydantic/pull/8072)* Make `@validate_call`
        return a function instead of a custom descriptor - fixes binding issue with
        inheritance and adds `self/cls` argument to validation errors by [@alexmojaki](https://github.com/alexmojaki)
        in [#8268](https://github.com/pydantic/pydantic/pull/8268)* Exclude `BaseModel`
        docstring from JSON schema description by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8352](https://github.com/pydantic/pydantic/pull/8352)* Introducing `classproperty`
        decorator for `model_computed_fields` by [@Jocelyn-Gas](https://github.com/Jocelyn-Gas)
        in [#8437](https://github.com/pydantic/pydantic/pull/8437)* Explicitly raise
        an error if field names clashes with types by [@Viicos](https://github.com/Viicos)
        in [#8243](https://github.com/pydantic/pydantic/pull/8243)* Use stricter serializer
        for unions of simple types by [@alexdrydew](https://github.com/alexdrydew)
        [pydantic/pydantic-core#1132](https://github.com/pydantic/pydantic-core/pull/1132)####
        Performance* Add Codspeed profiling Actions workflow  by [@lambertsbennett](https://github.com/lambertsbennett)
        in [#8054](https://github.com/pydantic/pydantic/pull/8054)* Improve `int`
        extraction by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/pydantic-core#1155](https://github.com/pydantic/pydantic-core/pull/1155)*
        Improve performance of recursion guard by [@samuelcolvin](https://github.com/samuelcolvin)
        in [pydantic/pydantic-core#1156](https://github.com/pydantic/pydantic-core/pull/1156)*
        `dataclass` serialization speedups by [@samuelcolvin](https://github.com/samuelcolvin)
        in [pydantic/pydantic-core#1162](https://github.com/pydantic/pydantic-core/pull/1162)*
        Avoid `HashMap` creation when looking up small JSON objects in `LazyIndexMaps`
        by [@samuelcolvin](https://github.com/samuelcolvin) in [pydantic/jiter#55](https://github.com/pydantic/jiter/pull/55)*
        use hashbrown to speedup python string caching by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/jiter#51](https://github.com/pydantic/jiter/pull/51)* Replace
        `Peak` with more efficient `Peek` by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/jiter#48](https://github.com/pydantic/jiter/pull/48)#### Fixes*
        Move `getattr` warning in deprecated `BaseConfig` by [@tlambert03](https://github.com/tlambert03)
        in [#7183](https://github.com/pydantic/pydantic/pull/7183)* Only hash `model_fields`,
        not whole `__dict__` by [@alexmojaki](https://github.com/alexmojaki) in [#7786](https://github.com/pydantic/pydantic/pull/7786)*
        Fix mishandling of unions while freezing types in the `mypy` plugin by [@dmontagu](https://github.com/dmontagu)
        in [#7411](https://github.com/pydantic/pydantic/pull/7411)* Fix `mypy` error
        on untyped `ClassVar` by [@vincent-hachin-wmx](https://github.com/vincent-hachin-wmx)
        in [#8138](https://github.com/pydantic/pydantic/pull/8138)* Only compare pydantic
        fields in `BaseModel.__eq__` instead of whole `__dict__` by [@QuentinSoubeyranAqemia](https://github.com/QuentinSoubeyranAqemia)
        in [#7825](https://github.com/pydantic/pydantic/pull/7825)* Update `strict`
        docstring in `model_validate` method. by [@LukeTonin](https://github.com/LukeTonin)
        in [#8223](https://github.com/pydantic/pydantic/pull/8223)* Fix overload position
        of `computed_field` by [@Viicos](https://github.com/Viicos) in [#8227](https://github.com/pydantic/pydantic/pull/8227)*
        Fix custom type type casting used in multiple attributes by [@ianhfc](https://github.com/ianhfc)
        in [#8066](https://github.com/pydantic/pydantic/pull/8066)* Fix issue not
        allowing `validate_call` decorator to be dynamically assigned to a class method
        by [@jusexton](https://github.com/jusexton) in [#8249](https://github.com/pydantic/pydantic/pull/8249)*
        Fix issue `unittest.mock` deprecation warnings  by [@ibleedicare](https://github.com/ibleedicare)
        in [#8262](https://github.com/pydantic/pydantic/pull/8262)* Added tests for
        the case `JsonValue` contains subclassed primitive values by [@jusexton](https://github.com/jusexton)
        in [#8286](https://github.com/pydantic/pydantic/pull/8286)* Fix `mypy` error
        on free before validator (classmethod) by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8285](https://github.com/pydantic/pydantic/pull/8285)* Fix `to_snake`
        conversion by [@jevins09](https://github.com/jevins09) in [#8316](https://github.com/pydantic/pydantic/pull/8316)*
        Fix type annotation of `ModelMetaclass.__prepare__` by [@slanzmich](https://github.com/slanzmich)
        in [#8305](https://github.com/pydantic/pydantic/pull/8305)* Disallow `config`
        specification when initializing a `TypeAdapter` when the annotated type has
        config already by [@sydney-runkle](https://github.com/sydney-runkle) in [#8365](https://github.com/pydantic/pydantic/pull/8365)*
        Fix a naming issue with JSON schema for generics parametrized by recursive
        type aliases by [@dmontagu](https://github.com/dmontagu) in [#8389](https://github.com/pydantic/pydantic/pull/8389)*
        Fix type annotation in pydantic people script by [@shenxiangzhuang](https://github.com/shenxiangzhuang)
        in [#8402](https://github.com/pydantic/pydantic/pull/8402)* Add support for
        field `alias` in `dataclass` signature by [@NeevCohen](https://github.com/NeevCohen)
        in [#8387](https://github.com/pydantic/pydantic/pull/8387)* Fix bug with schema
        generation with `Field(...)` in a forward ref by [@dmontagu](https://github.com/dmontagu)
        in [#8494](https://github.com/pydantic/pydantic/pull/8494)* Fix ordering of
        keys in `__dict__` with `model_construct` call by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8500](https://github.com/pydantic/pydantic/pull/8500)* Fix module `path_type`
        creation when globals does not contain `__name__` by [@hramezani](https://github.com/hramezani)
        in [#8470](https://github.com/pydantic/pydantic/pull/8470)* Fix for namespace
        issue with dataclasses with `from __future__ import annotations` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8513](https://github.com/pydantic/pydantic/pull/8513)* Fix: make function
        validator types positional-only by [@pmmmwh](https://github.com/pmmmwh) in
        [#8479](https://github.com/pydantic/pydantic/pull/8479)* Fix usage of `@deprecated`
        by [@Viicos](https://github.com/Viicos) in [#8294](https://github.com/pydantic/pydantic/pull/8294)*
        Add more support for private attributes in `model_construct` call by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8525](https://github.com/pydantic/pydantic/pull/8525)* Use a stack for
        the types namespace by [@dmontagu](https://github.com/dmontagu) in [#8378](https://github.com/pydantic/pydantic/pull/8378)*
        Fix schema-building bug with `TypeAliasType` for types with refs by [@dmontagu](https://github.com/dmontagu)
        in [#8526](https://github.com/pydantic/pydantic/pull/8526)* Support `pydantic.Field(repr=False)`
        in dataclasses by [@tigeryy2](https://github.com/tigeryy2) in [#8511](https://github.com/pydantic/pydantic/pull/8511)*
        Override `dataclass_transform` behavior for `RootModel` by [@Viicos](https://github.com/Viicos)
        in [#8163](https://github.com/pydantic/pydantic/pull/8163)* Refactor signature
        generation for simplicity by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8572](https://github.com/pydantic/pydantic/pull/8572)* Fix ordering bug
        of PlainValidator annotation by [@Anvil](https://github.com/Anvil) in [#8567](https://github.com/pydantic/pydantic/pull/8567)*
        Fix `exclude_none` for json serialization of `computed_field`s by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1098](https://github.com/pydantic/pydantic-core/pull/1098)*
        Support yyyy-MM-DD string for datetimes by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1124](https://github.com/pydantic/pydantic-core/pull/1124)*
        Tweak ordering of definitions in generated schemas by [@StrawHatDrag0n](https://github.com/StrawHatDrag0n)
        in [#8583](https://github.com/pydantic/pydantic/pull/8583)### New Contributors####
        `pydantic`* [@ekeew](https://github.com/ekeew) made their first contribution
        in [#6874](https://github.com/pydantic/pydantic/pull/6874)* [@lambertsbennett](https://github.com/lambertsbennett)
        made their first contribution in [#8054](https://github.com/pydantic/pydantic/pull/8054)*
        [@vincent-hachin-wmx](https://github.com/vincent-hachin-wmx) made their first
        contribution in [#8138](https://github.com/pydantic/pydantic/pull/8138)* [@QuentinSoubeyranAqemia](https://github.com/QuentinSoubeyranAqemia)
        made their first contribution in [#7825](https://github.com/pydantic/pydantic/pull/7825)*
        [@ariebovenberg](https://github.com/ariebovenberg) made their first contribution
        in [#8072](https://github.com/pydantic/pydantic/pull/8072)* [@LukeTonin](https://github.com/LukeTonin)
        made their first contribution in [#8223](https://github.com/pydantic/pydantic/pull/8223)*
        [@denisart](https://github.com/denisart) made their first contribution in
        [#8231](https://github.com/pydantic/pydantic/pull/8231)* [@ianhfc](https://github.com/ianhfc)
        made their first contribution in [#8066](https://github.com/pydantic/pydantic/pull/8066)*
        [@eonu](https://github.com/eonu) made their first contribution in [#8255](https://github.com/pydantic/pydantic/pull/8255)*
        [@amandahla](https://github.com/amandahla) made their first contribution in
        [#8263](https://github.com/pydantic/pydantic/pull/8263)* [@ibleedicare](https://github.com/ibleedicare)
        made their first contribution in [#8262](https://github.com/pydantic/pydantic/pull/8262)*
        [@jevins09](https://github.com/jevins09) made their first contribution in
        [#8316](https://github.com/pydantic/pydantic/pull/8316)* [@cuu508](https://github.com/cuu508)
        made their first contribution in [#8322](https://github.com/pydantic/pydantic/pull/8322)*
        [@slanzmich](https://github.com/slanzmich) made their first contribution in
        [#8305](https://github.com/pydantic/pydantic/pull/8305)* [@jensenbox](https://github.com/jensenbox)
        made their first contribution in [#8331](https://github.com/pydantic/pydantic/pull/8331)*
        [@szepeviktor](https://github.com/szepeviktor) made their first contribution
        in [#8356](https://github.com/pydantic/pydantic/pull/8356)* [@Elkiwa](https://github.com/Elkiwa)
        made their first contribution in [#8341](https://github.com/pydantic/pydantic/pull/8341)*
        [@parhamfh](https://github.com/parhamfh) made their first contribution in
        [#8395](https://github.com/pydantic/pydantic/pull/8395)* [@shenxiangzhuang](https://github.com/shenxiangzhuang)
        made their first contribution in [#8402](https://github.com/pydantic/pydantic/pull/8402)*
        [@NeevCohen](https://github.com/NeevCohen) made their first contribution in
        [#8387](https://github.com/pydantic/pydantic/pull/8387)* [@zby](https://github.com/zby)
        made their first contribution in [#8497](https://github.com/pydantic/pydantic/pull/8497)*
        [@patelnets](https://github.com/patelnets) made their first contribution in
        [#8491](https://github.com/pydantic/pydantic/pull/8491)* [@edwardwli](https://github.com/edwardwli)
        made their first contribution in [#8503](https://github.com/pydantic/pydantic/pull/8503)*
        [@luca-matei](https://github.com/luca-matei) made their first contribution
        in [#8507](https://github.com/pydantic/pydantic/pull/8507)* [@Jocelyn-Gas](https://github.com/Jocelyn-Gas)
        made their first contribution in [#8437](https://github.com/pydantic/pydantic/pull/8437)*
        [@bL34cHig0](https://github.com/bL34cHig0) made their first contribution in
        [#8501](https://github.com/pydantic/pydantic/pull/8501)* [@tigeryy2](https://github.com/tigeryy2)
        made their first contribution in [#8511](https://github.com/pydantic/pydantic/pull/8511)*
        [@geospackle](https://github.com/geospackle) made their first contribution
        in [#8537](https://github.com/pydantic/pydantic/pull/8537)* [@Anvil](https://github.com/Anvil)
        made their first contribution in [#8567](https://github.com/pydantic/pydantic/pull/8567)*
        [@hungtsetse](https://github.com/hungtsetse) made their first contribution
        in [#8546](https://github.com/pydantic/pydantic/pull/8546)* [@StrawHatDrag0n](https://github.com/StrawHatDrag0n)
        made their first contribution in [#8583](https://github.com/pydantic/pydantic/pull/8583)####
        `pydantic-core`* [@mariuswinger](https://github.com/mariuswinger) made their
        first contribution in [pydantic/pydantic-core#1087](https://github.com/pydantic/pydantic-core/pull/1087)*
        [@adamchainz](https://github.com/adamchainz) made their first contribution
        in [pydantic/pydantic-core#1090](https://github.com/pydantic/pydantic-core/pull/1090)*
        [@akx](https://github.com/akx) made their first contribution in [pydantic/pydantic-core#1123](https://github.com/pydantic/pydantic-core/pull/1123)##
        v2.6.0b1 (2024-01-19)Pre-release, see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.6.0b1)
        for details.## v2.5.3 (2023-12-22)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.3)###
        What's Changed#### Packaging* uprev `pydantic-core` to 2.14.6#### Fixes* Fix
        memory leak with recursive definitions creating reference cycles by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/pydantic-core#1125](https://github.com/pydantic/pydantic-core/pull/1125)##
        v2.5.2 (2023-11-22)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.2)###
        What's Changed#### Packaging* uprev `pydantic-core` to 2.14.5#### New Features*
        Add `ConfigDict.ser_json_inf_nan` by [@davidhewitt](https://github.com/davidhewitt)
        in [#8159](https://github.com/pydantic/pydantic/pull/8159)#### Fixes* Fix
        validation of `Literal` from JSON keys when used as `dict` key by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1075](https://github.com/pydantic/pydantic-core/pull/1075)*
        Fix bug re `custom_init` on members of `Union` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1076](https://github.com/pydantic/pydantic-core/pull/1076)*
        Fix `JsonValue` `bool` serialization by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8190](https://github.com/pydantic/pydantic/pull/8159)* Fix handling of
        unhashable inputs with `Literal` in `Union`s by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1089](https://github.com/pydantic/pydantic-core/pull/1089)##
        v2.5.1 (2023-11-15)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.1)###
        What's Changed#### Packaging* uprev pydantic-core to 2.14.3 by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#8120](https://github.com/pydantic/pydantic/pull/8120)#### Fixes* Fix
        package description limit by [@dmontagu](https://github.com/dmontagu) in [#8097](https://github.com/pydantic/pydantic/pull/8097)*
        Fix `ValidateCallWrapper` error when creating a model which has a [@validate_call](https://github.com/validate_call)
        wrapped field annotation by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#8110](https://github.com/pydantic/pydantic/pull/8110)## v2.5.0 (2023-11-13)[GitHub
        release](https://github.com/pydantic/pydantic/releases/tag/v2.5.0)The code
        released in v2.5.0 is functionally identical to that of v2.5.0b1.### What's
        Changed#### Packaging* Update pydantic-core from 2.10.1 to 2.14.1, significant
        changes from these updates are described below, full changelog [here](https://github.com/pydantic/pydantic-core/compare/v2.10.1...v2.14.1)*
        Update to `pyright==1.1.335` by [@Viicos](https://github.com/Viicos) in [#8075](https://github.com/pydantic/pydantic/pull/8075)####
        New Features* Allow plugins to catch non ValidationError errors by [@adriangb](https://github.com/adriangb)
        in [#7806](https://github.com/pydantic/pydantic/pull/7806)* Support `__doc__`
        argument in `create_model()` by [@chris-spann](https://github.com/chris-spann)
        in [#7863](https://github.com/pydantic/pydantic/pull/7863)* Expose `regex_engine`
        flag - meaning you can use with the Rust or Python regex libraries in constraints
        by [@utkini](https://github.com/utkini) in [#7768](https://github.com/pydantic/pydantic/pull/7768)*
        Save return type generated from type annotation in `ComputedFieldInfo` by
        [@alexmojaki](https://github.com/alexmojaki) in [#7889](https://github.com/pydantic/pydantic/pull/7889)*
        Adopting `ruff` formatter by [@Luca-Blight](https://github.com/Luca-Blight)
        in [#7930](https://github.com/pydantic/pydantic/pull/7930)* Added `validation_error_cause`
        to config by [@zakstucke](https://github.com/zakstucke) in [#7626](https://github.com/pydantic/pydantic/pull/7626)*
        Make path of the item to validate available in plugin by [@hramezani](https://github.com/hramezani)
        in [#7861](https://github.com/pydantic/pydantic/pull/7861)* Add `CallableDiscriminator`
        and `Tag` by [@dmontagu](https://github.com/dmontagu) in [#7983](https://github.com/pydantic/pydantic/pull/7983)
        \ * `CallableDiscriminator` renamed to `Discriminator` by [@dmontagu](https://github.com/dmontagu)
        in [#8047](https://github.com/pydantic/pydantic/pull/8047)* Make union case
        tags affect union error messages by [@dmontagu](https://github.com/dmontagu)
        in [#8001](https://github.com/pydantic/pydantic/pull/8001)* Add `examples`
        and `json_schema_extra` to `@computed_field` by [@alexmojaki](https://github.com/alexmojaki)
        in [#8013](https://github.com/pydantic/pydantic/pull/8013)* Add `JsonValue`
        type by [@dmontagu](https://github.com/dmontagu) in [#7998](https://github.com/pydantic/pydantic/pull/7998)*
        Allow `str` as argument to `Discriminator` by [@dmontagu](https://github.com/dmontagu)
        in [#8047](https://github.com/pydantic/pydantic/pull/8047)* Add `SchemaSerializer.__reduce__`
        method to enable pickle serialization by [@edoakes](https://github.com/edoakes)
        in [pydantic/pydantic-core#1006](https://github.com/pydantic/pydantic-core/pull/1006)####
        Changes* **Significant Change:** replace `ultra_strict` with new smart union
        implementation, the way unions are validated has changed significantly to
        improve performance and correctness, we have worked hard to absolutely minimise
        the number of cases where behaviour has changed, see the PR for details -
        by [@davidhewitt](https://github.com/davidhewitt) in [pydantic/pydantic-core#867](https://github.com/pydantic/pydantic-core/pull/867)*
        Add support for instance method reassignment when `extra='allow'` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7683](https://github.com/pydantic/pydantic/pull/7683)* Support JSON schema
        generation for `Enum` types with no cases by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7927](https://github.com/pydantic/pydantic/pull/7927)* Warn if a class
        inherits from `Generic` before `BaseModel` by [@alexmojaki](https://github.com/alexmojaki)
        in [#7891](https://github.com/pydantic/pydantic/pull/7891)#### Performance*
        New custom JSON parser, `jiter` by [@samuelcolvin](https://github.com/samuelcolvin)
        in [pydantic/pydantic-core#974](https://github.com/pydantic/pydantic-core/pull/974)*
        PGO build for MacOS M1 by [@samuelcolvin](https://github.com/samuelcolvin)
        in [pydantic/pydantic-core#1063](https://github.com/pydantic/pydantic-core/pull/1063)*
        Use `__getattr__` for all package imports, improve import time by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#7947](https://github.com/pydantic/pydantic/pull/7947)#### Fixes* Fix
        `mypy` issue with subclasses of `RootModel` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7677](https://github.com/pydantic/pydantic/pull/7677)* Properly rebuild
        the `FieldInfo` when a forward ref gets evaluated by [@dmontagu](https://github.com/dmontagu)
        in [#7698](https://github.com/pydantic/pydantic/pull/7698)* Fix failure to
        load `SecretStr` from JSON (regression in v2.4) by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7729](https://github.com/pydantic/pydantic/pull/7729)* Fix `defer_build`
        behavior with `TypeAdapter` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7736](https://github.com/pydantic/pydantic/pull/7736)* Improve compatibility
        with legacy `mypy` versions by [@dmontagu](https://github.com/dmontagu) in
        [#7742](https://github.com/pydantic/pydantic/pull/7742)* Fix: update `TypeVar`
        handling when default is not set by [@pmmmwh](https://github.com/pmmmwh) in
        [#7719](https://github.com/pydantic/pydantic/pull/7719)* Support specification
        of `strict` on `Enum` type fields by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7761](https://github.com/pydantic/pydantic/pull/7761)* Wrap `weakref.ref`
        instead of subclassing to fix `cloudpickle` serialization by [@edoakes](https://github.com/edoakes)
        in [#7780](https://github.com/pydantic/pydantic/pull/7780)* Keep values of
        private attributes set within `model_post_init` in subclasses by [@alexmojaki](https://github.com/alexmojaki)
        in [#7775](https://github.com/pydantic/pydantic/pull/7775)* Add more specific
        type for non-callable `json_schema_extra` by [@alexmojaki](https://github.com/alexmojaki)
        in [#7803](https://github.com/pydantic/pydantic/pull/7803)* Raise an error
        when deleting frozen (model) fields by [@alexmojaki](https://github.com/alexmojaki)
        in [#7800](https://github.com/pydantic/pydantic/pull/7800)* Fix schema sorting
        bug with default values by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7817](https://github.com/pydantic/pydantic/pull/7817)* Use generated
        alias for aliases that are not specified otherwise by [@alexmojaki](https://github.com/alexmojaki)
        in [#7802](https://github.com/pydantic/pydantic/pull/7802)* Support `strict`
        specification for `UUID` types by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7865](https://github.com/pydantic/pydantic/pull/7865)* JSON schema: fix
        extra parameter handling by [@me-and](https://github.com/me-and) in [#7810](https://github.com/pydantic/pydantic/pull/7810)*
        Fix: support `pydantic.Field(kw_only=True)` with inherited dataclasses by
        [@PrettyWood](https://github.com/PrettyWood) in [#7827](https://github.com/pydantic/pydantic/pull/7827)*
        Support `validate_call` decorator for methods in classes with `__slots__`
        by [@sydney-runkle](https://github.com/sydney-runkle) in [#7883](https://github.com/pydantic/pydantic/pull/7883)*
        Fix pydantic dataclass problem with `dataclasses.field` default by [@hramezani](https://github.com/hramezani)
        in [#7898](https://github.com/pydantic/pydantic/pull/7898)* Fix schema generation
        for generics with union type bounds by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7899](https://github.com/pydantic/pydantic/pull/7899)* Fix version for
        `importlib_metadata` on python 3.7 by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7904](https://github.com/pydantic/pydantic/pull/7904)* Support `|` operator
        (Union) in PydanticRecursiveRef by [@alexmojaki](https://github.com/alexmojaki)
        in [#7892](https://github.com/pydantic/pydantic/pull/7892)* Fix `display_as_type`
        for `TypeAliasType` in python 3.12 by [@dmontagu](https://github.com/dmontagu)
        in [#7929](https://github.com/pydantic/pydantic/pull/7929)* Add support for
        `NotRequired` generics in `TypedDict` by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7932](https://github.com/pydantic/pydantic/pull/7932)* Make generic `TypeAliasType`
        specifications produce different schema definitions by [@alexdrydew](https://github.com/alexdrydew)
        in [#7893](https://github.com/pydantic/pydantic/pull/7893)* Added fix for
        signature of inherited dataclass by [@howsunjow](https://github.com/howsunjow)
        in [#7925](https://github.com/pydantic/pydantic/pull/7925)* Make the model
        name generation more robust in JSON schema by [@joakimnordling](https://github.com/joakimnordling)
        in [#7881](https://github.com/pydantic/pydantic/pull/7881)* Fix plurals in
        validation error messages (in tests) by [@Iipin](https://github.com/Iipin)
        in [#7972](https://github.com/pydantic/pydantic/pull/7972)* `PrivateAttr`
        is passed from `Annotated` default position by [@tabassco](https://github.com/tabassco)
        in [#8004](https://github.com/pydantic/pydantic/pull/8004)* Don't decode bytes
        (which may not be UTF8) when displaying SecretBytes by [@alexmojaki](https://github.com/alexmojaki)
        in [#8012](https://github.com/pydantic/pydantic/pull/8012)* Use `classmethod`
        instead of `classmethod[Any, Any, Any]` by [@Mr-Pepe](https://github.com/Mr-Pepe)
        in [#7979](https://github.com/pydantic/pydantic/pull/7979)* Clearer error
        on invalid Plugin by [@samuelcolvin](https://github.com/samuelcolvin) in [#8023](https://github.com/pydantic/pydantic/pull/8023)*
        Correct pydantic dataclasses import by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#8027](https://github.com/pydantic/pydantic/pull/8027)* Fix misbehavior
        for models referencing redefined type aliases by [@dmontagu](https://github.com/dmontagu)
        in [#8050](https://github.com/pydantic/pydantic/pull/8050)* Fix `Optional`
        field with `validate_default` only performing one field validation by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1002](https://github.com/pydantic/pydantic-core/pull/1002)*
        Fix `definition-ref` bug with `Dict` keys by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1014](https://github.com/pydantic/pydantic-core/pull/1014)*
        Fix bug allowing validation of `bool` types with `coerce_numbers_to_str=True`
        by [@sydney-runkle](https://github.com/sydney-runkle) in [pydantic/pydantic-core#1017](https://github.com/pydantic/pydantic-core/pull/1017)*
        Don't accept `NaN` in float and decimal constraints by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/pydantic-core#1037](https://github.com/pydantic/pydantic-core/pull/1037)*
        Add `lax_str` and `lax_int` support for enum values not inherited from str/int
        by [@michaelhly](https://github.com/michaelhly) in [pydantic/pydantic-core#1015](https://github.com/pydantic/pydantic-core/pull/1015)*
        Support subclasses in lists in `Union` of `List` types by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1039](https://github.com/pydantic/pydantic-core/pull/1039)*
        Allow validation against `max_digits` and `decimals` to pass if normalized
        or non-normalized input is valid by [@sydney-runkle](https://github.com/sydney-runkle)
        in [pydantic/pydantic-core#1049](https://github.com/pydantic/pydantic-core/pull/1049)*
        Fix: proper pluralization in ValidationError messages by [@Iipin](https://github.com/Iipin)
        in [pydantic/pydantic-core#1050](https://github.com/pydantic/pydantic-core/pull/1050)*
        Disallow the string `'-'` as `datetime` input by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/speedate#52](https://github.com/pydantic/speedate/pull/52) &
        [pydantic/pydantic-core#1060](https://github.com/pydantic/pydantic-core/pull/1060)*
        Fix: NaN and Inf float serialization by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/pydantic-core#1062](https://github.com/pydantic/pydantic-core/pull/1062)*
        Restore manylinux-compatible PGO builds by [@davidhewitt](https://github.com/davidhewitt)
        in [pydantic/pydantic-core#1068](https://github.com/pydantic/pydantic-core/pull/1068)###
        New Contributors#### `pydantic`* [@schneebuzz](https://github.com/schneebuzz)
        made their first contribution in [#7699](https://github.com/pydantic/pydantic/pull/7699)*
        [@edoakes](https://github.com/edoakes) made their first contribution in [#7780](https://github.com/pydantic/pydantic/pull/7780)*
        [@alexmojaki](https://github.com/alexmojaki) made their first contribution
        in [#7775](https://github.com/pydantic/pydantic/pull/7775)* [@NickG123](https://github.com/NickG123)
        made their first contribution in [#7751](https://github.com/pydantic/pydantic/pull/7751)*
        [@gowthamgts](https://github.com/gowthamgts) made their first contribution
        in [#7830](https://github.com/pydantic/pydantic/pull/7830)* [@jamesbraza](https://github.com/jamesbraza)
        made their first contribution in [#7848](https://github.com/pydantic/pydantic/pull/7848)*
        [@laundmo](https://github.com/laundmo) made their first contribution in [#7850](https://github.com/pydantic/pydantic/pull/7850)*
        [@rahmatnazali](https://github.com/rahmatnazali) made their first contribution
        in [#7870](https://github.com/pydantic/pydantic/pull/7870)* [@waterfountain1996](https://github.com/waterfountain1996)
        made their first contribution in [#7878](https://github.com/pydantic/pydantic/pull/7878)*
        [@chris-spann](https://github.com/chris-spann) made their first contribution
        in [#7863](https://github.com/pydantic/pydantic/pull/7863)* [@me-and](https://github.com/me-and)
        made their first contribution in [#7810](https://github.com/pydantic/pydantic/pull/7810)*
        [@utkini](https://github.com/utkini) made their first contribution in [#7768](https://github.com/pydantic/pydantic/pull/7768)*
        [@bn-l](https://github.com/bn-l) made their first contribution in [#7744](https://github.com/pydantic/pydantic/pull/7744)*
        [@alexdrydew](https://github.com/alexdrydew) made their first contribution
        in [#7893](https://github.com/pydantic/pydantic/pull/7893)* [@Luca-Blight](https://github.com/Luca-Blight)
        made their first contribution in [#7930](https://github.com/pydantic/pydantic/pull/7930)*
        [@howsunjow](https://github.com/howsunjow) made their first contribution in
        [#7925](https://github.com/pydantic/pydantic/pull/7925)* [@joakimnordling](https://github.com/joakimnordling)
        made their first contribution in [#7881](https://github.com/pydantic/pydantic/pull/7881)*
        [@icfly2](https://github.com/icfly2) made their first contribution in [#7976](https://github.com/pydantic/pydantic/pull/7976)*
        [@Yummy-Yums](https://github.com/Yummy-Yums) made their first contribution
        in [#8003](https://github.com/pydantic/pydantic/pull/8003)* [@Iipin](https://github.com/Iipin)
        made their first contribution in [#7972](https://github.com/pydantic/pydantic/pull/7972)*
        [@tabassco](https://github.com/tabassco) made their first contribution in
        [#8004](https://github.com/pydantic/pydantic/pull/8004)* [@Mr-Pepe](https://github.com/Mr-Pepe)
        made their first contribution in [#7979](https://github.com/pydantic/pydantic/pull/7979)*
        [@0x00cl](https://github.com/0x00cl) made their first contribution in [#8010](https://github.com/pydantic/pydantic/pull/8010)*
        [@barraponto](https://github.com/barraponto) made their first contribution
        in [#8032](https://github.com/pydantic/pydantic/pull/8032)#### `pydantic-core`*
        [@sisp](https://github.com/sisp) made their first contribution in [pydantic/pydantic-core#995](https://github.com/pydantic/pydantic-core/pull/995)*
        [@michaelhly](https://github.com/michaelhly) made their first contribution
        in [pydantic/pydantic-core#1015](https://github.com/pydantic/pydantic-core/pull/1015)##
        v2.5.0b1 (2023-11-09)Pre-release, see [the GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.5.0b1)
        for details.## v2.4.2 (2023-09-27)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.4.2)###
        What's Changed#### Fixes* Fix bug with JSON schema for sequence of discriminated
        union by [@dmontagu](https://github.com/dmontagu) in [#7647](https://github.com/pydantic/pydantic/pull/7647)*
        Fix schema references in discriminated unions by [@adriangb](https://github.com/adriangb)
        in [#7646](https://github.com/pydantic/pydantic/pull/7646)* Fix json schema
        generation for recursive models by [@adriangb](https://github.com/adriangb)
        in [#7653](https://github.com/pydantic/pydantic/pull/7653)* Fix `models_json_schema`
        for generic models by [@adriangb](https://github.com/adriangb) in [#7654](https://github.com/pydantic/pydantic/pull/7654)*
        Fix xfailed test for generic model signatures by [@adriangb](https://github.com/adriangb)
        in [#7658](https://github.com/pydantic/pydantic/pull/7658)### New Contributors*
        [@austinorr](https://github.com/austinorr) made their first contribution in
        [#7657](https://github.com/pydantic/pydantic/pull/7657)* [@peterHoburg](https://github.com/peterHoburg)
        made their first contribution in [#7670](https://github.com/pydantic/pydantic/pull/7670)##
        v2.4.1 (2023-09-26)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.4.1)###
        What's Changed#### Packaging* Update pydantic-core to 2.10.1 by [@davidhewitt](https://github.com/davidhewitt)
        in [#7633](https://github.com/pydantic/pydantic/pull/7633)#### Fixes* Serialize
        unsubstituted type vars as `Any` by [@adriangb](https://github.com/adriangb)
        in [#7606](https://github.com/pydantic/pydantic/pull/7606)* Remove schema
        building caches by [@adriangb](https://github.com/adriangb) in [#7624](https://github.com/pydantic/pydantic/pull/7624)*
        Fix an issue where JSON schema extras weren't JSON encoded by [@dmontagu](https://github.com/dmontagu)
        in [#7625](https://github.com/pydantic/pydantic/pull/7625)## v2.4.0 (2023-09-22)[GitHub
        release](https://github.com/pydantic/pydantic/releases/tag/v2.4.0)### What's
        Changed#### Packaging* Update pydantic-core to 2.10.0 by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#7542](https://github.com/pydantic/pydantic/pull/7542)#### New Features*
        Add `Base64Url` types by [@dmontagu](https://github.com/dmontagu) in [#7286](https://github.com/pydantic/pydantic/pull/7286)*
        Implement optional `number` to `str` coercion by [@lig](https://github.com/lig)
        in [#7508](https://github.com/pydantic/pydantic/pull/7508)* Allow access to
        `field_name` and `data` in all validators if there is data and a field name
        by [@samuelcolvin](https://github.com/samuelcolvin) in [#7542](https://github.com/pydantic/pydantic/pull/7542)*
        Add `BaseModel.model_validate_strings` and `TypeAdapter.validate_strings`
        by [@hramezani](https://github.com/hramezani) in [#7552](https://github.com/pydantic/pydantic/pull/7552)*
        Add Pydantic `plugins` experimental implementation by [@lig](https://github.com/lig)
        [@samuelcolvin](https://github.com/samuelcolvin) and [@Kludex](https://github.com/Kludex)
        in [#6820](https://github.com/pydantic/pydantic/pull/6820)#### Changes* Do
        not override `model_post_init` in subclass with private attrs by [@Viicos](https://github.com/Viicos)
        in [#7302](https://github.com/pydantic/pydantic/pull/7302)* Make fields with
        defaults not required in the serialization schema by default by [@dmontagu](https://github.com/dmontagu)
        in [#7275](https://github.com/pydantic/pydantic/pull/7275)* Mark `Extra` as
        deprecated by [@disrupted](https://github.com/disrupted) in [#7299](https://github.com/pydantic/pydantic/pull/7299)*
        Make `EncodedStr` a dataclass by [@Kludex](https://github.com/Kludex) in [#7396](https://github.com/pydantic/pydantic/pull/7396)*
        Move `annotated_handlers` to be public by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#7569](https://github.com/pydantic/pydantic/pull/7569)#### Performance*
        Simplify flattening and inlining of `CoreSchema` by [@adriangb](https://github.com/adriangb)
        in [#7523](https://github.com/pydantic/pydantic/pull/7523)* Remove unused
        copies in `CoreSchema` walking by [@adriangb](https://github.com/adriangb)
        in [#7528](https://github.com/pydantic/pydantic/pull/7528)* Add caches for
        collecting definitions and invalid schemas from a CoreSchema by [@adriangb](https://github.com/adriangb)
        in [#7527](https://github.com/pydantic/pydantic/pull/7527)* Eagerly resolve
        discriminated unions and cache cases where we can't by [@adriangb](https://github.com/adriangb)
        in [#7529](https://github.com/pydantic/pydantic/pull/7529)* Replace `dict.get`
        and `dict.setdefault` with more verbose versions in `CoreSchema` building
        hot paths by [@adriangb](https://github.com/adriangb) in [#7536](https://github.com/pydantic/pydantic/pull/7536)*
        Cache invalid `CoreSchema` discovery by [@adriangb](https://github.com/adriangb)
        in [#7535](https://github.com/pydantic/pydantic/pull/7535)* Allow disabling
        `CoreSchema` validation for faster startup times by [@adriangb](https://github.com/adriangb)
        in [#7565](https://github.com/pydantic/pydantic/pull/7565)#### Fixes* Fix
        config detection for `TypedDict` from grandparent classes by [@dmontagu](https://github.com/dmontagu)
        in [#7272](https://github.com/pydantic/pydantic/pull/7272)* Fix hash function
        generation for frozen models with unusual MRO by [@dmontagu](https://github.com/dmontagu)
        in [#7274](https://github.com/pydantic/pydantic/pull/7274)* Make `strict`
        config overridable in field for Path by [@hramezani](https://github.com/hramezani)
        in [#7281](https://github.com/pydantic/pydantic/pull/7281)* Use `ser_json_<timedelta|bytes>`
        on default in `GenerateJsonSchema` by [@Kludex](https://github.com/Kludex)
        in [#7269](https://github.com/pydantic/pydantic/pull/7269)* Adding a check
        that alias is validated as an identifier for Python by [@andree0](https://github.com/andree0)
        in [#7319](https://github.com/pydantic/pydantic/pull/7319)* Raise an error
        when computed field overrides field by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7346](https://github.com/pydantic/pydantic/pull/7346)* Fix applying `SkipValidation`
        to referenced schemas by [@adriangb](https://github.com/adriangb) in [#7381](https://github.com/pydantic/pydantic/pull/7381)*
        Enforce behavior of private attributes having double leading underscore by
        [@lig](https://github.com/lig) in [#7265](https://github.com/pydantic/pydantic/pull/7265)*
        Standardize `__get_pydantic_core_schema__` signature by [@hramezani](https://github.com/hramezani)
        in [#7415](https://github.com/pydantic/pydantic/pull/7415)* Fix generic dataclass
        fields mutation bug (when using `TypeAdapter`) by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7435](https://github.com/pydantic/pydantic/pull/7435)* Fix `TypeError`
        on `model_validator` in `wrap` mode by [@pmmmwh](https://github.com/pmmmwh)
        in [#7496](https://github.com/pydantic/pydantic/pull/7496)* Improve enum error
        message by [@hramezani](https://github.com/hramezani) in [#7506](https://github.com/pydantic/pydantic/pull/7506)*
        Make `repr` work for instances that failed initialization when handling ValidationErrors
        by [@dmontagu](https://github.com/dmontagu) in [#7439](https://github.com/pydantic/pydantic/pull/7439)*
        Fixed a regular expression denial of service issue by limiting whitespaces
        by [@prodigysml](https://github.com/prodigysml) in [#7360](https://github.com/pydantic/pydantic/pull/7360)*
        Fix handling of `UUID` values having `UUID.version=None` by [@lig](https://github.com/lig)
        in [#7566](https://github.com/pydantic/pydantic/pull/7566)* Fix `__iter__`
        returning private `cached_property` info by [@sydney-runkle](https://github.com/sydney-runkle)
        in [#7570](https://github.com/pydantic/pydantic/pull/7570)* Improvements to
        version info message by [@samuelcolvin](https://github.com/samuelcolvin) in
        [#7594](https://github.com/pydantic/pydantic/pull/7594)### New Contributors*
        [@15498th](https://github.com/15498th) made their first contribution in [#7238](https://github.com/pydantic/pydantic/pull/7238)*
        [@GabrielCappelli](https://github.com/GabrielCappelli) made their first contribution
        in [#7213](https://github.com/pydantic/pydantic/pull/7213)* [@tobni](https://github.com/tobni)
        made their first contribution in [#7184](https://github.com/pydantic/pydantic/pull/7184)*
        [@redruin1](https://github.com/redruin1) made their first contribution in
        [#7282](https://github.com/pydantic/pydantic/pull/7282)* [@FacerAin](https://github.com/FacerAin)
        made their first contribution in [#7288](https://github.com/pydantic/pydantic/pull/7288)*
        [@acdha](https://github.com/acdha) made their first contribution in [#7297](https://github.com/pydantic/pydantic/pull/7297)*
        [@andree0](https://github.com/andree0) made their first contribution in [#7319](https://github.com/pydantic/pydantic/pull/7319)*
        [@gordonhart](https://github.com/gordonhart) made their first contribution
        in [#7375](https://github.com/pydantic/pydantic/pull/7375)* [@pmmmwh](https://github.com/pmmmwh)
        made their first contribution in [#7496](https://github.com/pydantic/pydantic/pull/7496)*
        [@disrupted](https://github.com/disrupted) made their first contribution in
        [#7299](https://github.com/pydantic/pydantic/pull/7299)* [@prodigysml](https://github.com/prodigysml)
        made their first contribution in [#7360](https://github.com/pydantic/pydantic/pull/7360)##
        v2.3.0 (2023-08-23)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.3.0)*
        \U0001F525 Remove orphaned changes file from repo by [@lig](https://github.com/lig)
        in [#7168](https://github.com/pydantic/pydantic/pull/7168)* Add copy button
        on documentation by [@Kludex](https://github.com/Kludex) in [#7190](https://github.com/pydantic/pydantic/pull/7190)*
        Fix docs on JSON type by [@Kludex](https://github.com/Kludex) in [#7189](https://github.com/pydantic/pydantic/pull/7189)*
        Update mypy 1.5.0 to 1.5.1 in CI by [@hramezani](https://github.com/hramezani)
        in [#7191](https://github.com/pydantic/pydantic/pull/7191)* fix download links
        badge by [@samuelcolvin](https://github.com/samuelcolvin) in [#7200](https://github.com/pydantic/pydantic/pull/7200)*
        add 2.2.1 to changelog by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#7212](https://github.com/pydantic/pydantic/pull/7212)* Make ModelWrapValidator
        protocols generic by [@dmontagu](https://github.com/dmontagu) in [#7154](https://github.com/pydantic/pydantic/pull/7154)*
        Correct `Field(..., exclude: bool)` docs by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#7214](https://github.com/pydantic/pydantic/pull/7214)* Make shadowing
        attributes a warning instead of an error by [@adriangb](https://github.com/adriangb)
        in [#7193](https://github.com/pydantic/pydantic/pull/7193)* Document `Base64Str`
        and `Base64Bytes` by [@Kludex](https://github.com/Kludex) in [#7192](https://github.com/pydantic/pydantic/pull/7192)*
        Fix `config.defer_build` for serialization first cases by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#7024](https://github.com/pydantic/pydantic/pull/7024)* clean Model docstrings
        in JSON Schema by [@samuelcolvin](https://github.com/samuelcolvin) in [#7210](https://github.com/pydantic/pydantic/pull/7210)*
        fix [#7228](https://github.com/pydantic/pydantic/pull/7228) (typo): docs in
        `validators.md` to correct `validate_default` kwarg by [@lmmx](https://github.com/lmmx)
        in [#7229](https://github.com/pydantic/pydantic/pull/7229)* \u2705 Implement
        `tzinfo.fromutc` method for `TzInfo` in `pydantic-core` by [@lig](https://github.com/lig)
        in [#7019](https://github.com/pydantic/pydantic/pull/7019)* Support `__get_validators__`
        by [@hramezani](https://github.com/hramezani) in [#7197](https://github.com/pydantic/pydantic/pull/7197)##
        v2.2.1 (2023-08-18)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.2.1)*
        Make `xfail`ing test for root model extra stop `xfail`ing by [@dmontagu](https://github.com/dmontagu)
        in [#6937](https://github.com/pydantic/pydantic/pull/6937)* Optimize recursion
        detection by stopping on the second visit for the same object by [@mciucu](https://github.com/mciucu)
        in [#7160](https://github.com/pydantic/pydantic/pull/7160)* fix link in docs
        by [@tlambert03](https://github.com/tlambert03) in [#7166](https://github.com/pydantic/pydantic/pull/7166)*
        Replace MiMalloc w/ default allocator by [@adriangb](https://github.com/adriangb)
        in [pydantic/pydantic-core#900](https://github.com/pydantic/pydantic-core/pull/900)*
        Bump pydantic-core to 2.6.1 and prepare 2.2.1 release by [@adriangb](https://github.com/adriangb)
        in [#7176](https://github.com/pydantic/pydantic/pull/7176)## v2.2.0 (2023-08-17)[GitHub
        release](https://github.com/pydantic/pydantic/releases/tag/v2.2.0)* Split
        \"pipx install\" setup command into two commands on the documentation site
        by [@nomadmtb](https://github.com/nomadmtb) in [#6869](https://github.com/pydantic/pydantic/pull/6869)*
        Deprecate `Field.include` by [@hramezani](https://github.com/hramezani) in
        [#6852](https://github.com/pydantic/pydantic/pull/6852)* Fix typo in default
        factory error msg by [@hramezani](https://github.com/hramezani) in [#6880](https://github.com/pydantic/pydantic/pull/6880)*
        Simplify handling of typing.Annotated in GenerateSchema by [@dmontagu](https://github.com/dmontagu)
        in [#6887](https://github.com/pydantic/pydantic/pull/6887)* Re-enable fastapi
        tests in CI by [@dmontagu](https://github.com/dmontagu) in [#6883](https://github.com/pydantic/pydantic/pull/6883)*
        Make it harder to hit collisions with json schema defrefs by [@dmontagu](https://github.com/dmontagu)
        in [#6566](https://github.com/pydantic/pydantic/pull/6566)* Cleaner error
        for invalid input to `Path` fields by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#6903](https://github.com/pydantic/pydantic/pull/6903)* :memo: support
        Coordinate Type by [@yezz123](https://github.com/yezz123) in [#6906](https://github.com/pydantic/pydantic/pull/6906)*
        Fix `ForwardRef` wrapper for py 3.10.0 (shim until bpo-45166) by [@randomir](https://github.com/randomir)
        in [#6919](https://github.com/pydantic/pydantic/pull/6919)* Fix misbehavior
        related to copying of RootModel by [@dmontagu](https://github.com/dmontagu)
        in [#6918](https://github.com/pydantic/pydantic/pull/6918)* Fix issue with
        recursion error caused by ParamSpec by [@dmontagu](https://github.com/dmontagu)
        in [#6923](https://github.com/pydantic/pydantic/pull/6923)* Add section about
        Constrained classes to the Migration Guide by [@Kludex](https://github.com/Kludex)
        in [#6924](https://github.com/pydantic/pydantic/pull/6924)* Use `main` branch
        for badge links by [@Viicos](https://github.com/Viicos) in [#6925](https://github.com/pydantic/pydantic/pull/6925)*
        Add test for v1/v2 Annotated discrepancy by [@carlbordum](https://github.com/carlbordum)
        in [#6926](https://github.com/pydantic/pydantic/pull/6926)* Make the v1 mypy
        plugin work with both v1 and v2 by [@dmontagu](https://github.com/dmontagu)
        in [#6921](https://github.com/pydantic/pydantic/pull/6921)* Fix issue where
        generic models couldn't be parametrized with BaseModel by [@dmontagu](https://github.com/dmontagu)
        in [#6933](https://github.com/pydantic/pydantic/pull/6933)* Remove xfail for
        discriminated union with alias by [@dmontagu](https://github.com/dmontagu)
        in [#6938](https://github.com/pydantic/pydantic/pull/6938)* add field_serializer
        to computed_field by [@andresliszt](https://github.com/andresliszt) in [#6965](https://github.com/pydantic/pydantic/pull/6965)*
        Use union_schema with Type[Union[...]] by [@JeanArhancet](https://github.com/JeanArhancet)
        in [#6952](https://github.com/pydantic/pydantic/pull/6952)* Fix inherited
        typeddict attributes / config by [@adriangb](https://github.com/adriangb)
        in [#6981](https://github.com/pydantic/pydantic/pull/6981)* fix dataclass
        annotated before validator called twice by [@davidhewitt](https://github.com/davidhewitt)
        in [#6998](https://github.com/pydantic/pydantic/pull/6998)* Update test-fastapi
        deselected tests by [@hramezani](https://github.com/hramezani) in [#7014](https://github.com/pydantic/pydantic/pull/7014)*
        Fix validator doc format by [@hramezani](https://github.com/hramezani) in
        [#7015](https://github.com/pydantic/pydantic/pull/7015)* Fix typo in docstring
        of model_json_schema by [@AdamVinch-Federated](https://github.com/AdamVinch-Federated)
        in [#7032](https://github.com/pydantic/pydantic/pull/7032)* remove unused
        \"type ignores\" with pyright by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#7026](https://github.com/pydantic/pydantic/pull/7026)* Add benchmark
        representing FastAPI startup time by [@adriangb](https://github.com/adriangb)
        in [#7030](https://github.com/pydantic/pydantic/pull/7030)* Fix json_encoders
        for Enum subclasses by [@adriangb](https://github.com/adriangb) in [#7029](https://github.com/pydantic/pydantic/pull/7029)*
        Update docstring of `ser_json_bytes` regarding base64 encoding by [@Viicos](https://github.com/Viicos)
        in [#7052](https://github.com/pydantic/pydantic/pull/7052)* Allow `@validate_call`
        to work on async methods by [@adriangb](https://github.com/adriangb) in [#7046](https://github.com/pydantic/pydantic/pull/7046)*
        Fix: mypy error with `Settings` and `SettingsConfigDict` by [@JeanArhancet](https://github.com/JeanArhancet)
        in [#7002](https://github.com/pydantic/pydantic/pull/7002)* Fix some typos
        (repeated words and it's/its) by [@eumiro](https://github.com/eumiro) in [#7063](https://github.com/pydantic/pydantic/pull/7063)*
        Fix the typo in docstring by [@harunyasar](https://github.com/harunyasar)
        in [#7062](https://github.com/pydantic/pydantic/pull/7062)* Docs: Fix broken
        URL in the pydantic-settings package recommendation by [@swetjen](https://github.com/swetjen)
        in [#6995](https://github.com/pydantic/pydantic/pull/6995)* Handle constraints
        being applied to schemas that don't accept it by [@adriangb](https://github.com/adriangb)
        in [#6951](https://github.com/pydantic/pydantic/pull/6951)* Replace almost_equal_floats
        with math.isclose by [@eumiro](https://github.com/eumiro) in [#7082](https://github.com/pydantic/pydantic/pull/7082)*
        bump pydantic-core to 2.5.0 by [@davidhewitt](https://github.com/davidhewitt)
        in [#7077](https://github.com/pydantic/pydantic/pull/7077)* Add `short_version`
        and use it in links by [@hramezani](https://github.com/hramezani) in [#7115](https://github.com/pydantic/pydantic/pull/7115)*
        \U0001F4DD Add usage link to `RootModel` by [@Kludex](https://github.com/Kludex)
        in [#7113](https://github.com/pydantic/pydantic/pull/7113)* Revert \"Fix default
        port for mongosrv DSNs (#6827)\" by [@Kludex](https://github.com/Kludex) in
        [#7116](https://github.com/pydantic/pydantic/pull/7116)* Clarify validate_default
        and _Unset handling in usage docs and migration guide by [@benbenbang](https://github.com/benbenbang)
        in [#6950](https://github.com/pydantic/pydantic/pull/6950)* Tweak documentation
        of `Field.exclude` by [@Viicos](https://github.com/Viicos) in [#7086](https://github.com/pydantic/pydantic/pull/7086)*
        Do not require `validate_assignment` to use `Field.frozen` by [@Viicos](https://github.com/Viicos)
        in [#7103](https://github.com/pydantic/pydantic/pull/7103)* tweaks to `_core_utils`
        by [@samuelcolvin](https://github.com/samuelcolvin) in [#7040](https://github.com/pydantic/pydantic/pull/7040)*
        Make DefaultDict working with set by [@hramezani](https://github.com/hramezani)
        in [#7126](https://github.com/pydantic/pydantic/pull/7126)* Don't always require
        typing.Generic as a base for partially parametrized models by [@dmontagu](https://github.com/dmontagu)
        in [#7119](https://github.com/pydantic/pydantic/pull/7119)* Fix issue with
        JSON schema incorrectly using parent class core schema by [@dmontagu](https://github.com/dmontagu)
        in [#7020](https://github.com/pydantic/pydantic/pull/7020)* Fix xfailed test
        related to TypedDict and alias_generator by [@dmontagu](https://github.com/dmontagu)
        in [#6940](https://github.com/pydantic/pydantic/pull/6940)* Improve error
        message for NameEmail by [@dmontagu](https://github.com/dmontagu) in [#6939](https://github.com/pydantic/pydantic/pull/6939)*
        Fix generic computed fields by [@dmontagu](https://github.com/dmontagu) in
        [#6988](https://github.com/pydantic/pydantic/pull/6988)* Reflect namedtuple
        default values during validation by [@dmontagu](https://github.com/dmontagu)
        in [#7144](https://github.com/pydantic/pydantic/pull/7144)* Update dependencies,
        fix pydantic-core usage, fix CI issues by [@dmontagu](https://github.com/dmontagu)
        in [#7150](https://github.com/pydantic/pydantic/pull/7150)* Add mypy 1.5.0
        by [@hramezani](https://github.com/hramezani) in [#7118](https://github.com/pydantic/pydantic/pull/7118)*
        Handle non-json native enum values by [@adriangb](https://github.com/adriangb)
        in [#7056](https://github.com/pydantic/pydantic/pull/7056)* document `round_trip`
        in Json type documentation  by [@jc-louis](https://github.com/jc-louis) in
        [#7137](https://github.com/pydantic/pydantic/pull/7137)* Relax signature checks
        to better support builtins and C extension functions as validators by [@adriangb](https://github.com/adriangb)
        in [#7101](https://github.com/pydantic/pydantic/pull/7101)* add union_mode='left_to_right'
        by [@davidhewitt](https://github.com/davidhewitt) in [#7151](https://github.com/pydantic/pydantic/pull/7151)*
        Include an error message hint for inherited ordering by [@yvalencia91](https://github.com/yvalencia91)
        in [#7124](https://github.com/pydantic/pydantic/pull/7124)* Fix one docs link
        and resolve some warnings for two others by [@dmontagu](https://github.com/dmontagu)
        in [#7153](https://github.com/pydantic/pydantic/pull/7153)* Include Field
        extra keys name in warning by [@hramezani](https://github.com/hramezani) in
        [#7136](https://github.com/pydantic/pydantic/pull/7136)## v2.1.1 (2023-07-25)[GitHub
        release](https://github.com/pydantic/pydantic/releases/tag/v2.1.1)* Skip FieldInfo
        merging when unnecessary by [@dmontagu](https://github.com/dmontagu) in [#6862](https://github.com/pydantic/pydantic/pull/6862)##
        v2.1.0 (2023-07-25)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.1.0)*
        Add `StringConstraints` for use as Annotated metadata by [@adriangb](https://github.com/adriangb)
        in [#6605](https://github.com/pydantic/pydantic/pull/6605)* Try to fix intermittently
        failing CI by [@adriangb](https://github.com/adriangb) in [#6683](https://github.com/pydantic/pydantic/pull/6683)*
        Remove redundant example of optional vs default. by [@ehiggs-deliverect](https://github.com/ehiggs-deliverect)
        in [#6676](https://github.com/pydantic/pydantic/pull/6676)* Docs update by
        [@samuelcolvin](https://github.com/samuelcolvin) in [#6692](https://github.com/pydantic/pydantic/pull/6692)*
        Remove the Validate always section in validator docs by [@adriangb](https://github.com/adriangb)
        in [#6679](https://github.com/pydantic/pydantic/pull/6679)* Fix recursion
        error in json schema generation by [@adriangb](https://github.com/adriangb)
        in [#6720](https://github.com/pydantic/pydantic/pull/6720)* Fix incorrect
        subclass check for secretstr by [@AlexVndnblcke](https://github.com/AlexVndnblcke)
        in [#6730](https://github.com/pydantic/pydantic/pull/6730)* update pdm / pdm
        lockfile to 2.8.0 by [@davidhewitt](https://github.com/davidhewitt) in [#6714](https://github.com/pydantic/pydantic/pull/6714)*
        unpin pdm on more CI jobs by [@davidhewitt](https://github.com/davidhewitt)
        in [#6755](https://github.com/pydantic/pydantic/pull/6755)* improve source
        locations for auxiliary packages in docs by [@davidhewitt](https://github.com/davidhewitt)
        in [#6749](https://github.com/pydantic/pydantic/pull/6749)* Assume builtins
        don't accept an info argument by [@adriangb](https://github.com/adriangb)
        in [#6754](https://github.com/pydantic/pydantic/pull/6754)* Fix bug where
        calling `help(BaseModelSubclass)` raises errors by [@hramezani](https://github.com/hramezani)
        in [#6758](https://github.com/pydantic/pydantic/pull/6758)* Fix mypy plugin
        handling of `@model_validator(mode=\"after\")` by [@ljodal](https://github.com/ljodal)
        in [#6753](https://github.com/pydantic/pydantic/pull/6753)* update pydantic-core
        to 2.3.1 by [@davidhewitt](https://github.com/davidhewitt) in [#6756](https://github.com/pydantic/pydantic/pull/6756)*
        Mypy plugin for settings by [@hramezani](https://github.com/hramezani) in
        [#6760](https://github.com/pydantic/pydantic/pull/6760)* Use `contentSchema`
        keyword for JSON schema by [@dmontagu](https://github.com/dmontagu) in [#6715](https://github.com/pydantic/pydantic/pull/6715)*
        fast-path checking finite decimals by [@davidhewitt](https://github.com/davidhewitt)
        in [#6769](https://github.com/pydantic/pydantic/pull/6769)* Docs update by
        [@samuelcolvin](https://github.com/samuelcolvin) in [#6771](https://github.com/pydantic/pydantic/pull/6771)*
        Improve json schema doc by [@hramezani](https://github.com/hramezani) in [#6772](https://github.com/pydantic/pydantic/pull/6772)*
        Update validator docs by [@adriangb](https://github.com/adriangb) in [#6695](https://github.com/pydantic/pydantic/pull/6695)*
        Fix typehint for wrap validator by [@dmontagu](https://github.com/dmontagu)
        in [#6788](https://github.com/pydantic/pydantic/pull/6788)* \U0001F41B Fix
        validation warning for unions of Literal and other type by [@lig](https://github.com/lig)
        in [#6628](https://github.com/pydantic/pydantic/pull/6628)* Update documentation
        for generics support in V2 by [@tpdorsey](https://github.com/tpdorsey) in
        [#6685](https://github.com/pydantic/pydantic/pull/6685)* add pydantic-core
        build info to `version_info()` by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#6785](https://github.com/pydantic/pydantic/pull/6785)* Fix pydantic dataclasses
        that use slots with default values by [@dmontagu](https://github.com/dmontagu)
        in [#6796](https://github.com/pydantic/pydantic/pull/6796)* Fix inheritance
        of hash function for frozen models by [@dmontagu](https://github.com/dmontagu)
        in [#6789](https://github.com/pydantic/pydantic/pull/6789)* \u2728 Add `SkipJsonSchema`
        annotation by [@Kludex](https://github.com/Kludex) in [#6653](https://github.com/pydantic/pydantic/pull/6653)*
        Error if an invalid field name is used with Field by [@dmontagu](https://github.com/dmontagu)
        in [#6797](https://github.com/pydantic/pydantic/pull/6797)* Add `GenericModel`
        to `MOVED_IN_V2` by [@adriangb](https://github.com/adriangb) in [#6776](https://github.com/pydantic/pydantic/pull/6776)*
        Remove unused code from `docs/usage/types/custom.md` by [@hramezani](https://github.com/hramezani)
        in [#6803](https://github.com/pydantic/pydantic/pull/6803)* Fix `float` ->
        `Decimal` coercion precision loss by [@adriangb](https://github.com/adriangb)
        in [#6810](https://github.com/pydantic/pydantic/pull/6810)* remove email validation
        from the north star benchmark by [@davidhewitt](https://github.com/davidhewitt)
        in [#6816](https://github.com/pydantic/pydantic/pull/6816)* Fix link to mypy
        by [@progsmile](https://github.com/progsmile) in [#6824](https://github.com/pydantic/pydantic/pull/6824)*
        Improve initialization hooks example by [@hramezani](https://github.com/hramezani)
        in [#6822](https://github.com/pydantic/pydantic/pull/6822)* Fix default port
        for mongosrv DSNs by [@dmontagu](https://github.com/dmontagu) in [#6827](https://github.com/pydantic/pydantic/pull/6827)*
        Improve API documentation, in particular more links between usage and API
        docs by [@samuelcolvin](https://github.com/samuelcolvin) in [#6780](https://github.com/pydantic/pydantic/pull/6780)*
        update pydantic-core to 2.4.0 by [@davidhewitt](https://github.com/davidhewitt)
        in [#6831](https://github.com/pydantic/pydantic/pull/6831)* Fix `annotated_types.MaxLen`
        validator for custom sequence types by [@ImogenBits](https://github.com/ImogenBits)
        in [#6809](https://github.com/pydantic/pydantic/pull/6809)* Update V1 by [@hramezani](https://github.com/hramezani)
        in [#6833](https://github.com/pydantic/pydantic/pull/6833)* Make it so callable
        JSON schema extra works by [@dmontagu](https://github.com/dmontagu) in [#6798](https://github.com/pydantic/pydantic/pull/6798)*
        Fix serialization issue with `InstanceOf` by [@dmontagu](https://github.com/dmontagu)
        in [#6829](https://github.com/pydantic/pydantic/pull/6829)* Add back support
        for `json_encoders` by [@adriangb](https://github.com/adriangb) in [#6811](https://github.com/pydantic/pydantic/pull/6811)*
        Update field annotations when building the schema by [@dmontagu](https://github.com/dmontagu)
        in [#6838](https://github.com/pydantic/pydantic/pull/6838)* Use `WeakValueDictionary`
        to fix generic memory leak by [@dmontagu](https://github.com/dmontagu) in
        [#6681](https://github.com/pydantic/pydantic/pull/6681)* Add `config.defer_build`
        to optionally make model building lazy by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#6823](https://github.com/pydantic/pydantic/pull/6823)* delegate `UUID`
        serialization to pydantic-core by [@davidhewitt](https://github.com/davidhewitt)
        in [#6850](https://github.com/pydantic/pydantic/pull/6850)* Update `json_encoders`
        docs by [@adriangb](https://github.com/adriangb) in [#6848](https://github.com/pydantic/pydantic/pull/6848)*
        Fix error message for `staticmethod`/`classmethod` order with validate_call
        by [@dmontagu](https://github.com/dmontagu) in [#6686](https://github.com/pydantic/pydantic/pull/6686)*
        Improve documentation for `Config` by [@samuelcolvin](https://github.com/samuelcolvin)
        in [#6847](https://github.com/pydantic/pydantic/pull/6847)* Update serialization
        doc to mention `Field.exclude` takes priority over call-time `include/exclude`
        by [@hramezani](https://github.com/hramezani) in [#6851](https://github.com/pydantic/pydantic/pull/6851)*
        Allow customizing core schema generation by making `GenerateSchema` public
        by [@adriangb](https://github.com/adriangb) in [#6737](https://github.com/pydantic/pydantic/pull/6737)##
        v2.0.3 (2023-07-05)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.0.3)*
        Mention PyObject (v1) moving to ImportString (v2) in migration doc by [@slafs](https://github.com/slafs)
        in [#6456](https://github.com/pydantic/pydantic/pull/6456)* Fix release-tweet
        CI by [@Kludex](https://github.com/Kludex) in [#6461](https://github.com/pydantic/pydantic/pull/6461)*
        Revise the section on required / optional / nullable fields. by [@ybressler](https://github.com/ybressler)
        in [#6468](https://github.com/pydantic/pydantic/pull/6468)* Warn if a type
        hint is not in fact a type by [@adriangb](https://github.com/adriangb) in
        [#6479](https://github.com/pydantic/pydantic/pull/6479)* Replace TransformSchema
        with GetPydanticSchema by [@dmontagu](https://github.com/dmontagu) in [#6484](https://github.com/pydantic/pydantic/pull/6484)*
        Fix the un-hashability of various annotation types, for use in caching generic
        containers by [@dmontagu](https://github.com/dmontagu) in [#6480](https://github.com/pydantic/pydantic/pull/6480)*
        PYD-164: Rework custom types docs by [@adriangb](https://github.com/adriangb)
        in [#6490](https://github.com/pydantic/pydantic/pull/6490)* Fix ci by [@adriangb](https://github.com/adriangb)
        in [#6507](https://github.com/pydantic/pydantic/pull/6507)* Fix forward ref
        in generic by [@adriangb](https://github.com/adriangb) in [#6511](https://github.com/pydantic/pydantic/pull/6511)*
        Fix generation of serialization JSON schemas for core_schema.ChainSchema by
        [@dmontagu](https://github.com/dmontagu) in [#6515](https://github.com/pydantic/pydantic/pull/6515)*
        Document the change in `Field.alias` behavior in Pydantic V2 by [@hramezani](https://github.com/hramezani)
        in [#6508](https://github.com/pydantic/pydantic/pull/6508)* Give better error
        message attempting to compute the json schema of a model with undefined fields
        by [@dmontagu](https://github.com/dmontagu) in [#6519](https://github.com/pydantic/pydantic/pull/6519)*
        Document `alias_priority` by [@tpdorsey](https://github.com/tpdorsey) in [#6520](https://github.com/pydantic/pydantic/pull/6520)*
        Add redirect for types documentation by [@tpdorsey](https://github.com/tpdorsey)
        in [#6513](https://github.com/pydantic/pydantic/pull/6513)* Allow updating
        docs without release by [@samuelcolvin](https://github.com/samuelcolvin) in
        [#6551](https://github.com/pydantic/pydantic/pull/6551)* Ensure docs tests
        always run in the right folder by [@dmontagu](https://github.com/dmontagu)
        in [#6487](https://github.com/pydantic/pydantic/pull/6487)* Defer evaluation
        of return type hints for serializer functions by [@dmontagu](https://github.com/dmontagu)
        in [#6516](https://github.com/pydantic/pydantic/pull/6516)* Disable E501 from
        Ruff and rely on just Black by [@adriangb](https://github.com/adriangb) in
        [#6552](https://github.com/pydantic/pydantic/pull/6552)* Update JSON Schema
        documentation for V2 by [@tpdorsey](https://github.com/tpdorsey) in [#6492](https://github.com/pydantic/pydantic/pull/6492)*
        Add documentation of cyclic reference handling by [@dmontagu](https://github.com/dmontagu)
        in [#6493](https://github.com/pydantic/pydantic/pull/6493)* Remove the need
        for change files by [@samuelcolvin](https://github.com/samuelcolvin) in [#6556](https://github.com/pydantic/pydantic/pull/6556)*
        add \"north star\" benchmark by [@davidhewitt](https://github.com/davidhewitt)
        in [#6547](https://github.com/pydantic/pydantic/pull/6547)* Update Dataclasses
        docs by [@tpdorsey](https://github.com/tpdorsey) in [#6470](https://github.com/pydantic/pydantic/pull/6470)*
        \u267B\uFE0F Use different error message on v1 redirects by [@Kludex](https://github.com/Kludex)
        in [#6595](https://github.com/pydantic/pydantic/pull/6595)* \u2B06 Upgrade
        `pydantic-core` to v2.2.0 by [@lig](https://github.com/lig) in [#6589](https://github.com/pydantic/pydantic/pull/6589)*
        Fix serialization for IPvAny by [@dmontagu](https://github.com/dmontagu) in
        [#6572](https://github.com/pydantic/pydantic/pull/6572)* Improve CI by using
        PDM instead of pip to install typing-extensions by [@adriangb](https://github.com/adriangb)
        in [#6602](https://github.com/pydantic/pydantic/pull/6602)* Add `enum` error
        type docs  by [@lig](https://github.com/lig) in [#6603](https://github.com/pydantic/pydantic/pull/6603)*
        \U0001F41B Fix `max_length` for unicode strings by [@lig](https://github.com/lig)
        in [#6559](https://github.com/pydantic/pydantic/pull/6559)* Add documentation
        for accessing features via `pydantic.v1` by [@tpdorsey](https://github.com/tpdorsey)
        in [#6604](https://github.com/pydantic/pydantic/pull/6604)* Include extra
        when iterating over a model by [@adriangb](https://github.com/adriangb) in
        [#6562](https://github.com/pydantic/pydantic/pull/6562)* Fix typing of model_validator
        by [@adriangb](https://github.com/adriangb) in [#6514](https://github.com/pydantic/pydantic/pull/6514)*
        Touch up Decimal validator by [@adriangb](https://github.com/adriangb) in
        [#6327](https://github.com/pydantic/pydantic/pull/6327)* Fix various docstrings
        using fixed pytest-examples by [@dmontagu](https://github.com/dmontagu) in
        [#6607](https://github.com/pydantic/pydantic/pull/6607)* Handle function validators
        in a discriminated union by [@dmontagu](https://github.com/dmontagu) in [#6570](https://github.com/pydantic/pydantic/pull/6570)*
        Review json_schema.md by [@tpdorsey](https://github.com/tpdorsey) in [#6608](https://github.com/pydantic/pydantic/pull/6608)*
        Make validate_call work on basemodel methods by [@dmontagu](https://github.com/dmontagu)
        in [#6569](https://github.com/pydantic/pydantic/pull/6569)* add test for big
        int json serde by [@davidhewitt](https://github.com/davidhewitt) in [#6614](https://github.com/pydantic/pydantic/pull/6614)*
        Fix pydantic dataclass problem with dataclasses.field default_factory by [@hramezani](https://github.com/hramezani)
        in [#6616](https://github.com/pydantic/pydantic/pull/6616)* Fixed mypy type
        inference for TypeAdapter by [@zakstucke](https://github.com/zakstucke) in
        [#6617](https://github.com/pydantic/pydantic/pull/6617)* Make it work to use
        None as a generic parameter by [@dmontagu](https://github.com/dmontagu) in
        [#6609](https://github.com/pydantic/pydantic/pull/6609)* Make it work to use
        `$ref` as an alias by [@dmontagu](https://github.com/dmontagu) in [#6568](https://github.com/pydantic/pydantic/pull/6568)*
        add note to migration guide about changes to `AnyUrl` etc by [@davidhewitt](https://github.com/davidhewitt)
        in [#6618](https://github.com/pydantic/pydantic/pull/6618)* \U0001F41B Support
        defining `json_schema_extra` on `RootModel` using `Field` by [@lig](https://github.com/lig)
        in [#6622](https://github.com/pydantic/pydantic/pull/6622)* Update pre-commit
        to prevent commits to main branch on accident by [@dmontagu](https://github.com/dmontagu)
        in [#6636](https://github.com/pydantic/pydantic/pull/6636)* Fix PDM CI for
        python 3.7 on MacOS/windows by [@dmontagu](https://github.com/dmontagu) in
        [#6627](https://github.com/pydantic/pydantic/pull/6627)* Produce more accurate
        signatures for pydantic dataclasses by [@dmontagu](https://github.com/dmontagu)
        in [#6633](https://github.com/pydantic/pydantic/pull/6633)* Updates to Url
        types for Pydantic V2 by [@tpdorsey](https://github.com/tpdorsey) in [#6638](https://github.com/pydantic/pydantic/pull/6638)*
        Fix list markdown in `transform` docstring by [@StefanBRas](https://github.com/StefanBRas)
        in [#6649](https://github.com/pydantic/pydantic/pull/6649)* simplify slots_dataclass
        construction to appease mypy by [@davidhewitt](https://github.com/davidhewitt)
        in [#6639](https://github.com/pydantic/pydantic/pull/6639)* Update TypedDict
        schema generation docstring by [@adriangb](https://github.com/adriangb) in
        [#6651](https://github.com/pydantic/pydantic/pull/6651)* Detect and lint-error
        for prints by [@dmontagu](https://github.com/dmontagu) in [#6655](https://github.com/pydantic/pydantic/pull/6655)*
        Add xfailing test for pydantic-core PR 766 by [@dmontagu](https://github.com/dmontagu)
        in [#6641](https://github.com/pydantic/pydantic/pull/6641)* Ignore unrecognized
        fields from dataclasses metadata by [@dmontagu](https://github.com/dmontagu)
        in [#6634](https://github.com/pydantic/pydantic/pull/6634)* Make non-existent
        class getattr a mypy error by [@dmontagu](https://github.com/dmontagu) in
        [#6658](https://github.com/pydantic/pydantic/pull/6658)* Update pydantic-core
        to 2.3.0 by [@hramezani](https://github.com/hramezani) in [#6648](https://github.com/pydantic/pydantic/pull/6648)*
        Use OrderedDict from typing_extensions by [@dmontagu](https://github.com/dmontagu)
        in [#6664](https://github.com/pydantic/pydantic/pull/6664)* Fix typehint for
        JSON schema extra callable by [@dmontagu](https://github.com/dmontagu) in
        [#6659](https://github.com/pydantic/pydantic/pull/6659)## v2.0.2 (2023-07-05)[GitHub
        release](https://github.com/pydantic/pydantic/releases/tag/v2.0.2)* Fix bug
        where round-trip pickling/unpickling a `RootModel` would change the value
        of `__dict__`, [#6457](https://github.com/pydantic/pydantic/pull/6457) by
        [@dmontagu](https://github.com/dmontagu)* Allow single-item discriminated
        unions, [#6405](https://github.com/pydantic/pydantic/pull/6405) by [@dmontagu](https://github.com/dmontagu)*
        Fix issue with union parsing of enums, [#6440](https://github.com/pydantic/pydantic/pull/6440)
        by [@dmontagu](https://github.com/dmontagu)* Docs: Fixed `constr` documentation,
        renamed old `regex` to new `pattern`, [#6452](https://github.com/pydantic/pydantic/pull/6452)
        by [@miili](https://github.com/miili)* Change `GenerateJsonSchema.generate_definitions`
        signature, [#6436](https://github.com/pydantic/pydantic/pull/6436) by [@dmontagu](https://github.com/dmontagu)See
        the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0.2)##
        v2.0.1 (2023-07-04)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.0.1)First
        patch release of Pydantic V2* Extra fields added via `setattr` (i.e. `m.some_extra_field
        = 'extra_value'`)  are added to `.model_extra` if `model_config` `extra='allowed'`.
        Fixed [#6333](https://github.com/pydantic/pydantic/pull/6333), [#6365](https://github.com/pydantic/pydantic/pull/6365)
        by [@aaraney](https://github.com/aaraney)* Automatically unpack JSON schema
        '$ref' for custom types, [#6343](https://github.com/pydantic/pydantic/pull/6343)
        by [@adriangb](https://github.com/adriangb)* Fix tagged unions multiple processing
        in submodels, [#6340](https://github.com/pydantic/pydantic/pull/6340) by [@suharnikov](https://github.com/suharnikov)See
        the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0.1)##
        v2.0 (2023-06-30)[GitHub release](https://github.com/pydantic/pydantic/releases/tag/v2.0)Pydantic
        V2 is here! :tada:See [this post](https://docs.pydantic.dev/2.0/blog/pydantic-v2-final/)
        for more details.## v2.0b3 (2023-06-16)Third beta pre-release of Pydantic
        V2See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0b3)##
        v2.0b2 (2023-06-03)Add `from_attributes` runtime flag to `TypeAdapter.validate_python`
        and `BaseModel.model_validate`.See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0b2)##
        v2.0b1 (2023-06-01)First beta pre-release of Pydantic V2See the full changelog
        [here](https://github.com/pydantic/pydantic/releases/tag/v2.0b1)## v2.0a4
        (2023-05-05)Fourth pre-release of Pydantic V2See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0a4)##
        v2.0a3 (2023-04-20)Third pre-release of Pydantic V2See the full changelog
        [here](https://github.com/pydantic/pydantic/releases/tag/v2.0a3)## v2.0a2
        (2023-04-12)Second pre-release of Pydantic V2See the full changelog [here](https://github.com/pydantic/pydantic/releases/tag/v2.0a2)##
        v2.0a1 (2023-04-03)First pre-release of Pydantic V2!See [this post](https://docs.pydantic.dev/blog/pydantic-v2-alpha/)
        for more details.... see [here](https://docs.pydantic.dev/changelog/#v0322-2019-08-17)
        for earlier changes."
      Package: pydantic
      Source: pip
      Version: 1.10.14
      Hash: ''
      licenses:
      - MIT
      Title: pydantic
      DownloadURL: https://files.pythonhosted.org/packages/df/ab/67eda485b025e9253cce0eaede9b6158a08f62af7013a883b2c8775917b2/pydantic-1.10.14.tar.gz
  bazaar:
    register: 'no'
    prim: 24/CTX1030577
    community_link: https://pypi.org/project/pydantic
    community_name: https://pypi.org/project/pydantic
    community_url: https://pypi.org/project/pydantic
    component_comment: ''
    component_highlevel_description: Data validation using Python type hints
    component_name: pydantic
    component_platform: linux
    component_programing_language: Python
    component_version: 1.10.14
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://files.pythonhosted.org/packages/df/ab/67eda485b025e9253cce0eaede9b6158a08f62af7013a883b2c8775917b2/pydantic-1.10.14.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Newer versions exists
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1076622&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United Kingdom
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: pydantic
    target_sw: linux
    vendor: pip
    version: 1.10.14
    web_url: https://pypi.org/project/pydantic/2.7.0/
  licenses:
  - MIT
  name: pydantic
  primary:
  - this
  subcomponent: false
  type: FOSS
  versions:
  - 1.10.14
  mimer:
    linking: Static
    product_number: CTX1030577
    product_version_label: 1.10.14
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'True'
- ID: pyreadline3+3.4.1
  additional_info:
    fossa-attribution:
      Description: The `pyreadline3` package is based on the stale package `pyreadline`
        locatedat https://github.com/pyreadline/pyreadline.The original `pyreadline`
        package is a python implementation of GNU `readline`functionality.It is based
        on the `ctypes` based UNC `readline` package by Gary Bishop.It is not complete.It
        has been tested for use with Windows 10.Version 3.4+ or pyreadline3 runs on
        Python 3.5+.Features- keyboard text selection and copy/paste- Shift-arrowkeys
        for text selection- Control-c can be used for copy activate with allow_ctrl_c(True)
        in config file- Double tapping ctrl-c will raise a KeyboardInterrupt, use  ctrl_c_tap_time_interval(x)-
        where x is your preferred tap time window, default 0.3 s.- paste pastes first
        line of content on clipboard.- ipython_paste, pastes tab-separated data as
        list of lists or numpy array if  all data is numeric- paste_mulitline_code
        pastes multi line code, removing any empty lines.The latest development version
        is always available at the project gitrepository https://github.com/pyreadline3/pyreadline3
      Package: pyreadline3
      Source: pip
      Version: 3.4.1
      Hash: ''
      licenses: []
      Title: pyreadline3
      DownloadURL: https://github.com/pyreadline3/pyreadline3/archive/refs/tags/v3.4.1.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/pyreadline3/pyreadline3
    community_name: https://github.com/pyreadline3/pyreadline3
    community_url: https://github.com/pyreadline3/pyreadline3
    component_comment: ''
    component_highlevel_description: ''
    component_name: pyreadline3
    component_platform: linux
    component_programing_language: ''
    component_version: v3.4.1
    licenses: []
    src_download_link: https://github.com/pyreadline3/pyreadline3/archive/refs/tags/v3.4.1.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: pyreadline3
    target_sw: linux
    vendor: pip
    version: 3.4.1
    web_url: https://pypi.python.org/pypi/pyreadline3/
  licenses: []
  name: pyreadline3
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 3.4.1
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 3.4.1
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: python-dateutil+2.8.2
  additional_info:
    fossa-attribution:
      Description: "dateutil - powerful extensions to datetime==========================================|pypi|
        |support| |licence||gitter| |readthedocs||travis| |appveyor| |pipelines| |coverage|..
        |pypi| image:: https://img.shields.io/pypi/v/python-dateutil.svg?style=flat-square
        \   :target: https://pypi.org/project/python-dateutil/    :alt: pypi version..
        |support| image:: https://img.shields.io/pypi/pyversions/python-dateutil.svg?style=flat-square
        \   :target: https://pypi.org/project/python-dateutil/    :alt: supported
        Python version.. |travis| image:: https://img.shields.io/travis/dateutil/dateutil/master.svg?style=flat-square&label=Travis%20Build
        \   :target: https://travis-ci.org/dateutil/dateutil    :alt: travis build
        status.. |appveyor| image:: https://img.shields.io/appveyor/ci/dateutil/dateutil/master.svg?style=flat-square&logo=appveyor
        \   :target: https://ci.appveyor.com/project/dateutil/dateutil    :alt: appveyor
        build status.. |pipelines| image:: https://dev.azure.com/pythondateutilazure/dateutil/_apis/build/status/dateutil.dateutil?branchName=master
        \   :target: https://dev.azure.com/pythondateutilazure/dateutil/_build/latest?definitionId=1&branchName=master
        \   :alt: azure pipelines build status.. |coverage| image:: https://codecov.io/gh/dateutil/dateutil/branch/master/graphs/badge.svg?branch=master
        \   :target: https://codecov.io/gh/dateutil/dateutil?branch=master    :alt:
        Code coverage.. |gitter| image:: https://badges.gitter.im/dateutil/dateutil.svg
        \  :alt: Join the chat at https://gitter.im/dateutil/dateutil   :target: https://gitter.im/dateutil/dateutil..
        |licence| image:: https://img.shields.io/pypi/l/python-dateutil.svg?style=flat-square
        \   :target: https://pypi.org/project/python-dateutil/    :alt: licence..
        |readthedocs| image:: https://img.shields.io/readthedocs/dateutil/latest.svg?style=flat-square&label=Read%20the%20Docs
        \  :alt: Read the documentation at https://dateutil.readthedocs.io/en/latest/
        \  :target: https://dateutil.readthedocs.io/en/latest/The `dateutil` module
        provides powerful extensions tothe standard `datetime` module, available in
        Python.Installation============`dateutil` can be installed from PyPI using
        `pip` (note that the package name isdifferent from the importable name)::
        \   pip install python-dateutilDownload========dateutil is available on PyPIhttps://pypi.org/project/python-dateutil/The
        documentation is hosted at:https://dateutil.readthedocs.io/en/stable/Code====The
        code and issue tracker are hosted on GitHub:https://github.com/dateutil/dateutil/Features========*
        Computing of relative deltas (next month, next year,  next Monday, last week
        of month, etc);* Computing of relative deltas between two given  date and/or
        datetime objects;* Computing of dates based on very flexible recurrence rules,
        \ using a superset of the `iCalendar <https://www.ietf.org/rfc/rfc2445.txt>`_
        \ specification. Parsing of RFC strings is supported as well.* Generic parsing
        of dates in almost any string format;* Timezone (tzinfo) implementations for
        tzfile(5) format  files (/etc/localtime, /usr/share/zoneinfo, etc), TZ  environment
        string (in all known formats), iCalendar  format files, given ranges (with
        help from relative deltas),  local machine timezone, fixed offset timezone,
        UTC timezone,  and Windows registry-based time zones.* Internal up-to-date
        world timezone information based on  Olson's database.* Computing of Easter
        Sunday dates for any given year,  using Western, Orthodox or Julian algorithms;*
        A comprehensive test suite.Quick example=============Here's a snapshot, just
        to give an idea about the power of thepackage. For more examples, look at
        the documentation.Suppose you want to know how much time is left, inyears/months/days/etc,
        before the next easter happening on ayear with a Friday 13th in August, and
        you want to get today'sdate out of the \"date\" unix system command. Here
        is the code:.. code-block:: python3    >>> from dateutil.relativedelta import
        *    >>> from dateutil.easter import *    >>> from dateutil.rrule import *
        \   >>> from dateutil.parser import *    >>> from datetime import *    >>>
        now = parse(\"Sat Oct 11 17:13:46 UTC 2003\")    >>> today = now.date()    >>>
        year = rrule(YEARLY,dtstart=now,bymonth=8,bymonthday=13,byweekday=FR)[0].year
        \   >>> rdelta = relativedelta(easter(year), today)    >>> print(\"Today is:
        %s\" % today)    Today is: 2003-10-11    >>> print(\"Year with next Aug 13th
        on a Friday is: %s\" % year)    Year with next Aug 13th on a Friday is: 2004
        \   >>> print(\"How far is the Easter of that year: %s\" % rdelta)    How
        far is the Easter of that year: relativedelta(months=+6)    >>> print(\"And
        the Easter of that year is: %s\" % (today+rdelta))    And the Easter of that
        year is: 2004-04-11Being exactly 6 months ahead was **really** a coincidence
        :)Contributing============We welcome many types of contributions - bug reports,
        pull requests (code, infrastructure or documentation fixes). For more information
        about how to contribute to the project, see the ``CONTRIBUTING.md`` file in
        the repository.Author======The dateutil module was written by Gustavo Niemeyer
        <gustavo@niemeyer.net>in 2003.It is maintained by:* Gustavo Niemeyer <gustavo@niemeyer.net>
        2003-2011* Tomi Pievil\xE4inen <tomi.pievilainen@iki.fi> 2012-2014* Yaron
        de Leeuw <me@jarondl.net> 2014-2016* Paul Ganssle <paul@ganssle.io> 2015-Starting
        with version 2.4.1 and running until 2.8.2, all source and binarydistributions
        will be signed by a PGP key that has, at the very least, beensigned by the
        key which made the previous release. A table of release signingkeys can be
        found below:===========  ============================Releases     Signing
        key fingerprint===========  ============================2.4.1-2.8.2  `6B49
        ACBA DCF6 BD1C A206 67AB CD54 FCE3 D964 BEFB`_===========  ============================New
        releases *may* have signed tags, but binary and source distributionsuploaded
        to PyPI will no longer have GPG signatures attached.Contact=======Our mailing
        list is available at `dateutil@python.org <https://mail.python.org/mailman/listinfo/dateutil>`_.
        As it is hosted by the PSF, it is subject to the `PSF code ofconduct <https://www.python.org/psf/conduct/>`_.License=======All
        contributions after December 1, 2017 released under dual license - either
        `Apache 2.0 License <https://www.apache.org/licenses/LICENSE-2.0>`_ or the
        `BSD 3-Clause License <https://opensource.org/licenses/BSD-3-Clause>`_. Contributions
        before December 1, 2017 - except those those explicitly relicensed - are released
        only under the BSD 3-Clause License... _6B49 ACBA DCF6 BD1C A206 67AB CD54
        FCE3 D964 BEFB:   https://pgp.mit.edu/pks/lookup?op=vindex&search=0xCD54FCE3D964BEFB"
      Package: python-dateutil
      Source: pip
      Version: 2.8.2
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-3-Clause
      Title: python-dateutil
      DownloadURL: https://files.pythonhosted.org/packages/36/7a/87837f39d0296e723bb9b62bbb257d0355c7f6128853c78955f57342a56d/python_dateutil-2.8.2-py2.py3-none-any.whl
  bazaar:
    register: 'no'
    prim: 13/CAX1057771
    community_link: https://github.com/dateutil/dateutil
    community_name: https://github.com/dateutil/dateutil
    community_url: https://github.com/dateutil/dateutil
    component_comment: ''
    component_highlevel_description: Extensions to the standard Python datetime module.The
      dateutil module provides powerful extensions to the datetime module available
      in the Python standard library.
    component_name: Python-Dateutil
    component_platform: linux
    component_programing_language: Python
    component_version: 2.8.2
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    - FAL1159998 (Dual License)
    src_download_link: https://github.com/dateutil/dateutil/archive/2.8.2.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=980699&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: python-dateutil
    target_sw: linux
    vendor: pip
    version: 2.8.2
    web_url: https://github.com/dateutil/dateutil
  licenses:
  - Apache-2.0
  - BSD-3-Clause
  name: python-dateutil
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 2.8.2
  mimer:
    linking: Static
    product_number: CAX1057771
    product_version_label: 2.8.2
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: pytz+2024.1
  additional_info:
    fossa-attribution:
      Description: 'pytz - World Timezone Definitions for Python============================================:Author:
        Stuart Bishop <stuart@stuartbishop.net>Introduction~~~~~~~~~~~~pytz brings
        the Olson tz database into Python. This library allowsaccurate and cross platform
        timezone calculations using Python 2.4or higher. It also solves the issue
        of ambiguous times at the endof daylight saving time, which you can read more
        about in the PythonLibrary Reference (``datetime.tzinfo``).Almost all of the
        Olson timezones are supported... note::    Projects using Python 3.9 or later
        should be using the support    now included as part of the standard library,
        and third party    packages work with it such as `tzdata <https://pypi.org/project/tzdata/>`_.    pytz
        offers no advantages beyond backwards compatibility with    code written for
        earlier versions of Python... note::    This library differs from the documented
        Python API for    tzinfo implementations; if you want to create local wallclock    times
        you need to use the ``localize()`` method documented in this    document.
        In addition, if you perform date arithmetic on local    times that cross DST
        boundaries, the result may be in an incorrect    timezone (ie. subtract 1
        minute from 2002-10-27 1:00 EST and you get    2002-10-27 0:59 EST instead
        of the correct 2002-10-27 1:59 EDT). A    ``normalize()`` method is provided
        to correct this. Unfortunately these    issues cannot be resolved without
        modifying the Python datetime    implementation (see PEP-431).Installation~~~~~~~~~~~~This
        package can either be installed using ``pip`` or from a tarball using thestandard
        Python distutils.If you are installing using ``pip``, you don''t need to download
        anything as thelatest version will be downloaded for you from PyPI::    pip
        install pytzIf you are installing from a tarball, run the following command
        as anadministrative user::    python setup.py installpytz for Enterprise~~~~~~~~~~~~~~~~~~~Available
        as part of the Tidelift Subscription.The maintainers of pytz and thousands
        of other packages are working with Tidelift to deliver commercial support
        and maintenance for the open source dependencies you use to build your applications.
        Save time, reduce risk, and improve code health, while paying the maintainers
        of the exact dependencies you use. `Learn more. <https://tidelift.com/subscription/pkg/pypi-pytz?utm_source=pypi-pytz&utm_medium=referral&utm_campaign=enterprise&utm_term=repo>`_.Example
        & Usage~~~~~~~~~~~~~~~Localized times and date arithmetic----------------------------------->>>
        from datetime import datetime, timedelta>>> from pytz import timezone>>> import
        pytz>>> utc = pytz.utc>>> utc.zone''UTC''>>> eastern = timezone(''US/Eastern'')>>>
        eastern.zone''US/Eastern''>>> amsterdam = timezone(''Europe/Amsterdam'')>>>
        fmt = ''%Y-%m-%d %H:%M:%S %Z%z''This library only supports two ways of building
        a localized time. Thefirst is to use the ``localize()`` method provided by
        the pytz library.This is used to localize a naive datetime (datetime with
        no timezoneinformation):>>> loc_dt = eastern.localize(datetime(2002, 10, 27,
        6, 0, 0))>>> print(loc_dt.strftime(fmt))2002-10-27 06:00:00 EST-0500The second
        way of building a localized time is by converting an existinglocalized time
        using the standard ``astimezone()`` method:>>> ams_dt = loc_dt.astimezone(amsterdam)>>>
        ams_dt.strftime(fmt)''2002-10-27 12:00:00 CET+0100''Unfortunately using the
        tzinfo argument of the standard datetimeconstructors ''''does not work''''
        with pytz for many timezones.>>> datetime(2002, 10, 27, 12, 0, 0, tzinfo=amsterdam).strftime(fmt)  #
        /!\ Does not work this way!''2002-10-27 12:00:00 LMT+0018''It is safe for
        timezones without daylight saving transitions though, suchas UTC:>>> datetime(2002,
        10, 27, 12, 0, 0, tzinfo=pytz.utc).strftime(fmt)  # /!\ Not recommended except
        for UTC''2002-10-27 12:00:00 UTC+0000''The preferred way of dealing with times
        is to always work in UTC,converting to localtime only when generating output
        to be readby humans.>>> utc_dt = datetime(2002, 10, 27, 6, 0, 0, tzinfo=utc)>>>
        loc_dt = utc_dt.astimezone(eastern)>>> loc_dt.strftime(fmt)''2002-10-27 01:00:00
        EST-0500''This library also allows you to do date arithmetic using localtimes,
        although it is more complicated than working in UTC as youneed to use the
        ``normalize()`` method to handle daylight saving timeand other timezone transitions.
        In this example, ``loc_dt`` is setto the instant when daylight saving time
        ends in the US/Easterntimezone.>>> before = loc_dt - timedelta(minutes=10)>>>
        before.strftime(fmt)''2002-10-27 00:50:00 EST-0500''>>> eastern.normalize(before).strftime(fmt)''2002-10-27
        01:50:00 EDT-0400''>>> after = eastern.normalize(before + timedelta(minutes=20))>>>
        after.strftime(fmt)''2002-10-27 01:10:00 EST-0500''Creating local times is
        also tricky, and the reason why working withlocal times is not recommended.
        Unfortunately, you cannot just passa ``tzinfo`` argument when constructing
        a datetime (see the nextsection for more details)>>> dt = datetime(2002, 10,
        27, 1, 30, 0)>>> dt1 = eastern.localize(dt, is_dst=True)>>> dt1.strftime(fmt)''2002-10-27
        01:30:00 EDT-0400''>>> dt2 = eastern.localize(dt, is_dst=False)>>> dt2.strftime(fmt)''2002-10-27
        01:30:00 EST-0500''Converting between timezones is more easily done, using
        thestandard astimezone method.>>> utc_dt = datetime.fromtimestamp(1143408899,
        tz=utc)>>> utc_dt.strftime(fmt)''2006-03-26 21:34:59 UTC+0000''>>> au_tz =
        timezone(''Australia/Sydney'')>>> au_dt = utc_dt.astimezone(au_tz)>>> au_dt.strftime(fmt)''2006-03-27
        08:34:59 AEDT+1100''>>> utc_dt2 = au_dt.astimezone(utc)>>> utc_dt2.strftime(fmt)''2006-03-26
        21:34:59 UTC+0000''>>> utc_dt == utc_dt2TrueYou can take shortcuts when dealing
        with the UTC side of timezoneconversions. ``normalize()`` and ``localize()``
        are not reallynecessary when there are no daylight saving time transitions
        todeal with.>>> utc_dt = datetime.fromtimestamp(1143408899, tz=utc)>>> utc_dt.strftime(fmt)''2006-03-26
        21:34:59 UTC+0000''>>> au_tz = timezone(''Australia/Sydney'')>>> au_dt = au_tz.normalize(utc_dt.astimezone(au_tz))>>>
        au_dt.strftime(fmt)''2006-03-27 08:34:59 AEDT+1100''>>> utc_dt2 = au_dt.astimezone(utc)>>>
        utc_dt2.strftime(fmt)''2006-03-26 21:34:59 UTC+0000''``tzinfo`` API--------------The
        ``tzinfo`` instances returned by the ``timezone()`` function havebeen extended
        to cope with ambiguous times by adding an ``is_dst``parameter to the ``utcoffset()``,
        ``dst()`` && ``tzname()`` methods.>>> tz = timezone(''America/St_Johns'')>>>
        normal = datetime(2009, 9, 1)>>> ambiguous = datetime(2009, 10, 31, 23, 30)The
        ``is_dst`` parameter is ignored for most timestamps. It is only usedduring
        DST transition ambiguous periods to resolve that ambiguity.>>> print(tz.utcoffset(normal,
        is_dst=True))-1 day, 21:30:00>>> print(tz.dst(normal, is_dst=True))1:00:00>>>
        tz.tzname(normal, is_dst=True)''NDT''>>> print(tz.utcoffset(ambiguous, is_dst=True))-1
        day, 21:30:00>>> print(tz.dst(ambiguous, is_dst=True))1:00:00>>> tz.tzname(ambiguous,
        is_dst=True)''NDT''>>> print(tz.utcoffset(normal, is_dst=False))-1 day, 21:30:00>>>
        tz.dst(normal, is_dst=False).seconds3600>>> tz.tzname(normal, is_dst=False)''NDT''>>>
        print(tz.utcoffset(ambiguous, is_dst=False))-1 day, 20:30:00>>> tz.dst(ambiguous,
        is_dst=False)datetime.timedelta(0)>>> tz.tzname(ambiguous, is_dst=False)''NST''If
        ``is_dst`` is not specified, ambiguous timestamps will raisean ``pytz.exceptions.AmbiguousTimeError``
        exception.>>> print(tz.utcoffset(normal))-1 day, 21:30:00>>> print(tz.dst(normal))1:00:00>>>
        tz.tzname(normal)''NDT''>>> import pytz.exceptions>>> try:...     tz.utcoffset(ambiguous)...
        except pytz.exceptions.AmbiguousTimeError:...     print(''pytz.exceptions.AmbiguousTimeError:
        %s'' % ambiguous)pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00>>>
        try:...     tz.dst(ambiguous)... except pytz.exceptions.AmbiguousTimeError:...     print(''pytz.exceptions.AmbiguousTimeError:
        %s'' % ambiguous)pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00>>>
        try:...     tz.tzname(ambiguous)... except pytz.exceptions.AmbiguousTimeError:...     print(''pytz.exceptions.AmbiguousTimeError:
        %s'' % ambiguous)pytz.exceptions.AmbiguousTimeError: 2009-10-31 23:30:00Problems
        with Localtime~~~~~~~~~~~~~~~~~~~~~~~The major problem we have to deal with
        is that certain datetimesmay occur twice in a year. For example, in the US/Eastern
        timezoneon the last Sunday morning in October, the following sequencehappens:    -
        01:00 EDT occurs    - 1 hour later, instead of 2:00am the clock is turned
        back 1 hour      and 01:00 happens again (this time 01:00 EST)In fact, every
        instant between 01:00 and 02:00 occurs twice. This meansthat if you try and
        create a time in the ''US/Eastern'' timezonethe standard datetime syntax,
        there is no way to specify if you meantbefore of after the end-of-daylight-saving-time
        transition. Using thepytz custom syntax, the best you can do is make an educated
        guess:>>> loc_dt = eastern.localize(datetime(2002, 10, 27, 1, 30, 00))>>>
        loc_dt.strftime(fmt)''2002-10-27 01:30:00 EST-0500''As you can see, the system
        has chosen one for you and there is a 50%chance of it being out by one hour.
        For some applications, this doesnot matter. However, if you are trying to
        schedule meetings with peoplein different timezones or analyze log files it
        is not acceptable.The best and simplest solution is to stick with using UTC.  The
        pytzpackage encourages using UTC for internal timezone representation byincluding
        a special UTC implementation based on the standard Pythonreference implementation
        in the Python documentation.The UTC timezone unpickles to be the same instance,
        and pickles to asmaller size than other pytz tzinfo instances.  The UTC implementationcan
        be obtained as pytz.utc, pytz.UTC, or pytz.timezone(''UTC'').>>> import pickle,
        pytz>>> dt = datetime(2005, 3, 1, 14, 13, 21, tzinfo=utc)>>> naive = dt.replace(tzinfo=None)>>>
        p = pickle.dumps(dt, 1)>>> naive_p = pickle.dumps(naive, 1)>>> len(p) - len(naive_p)17>>>
        new = pickle.loads(p)>>> new == dtTrue>>> new is dtFalse>>> new.tzinfo is
        dt.tzinfoTrue>>> pytz.utc is pytz.UTC is pytz.timezone(''UTC'')TrueNote that
        some other timezones are commonly thought of as the same (GMT,Greenwich, Universal,
        etc.). The definition of UTC is distinct from theseother timezones, and they
        are not equivalent. For this reason, they willnot compare the same in Python.>>>
        utc == pytz.timezone(''GMT'')FalseSee the section `What is UTC`_, below.If
        you insist on working with local times, this library provides afacility for
        constructing them unambiguously:>>> loc_dt = datetime(2002, 10, 27, 1, 30,
        00)>>> est_dt = eastern.localize(loc_dt, is_dst=True)>>> edt_dt = eastern.localize(loc_dt,
        is_dst=False)>>> print(est_dt.strftime(fmt) + '' / '' + edt_dt.strftime(fmt))2002-10-27
        01:30:00 EDT-0400 / 2002-10-27 01:30:00 EST-0500If you pass None as the is_dst
        flag to localize(), pytz will refuse toguess and raise exceptions if you try
        to build ambiguous or non-existenttimes.For example, 1:30am on 27th Oct 2002
        happened twice in the US/Easterntimezone when the clocks where put back at
        the end of Daylight SavingTime:>>> dt = datetime(2002, 10, 27, 1, 30, 00)>>>
        try:...     eastern.localize(dt, is_dst=None)... except pytz.exceptions.AmbiguousTimeError:...     print(''pytz.exceptions.AmbiguousTimeError:
        %s'' % dt)pytz.exceptions.AmbiguousTimeError: 2002-10-27 01:30:00Similarly,
        2:30am on 7th April 2002 never happened at all in theUS/Eastern timezone,
        as the clocks where put forward at 2:00am skippingthe entire hour:>>> dt =
        datetime(2002, 4, 7, 2, 30, 00)>>> try:...     eastern.localize(dt, is_dst=None)...
        except pytz.exceptions.NonExistentTimeError:...     print(''pytz.exceptions.NonExistentTimeError:
        %s'' % dt)pytz.exceptions.NonExistentTimeError: 2002-04-07 02:30:00Both of
        these exceptions share a common base class to make error handlingeasier:>>>
        isinstance(pytz.AmbiguousTimeError(), pytz.InvalidTimeError)True>>> isinstance(pytz.NonExistentTimeError(),
        pytz.InvalidTimeError)TrueA special case is where countries change their timezone
        definitionswith no daylight savings time switch. For example, in 1915 Warsawswitched
        from Warsaw time to Central European time with no daylight savingstransition.
        So at the stroke of midnight on August 5th 1915 the clockswere wound back
        24 minutes creating an ambiguous time period that cannotbe specified without
        referring to the timezone abbreviation or theactual UTC offset. In this case
        midnight happened twice, neither timeduring a daylight saving time period.
        pytz handles this transition bytreating the ambiguous period before the switch
        as daylight savingstime, and the ambiguous period after as standard time.>>>
        warsaw = pytz.timezone(''Europe/Warsaw'')>>> amb_dt1 = warsaw.localize(datetime(1915,
        8, 4, 23, 59, 59), is_dst=True)>>> amb_dt1.strftime(fmt)''1915-08-04 23:59:59
        WMT+0124''>>> amb_dt2 = warsaw.localize(datetime(1915, 8, 4, 23, 59, 59),
        is_dst=False)>>> amb_dt2.strftime(fmt)''1915-08-04 23:59:59 CET+0100''>>>
        switch_dt = warsaw.localize(datetime(1915, 8, 5, 00, 00, 00), is_dst=False)>>>
        switch_dt.strftime(fmt)''1915-08-05 00:00:00 CET+0100''>>> str(switch_dt -
        amb_dt1)''0:24:01''>>> str(switch_dt - amb_dt2)''0:00:01''The best way of
        creating a time during an ambiguous time period isby converting from another
        timezone such as UTC:>>> utc_dt = datetime(1915, 8, 4, 22, 36, tzinfo=pytz.utc)>>>
        utc_dt.astimezone(warsaw).strftime(fmt)''1915-08-04 23:36:00 CET+0100''The
        standard Python way of handling all these ambiguities is not tohandle them,
        such as demonstrated in this example using the US/Easterntimezone definition
        from the Python documentation (Note that thisimplementation only works for
        dates between 1987 and 2006 - it isincluded for tests only!):>>> from pytz.reference
        import Eastern # pytz.reference only for tests>>> dt = datetime(2002, 10,
        27, 0, 30, tzinfo=Eastern)>>> str(dt)''2002-10-27 00:30:00-04:00''>>> str(dt
        + timedelta(hours=1))''2002-10-27 01:30:00-05:00''>>> str(dt + timedelta(hours=2))''2002-10-27
        02:30:00-05:00''>>> str(dt + timedelta(hours=3))''2002-10-27 03:30:00-05:00''Notice
        the first two results? At first glance you might think they arecorrect, but
        taking the UTC offset into account you find that they areactually two hours
        appart instead of the 1 hour we asked for.>>> from pytz.reference import UTC
        # pytz.reference only for tests>>> str(dt.astimezone(UTC))''2002-10-27 04:30:00+00:00''>>>
        str((dt + timedelta(hours=1)).astimezone(UTC))''2002-10-27 06:30:00+00:00''Country
        Information~~~~~~~~~~~~~~~~~~~A mechanism is provided to access the timezones
        commonly in usefor a particular country, looked up using the ISO 3166 country
        code.It returns a list of strings that can be used to retrieve the relevanttzinfo
        instance using ``pytz.timezone()``:>>> print('' ''.join(pytz.country_timezones[''nz'']))Pacific/Auckland
        Pacific/ChathamThe Olson database comes with a ISO 3166 country code to English
        countryname mapping that pytz exposes as a dictionary:>>> print(pytz.country_names[''nz''])New
        ZealandWhat is UTC~~~~~~~~~~~''UTC'' is `Coordinated Universal Time`_. It
        is a successor to, but distinctfrom, Greenwich Mean Time (GMT) and the various
        definitions of UniversalTime. UTC is now the worldwide standard for regulating
        clocks and timemeasurement.All other timezones are defined relative to UTC,
        and include offsets likeUTC+0800 - hours to add or subtract from UTC to derive
        the local time. Nodaylight saving time occurs in UTC, making it a useful timezone
        to performdate arithmetic without worrying about the confusion and ambiguities
        causedby daylight saving time transitions, your country changing its timezone,
        ormobile computers that roam through multiple timezones...  _Coordinated Universal
        Time: https://en.wikipedia.org/wiki/Coordinated_Universal_TimeHelpers~~~~~~~There
        are two lists of timezones provided.``all_timezones`` is the exhaustive list
        of the timezone names that canbe used.>>> from pytz import all_timezones>>>
        len(all_timezones) >= 500True>>> ''Etc/Greenwich'' in all_timezonesTrue``common_timezones``
        is a list of useful, current timezones. It doesn''tcontain deprecated zones
        or historical zones, except for a few I''vedeemed in common usage, such as
        US/Eastern (open a bug report if youthink other timezones are deserving of
        being included here). It is alsoa sequence of strings.>>> from pytz import
        common_timezones>>> len(common_timezones) < len(all_timezones)True>>> ''Etc/Greenwich''
        in common_timezonesFalse>>> ''Australia/Melbourne'' in common_timezonesTrue>>>
        ''US/Eastern'' in common_timezonesTrue>>> ''Canada/Eastern'' in common_timezonesTrue>>>
        ''Australia/Yancowinna'' in all_timezonesTrue>>> ''Australia/Yancowinna''
        in common_timezonesFalseBoth ``common_timezones`` and ``all_timezones`` are
        alphabeticallysorted:>>> common_timezones_dupe = common_timezones[:]>>> common_timezones_dupe.sort()>>>
        common_timezones == common_timezones_dupeTrue>>> all_timezones_dupe = all_timezones[:]>>>
        all_timezones_dupe.sort()>>> all_timezones == all_timezones_dupeTrue``all_timezones``
        and ``common_timezones`` are also available as sets.>>> from pytz import all_timezones_set,
        common_timezones_set>>> ''US/Eastern'' in all_timezones_setTrue>>> ''US/Eastern''
        in common_timezones_setTrue>>> ''Australia/Victoria'' in common_timezones_setFalseYou
        can also retrieve lists of timezones used by particular countriesusing the
        ``country_timezones()`` function. It requires an ISO-3166two letter country
        code.>>> from pytz import country_timezones>>> print('' ''.join(country_timezones(''ch'')))Europe/Zurich>>>
        print('' ''.join(country_timezones(''CH'')))Europe/ZurichInternationalization
        - i18n/l10n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Pytz is an interface to the IANA
        database, which uses ASCII names. The `Unicode  Consortium''s Unicode Locales
        (CLDR) <http://cldr.unicode.org>`_project provides translations. Python packages
        such as`Babel <https://babel.pocoo.org/en/latest/api/dates.html#timezone-functionality>`_and
        Thomas Khyn''s `l18n <https://pypi.org/project/l18n/>`_ package can be usedto
        access these translations from Python.License~~~~~~~MIT license.This code
        is also available as part of Zope 3 under the Zope PublicLicense,  Version
        2.1 (ZPL).I''m happy to relicense this code if necessary for inclusion in
        otheropen source projects.Latest Versions~~~~~~~~~~~~~~~This package will
        be updated after releases of the Olson timezonedatabase.  The latest version
        can be downloaded from the `Python PackageIndex <https://pypi.org/project/pytz/>`_.  The
        code that is usedto generate this distribution is hosted on Github and availableusing
        git::    git clone https://github.com/stub42/pytz.gitAnnouncements of new
        releases are made on`Launchpad <https://launchpad.net/pytz>`_, and the`Atom
        feed <http://feeds.launchpad.net/pytz/announcements.atom>`_hosted there.Bugs,
        Feature Requests & Patches~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Bugs should be reported
        on `Github <https://github.com/stub42/pytz/issues>`_.Feature requests are
        unlikely to be considered, and efforts instead directedto timezone support
        now built into Python or packages that work with it.Security Issues~~~~~~~~~~~~~~~Reports
        about security issues can be made via `Tidelift <https://tidelift.com/security>`_.Issues
        & Limitations~~~~~~~~~~~~~~~~~~~~- This project is in maintenance mode. Projects
        using Python 3.9 or later  are best served by using the timezone functionaly
        now included in core  Python and packages that work with it such as `tzdata
        <https://pypi.org/project/tzdata/>`_.- Offsets from UTC are rounded to the
        nearest whole minute, so timezones  such as Europe/Amsterdam pre 1937 will
        be up to 30 seconds out. This  was a limitation of the Python datetime library.-
        If you think a timezone definition is incorrect, I probably can''t fix  it.
        pytz is a direct translation of the Olson timezone database, and  changes
        to the timezone definitions need to be made to this source.  If you find errors
        they should be reported to the time zone mailing  list, linked from http://www.iana.org/time-zones.Further
        Reading~~~~~~~~~~~~~~~More info than you want to know about timezones:https://data.iana.org/time-zones/tz-link.htmlContact~~~~~~~Stuart
        Bishop <stuart@stuartbishop.net>'
      Package: pytz
      Source: pip
      Version: '2024.1'
      Hash: ''
      licenses:
      - MIT
      - public-domain
      Title: pytz
      DownloadURL: https://files.pythonhosted.org/packages/90/26/9f1f00a5d021fff16dee3de13d43e5e978f3d58928e129c3a62cf7eb9738/pytz-2024.1.tar.gz
  bazaar:
    register: 'no'
    prim: 38/CAX1056526
    community_link: http://pythonhosted.org/pytz
    community_name: http://pythonhosted.org/pytz
    community_url: http://pythonhosted.org/pytz
    component_comment: ''
    component_highlevel_description: pytz Python historical timezone library and database
    component_name: pytz
    component_platform: linux
    component_programing_language: ''
    component_version: RELEASE_2024.1
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://files.pythonhosted.org/packages/90/26/9f1f00a5d021fff16dee3de13d43e5e978f3d58928e129c3a62cf7eb9738/pytz-2024.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078650&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: pytz
    target_sw: linux
    vendor: pip
    version: '2024.1'
    web_url: http://pythonhosted.org/pytz
  licenses:
  - MIT
  - public-domain
  name: pytz
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '2024.1'
  mimer:
    linking: Static
    product_number: CAX1056526
    product_version_label: release_2024.1
    selected_licenses:
    - MIT
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: PyYAML+6.0.1
  additional_info:
    fossa-attribution:
      Description: YAML is a data serialization format designed for human readabilityand
        interaction with scripting languages.  PyYAML is a YAML parserand emitter
        for Python.PyYAML features a complete YAML 1.1 parser, Unicode support, picklesupport,
        capable extension API, and sensible error messages.  PyYAMLsupports standard
        YAML tags and provides Python-specific tags thatallow to represent an arbitrary
        Python object.PyYAML is applicable for a broad range of tasks from complexconfiguration
        files to object serialization and persistence.
      Package: PyYAML
      Source: pip
      Version: 6.0.1
      Hash: ''
      licenses:
      - MIT
      Title: PyYAML
      DownloadURL: https://files.pythonhosted.org/packages/cd/e5/af35f7ea75cf72f2cd079c95ee16797de7cd71f29ea7c68ae5ce7be1eda0/PyYAML-6.0.1.tar.gz
  bazaar:
    register: 'no'
    prim: 15/CAX1056382
    community_link: https://github.com/yaml/pyyaml
    community_name: https://github.com/yaml/pyyaml
    community_url: https://github.com/yaml/pyyaml
    component_comment: ''
    component_highlevel_description: Canonical source repository for PyYAML
    component_name: pyyaml
    component_platform: linux
    component_programing_language: Python
    component_version: 6.0.1
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://github.com/yaml/pyyaml/archive/6.0.1.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Low Community Activity.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1053360&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: PyYAML
    target_sw: linux
    vendor: pip
    version: 6.0.1
    web_url: https://pyyaml.org/
  licenses:
  - MIT
  name: PyYAML
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 6.0.1
  mimer:
    linking: Static
    product_number: CAX1056382
    product_version_label: 6.0.1
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: regex+2023.12.25
  additional_info:
    fossa-attribution:
      Description: 'Introduction------------This regex implementation is backwards-compatible
        with the standard ''re'' module, but offers additional functionality.Note----The
        re module''s behaviour with zero-width matches changed in Python 3.7, and
        this module follows that behaviour when compiled for Python 3.7.Python 2--------Python
        2 is no longer supported. The last release that supported Python 2 was 2021.11.10.PyPy----This
        module is targeted at CPython. It expects that all codepoints are the same
        width, so it won''t behave properly with PyPy outside U+0000..U+007F because
        PyPy stores strings as UTF-8.Multithreading--------------The regex module
        releases the GIL during matching on instances of the built-in (immutable)
        string classes, enabling other Python threads to run concurrently. It is also
        possible to force the regex module to release the GIL during matching by calling
        the matching methods with the keyword argument ``concurrent=True``. The behaviour
        is undefined if the string changes during matching, so use it *only* when
        it is guaranteed that that won''t happen.Unicode-------This module supports
        Unicode 15.1.0. Full Unicode case-folding is supported.Flags-----There are
        2 kinds of flag: scoped and global. Scoped flags can apply to only part of
        a pattern and can be turned on or off; global flags apply to the entire pattern
        and can only be turned on.The scoped flags are: ``ASCII (?a)``, ``FULLCASE
        (?f)``, ``IGNORECASE (?i)``, ``LOCALE (?L)``, ``MULTILINE (?m)``, ``DOTALL
        (?s)``, ``UNICODE (?u)``, ``VERBOSE (?x)``, ``WORD (?w)``.The global flags
        are: ``BESTMATCH (?b)``, ``ENHANCEMATCH (?e)``, ``POSIX (?p)``, ``REVERSE
        (?r)``, ``VERSION0 (?V0)``, ``VERSION1 (?V1)``.If neither the ``ASCII``, ``LOCALE``
        nor ``UNICODE`` flag is specified, it will default to ``UNICODE`` if the regex
        pattern is a Unicode string and ``ASCII`` if it''s a bytestring.The ``ENHANCEMATCH``
        flag makes fuzzy matching attempt to improve the fit of the next match that
        it finds.The ``BESTMATCH`` flag makes fuzzy matching search for the best match
        instead of the next match.Old vs new behaviour--------------------In order
        to be compatible with the re module, this module has 2 behaviours:* **Version
        0** behaviour (old behaviour, compatible with the re module):  Please note
        that the re module''s behaviour may change over time, and I''ll endeavour
        to match that behaviour in version 0.  * Indicated by the ``VERSION0`` flag.  *
        Zero-width matches are not handled correctly in the re module before Python
        3.7. The behaviour in those earlier versions is:    * ``.split`` won''t split
        a string at a zero-width match.    * ``.sub`` will advance by one character
        after a zero-width match.  * Inline flags apply to the entire pattern, and
        they can''t be turned off.  * Only simple sets are supported.  * Case-insensitive
        matches in Unicode use simple case-folding by default.* **Version 1** behaviour
        (new behaviour, possibly different from the re module):  * Indicated by the
        ``VERSION1`` flag.  * Zero-width matches are handled correctly.  * Inline
        flags apply to the end of the group or pattern, and they can be turned off.  *
        Nested sets and set operations are supported.  * Case-insensitive matches
        in Unicode use full case-folding by default.If no version is specified, the
        regex module will default to ``regex.DEFAULT_VERSION``.Case-insensitive matches
        in Unicode-----------------------------------The regex module supports both
        simple and full case-folding for case-insensitive matches in Unicode. Use
        of full case-folding can be turned on using the ``FULLCASE`` flag. Please
        note that this flag affects how the ``IGNORECASE`` flag works; the ``FULLCASE``
        flag itself does not turn on case-insensitive matching.Version 0 behaviour:
        the flag is off by default.Version 1 behaviour: the flag is on by default.Nested
        sets and set operations------------------------------It''s not possible to
        support both simple sets, as used in the re module, and nested sets at the
        same time because of a difference in the meaning of an unescaped ``"["`` in
        a set.For example, the pattern ``[[a-z]--[aeiou]]`` is treated in the version
        0 behaviour (simple sets, compatible with the re module) as:* Set containing
        "[" and the letters "a" to "z"* Literal "--"* Set containing letters "a",
        "e", "i", "o", "u"* Literal "]"but in the version 1 behaviour (nested sets,
        enhanced behaviour) as:* Set which is:  * Set containing the letters "a" to
        "z"* but excluding:  * Set containing the letters "a", "e", "i", "o", "u"Version
        0 behaviour: only simple sets are supported.Version 1 behaviour: nested sets
        and set operations are supported.Notes on named groups---------------------All
        groups have a group number, starting from 1.Groups with the same group name
        will have the same group number, and groups with a different group name will
        have a different group number.The same name can be used by more than one group,
        with later captures ''overwriting'' earlier captures. All the captures of
        the group will be available from the ``captures`` method of the match object.Group
        numbers will be reused across different branches of a branch reset, eg. ``(?|(first)|(second))``
        has only group 1. If groups have different group names then they will, of
        course, have different group numbers, eg. ``(?|(?P<foo>first)|(?P<bar>second))``
        has group 1 ("foo") and group 2 ("bar").In the regex ``(\s+)(?|(?P<foo>[A-Z]+)|(\w+)
        (?P<foo>[0-9]+)`` there are 2 groups:* ``(\s+)`` is group 1.* ``(?P<foo>[A-Z]+)``
        is group 2, also called "foo".* ``(\w+)`` is group 2 because of the branch
        reset.* ``(?P<foo>[0-9]+)`` is group 2 because it''s called "foo".If you want
        to prevent ``(\w+)`` from being group 2, you need to name it (different name,
        different group number).Additional features-------------------The issue numbers
        relate to the Python bug tracker, except where listed otherwise.Added ``\p{Horiz_Space}``
        and ``\p{Vert_Space}`` (`GitHub issue 477 <https://github.com/mrabarnett/mrab-regex/issues/477#issuecomment-1216779547>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``\p{Horiz_Space}``
        or ``\p{H}`` matches horizontal whitespace and ``\p{Vert_Space}`` or ``\p{V}``
        matches vertical whitespace.Added support for lookaround in conditional pattern
        (`Hg issue 163 <https://github.com/mrabarnett/mrab-regex/issues/163>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^The
        test of a conditional pattern can be a lookaround... sourcecode:: python  >>>
        regex.match(r''(?(?=\d)\d+|\w+)'', ''123abc'')  <regex.Match object; span=(0,
        3), match=''123''>  >>> regex.match(r''(?(?=\d)\d+|\w+)'', ''abc123'')  <regex.Match
        object; span=(0, 6), match=''abc123''>This is not quite the same as putting
        a lookaround in the first branch of a pair of alternatives... sourcecode::
        python  >>> print(regex.match(r''(?:(?=\d)\d+\b|\w+)'', ''123abc''))  <regex.Match
        object; span=(0, 6), match=''123abc''>  >>> print(regex.match(r''(?(?=\d)\d+\b|\w+)'',
        ''123abc''))  NoneIn the first example, the lookaround matched, but the remainder
        of the first branch failed to match, and so the second branch was attempted,
        whereas in the second example, the lookaround matched, and the first branch
        failed to match, but the second branch was **not** attempted.Added POSIX matching
        (leftmost longest) (`Hg issue 150 <https://github.com/mrabarnett/mrab-regex/issues/150>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^The
        POSIX standard for regex is to return the leftmost longest match. This can
        be turned on using the ``POSIX`` flag... sourcecode:: python  >>> # Normal
        matching.  >>> regex.search(r''Mr|Mrs'', ''Mrs'')  <regex.Match object; span=(0,
        2), match=''Mr''>  >>> regex.search(r''one(self)?(selfsufficient)?'', ''oneselfsufficient'')  <regex.Match
        object; span=(0, 7), match=''oneself''>  >>> # POSIX matching.  >>> regex.search(r''(?p)Mr|Mrs'',
        ''Mrs'')  <regex.Match object; span=(0, 3), match=''Mrs''>  >>> regex.search(r''(?p)one(self)?(selfsufficient)?'',
        ''oneselfsufficient'')  <regex.Match object; span=(0, 17), match=''oneselfsufficient''>Note
        that it will take longer to find matches because when it finds a match at
        a certain position, it won''t return that immediately, but will keep looking
        to see if there''s another longer match there.Added ``(?(DEFINE)...)`` (`Hg
        issue 152 <https://github.com/mrabarnett/mrab-regex/issues/152>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^If
        there''s no group called "DEFINE", then ... will be ignored except that any
        groups defined within it can be called and that the normal rules for numbering
        groups still apply... sourcecode:: python  >>> regex.search(r''(?(DEFINE)(?P<quant>\d+)(?P<item>\w+))(?&quant)
        (?&item)'', ''5 elephants'')  <regex.Match object; span=(0, 11), match=''5
        elephants''>Added ``(*PRUNE)``, ``(*SKIP)`` and ``(*FAIL)`` (`Hg issue 153
        <https://github.com/mrabarnett/mrab-regex/issues/153>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``(*PRUNE)``
        discards the backtracking info up to that point. When used in an atomic group
        or a lookaround, it won''t affect the enclosing pattern.``(*SKIP)`` is similar
        to ``(*PRUNE)``, except that it also sets where in the text the next attempt
        to match will start. When used in an atomic group or a lookaround, it won''t
        affect the enclosing pattern.``(*FAIL)`` causes immediate backtracking. ``(*F)``
        is a permitted abbreviation.Added ``\K`` (`Hg issue 151 <https://github.com/mrabarnett/mrab-regex/issues/151>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Keeps
        the part of the entire match after the position where ``\K`` occurred; the
        part before it is discarded.It does not affect what groups return... sourcecode::
        python  >>> m = regex.search(r''(\w\w\K\w\w\w)'', ''abcdef'')  >>> m[0]  ''cde''  >>>
        m[1]  ''abcde''  >>>  >>> m = regex.search(r''(?r)(\w\w\K\w\w\w)'', ''abcdef'')  >>>
        m[0]  ''bc''  >>> m[1]  ''bcdef''Added capture subscripting for ``expandf``
        and ``subf``/``subfn`` (`Hg issue 133 <https://github.com/mrabarnett/mrab-regex/issues/133>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^You
        can use subscripting to get the captures of a repeated group... sourcecode::
        python  >>> m = regex.match(r"(\w)+", "abc")  >>> m.expandf("{1}")  ''c''  >>>
        m.expandf("{1[0]} {1[1]} {1[2]}")  ''a b c''  >>> m.expandf("{1[-1]} {1[-2]}
        {1[-3]}")  ''c b a''  >>>  >>> m = regex.match(r"(?P<letter>\w)+", "abc")  >>>
        m.expandf("{letter}")  ''c''  >>> m.expandf("{letter[0]} {letter[1]} {letter[2]}")  ''a
        b c''  >>> m.expandf("{letter[-1]} {letter[-2]} {letter[-3]}")  ''c b a''Added
        support for referring to a group by number using ``(?P=...)``^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^This
        is in addition to the existing ``\g<...>``.Fixed the handling of locale-sensitive
        regexes^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^The ``LOCALE`` flag is
        intended for legacy code and has limited support. You''re still recommended
        to use Unicode instead.Added partial matches (`Hg issue 102 <https://github.com/mrabarnett/mrab-regex/issues/102>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^A
        partial match is one that matches up to the end of string, but that string
        has been truncated and you want to know whether a complete match could be
        possible if the string had not been truncated.Partial matches are supported
        by ``match``, ``search``, ``fullmatch`` and ``finditer`` with the ``partial``
        keyword argument.Match objects have a ``partial`` attribute, which is ``True``
        if it''s a partial match.For example, if you wanted a user to enter a 4-digit
        number and check it character by character as it was being entered:.. sourcecode::
        python  >>> pattern = regex.compile(r''\d{4}'')  >>> # Initially, nothing
        has been entered:  >>> print(pattern.fullmatch('''', partial=True))  <regex.Match
        object; span=(0, 0), match='''', partial=True>  >>> # An empty string is OK,
        but it''s only a partial match.  >>> # The user enters a letter:  >>> print(pattern.fullmatch(''a'',
        partial=True))  None  >>> # It''ll never match.  >>> # The user deletes that
        and enters a digit:  >>> print(pattern.fullmatch(''1'', partial=True))  <regex.Match
        object; span=(0, 1), match=''1'', partial=True>  >>> # It matches this far,
        but it''s only a partial match.  >>> # The user enters 2 more digits:  >>>
        print(pattern.fullmatch(''123'', partial=True))  <regex.Match object; span=(0,
        3), match=''123'', partial=True>  >>> # It matches this far, but it''s only
        a partial match.  >>> # The user enters another digit:  >>> print(pattern.fullmatch(''1234'',
        partial=True))  <regex.Match object; span=(0, 4), match=''1234''>  >>> # It''s
        a complete match.  >>> # If the user enters another digit:  >>> print(pattern.fullmatch(''12345'',
        partial=True))  None  >>> # It''s no longer a match.  >>> # This is a partial
        match:  >>> pattern.match(''123'', partial=True).partial  True  >>> # This
        is a complete match:  >>> pattern.match(''1233'', partial=True).partial  False``*``
        operator not working correctly with sub() (`Hg issue 106 <https://github.com/mrabarnett/mrab-regex/issues/106>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Sometimes
        it''s not clear how zero-width matches should be handled. For example, should
        ``.*`` match 0 characters directly after matching >0 characters?.. sourcecode::
        python  # Python 3.7 and later  >>> regex.sub(''.*'', ''x'', ''test'')  ''xx''  >>>
        regex.sub(''.*?'', ''|'', ''test'')  ''|||||||||''  # Python 3.6 and earlier  >>>
        regex.sub(''(?V0).*'', ''x'', ''test'')  ''x''  >>> regex.sub(''(?V1).*'',
        ''x'', ''test'')  ''xx''  >>> regex.sub(''(?V0).*?'', ''|'', ''test'')  ''|t|e|s|t|''  >>>
        regex.sub(''(?V1).*?'', ''|'', ''test'')  ''|||||||||''Added ``capturesdict``
        (`Hg issue 86 <https://github.com/mrabarnett/mrab-regex/issues/86>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``capturesdict``
        is a combination of ``groupdict`` and ``captures``:``groupdict`` returns a
        dict of the named groups and the last capture of those groups.``captures``
        returns a list of all the captures of a group``capturesdict`` returns a dict
        of the named groups and lists of all the captures of those groups... sourcecode::
        python  >>> m = regex.match(r"(?:(?P<word>\w+) (?P<digits>\d+)\n)+", "one
        1\ntwo 2\nthree 3\n")  >>> m.groupdict()  {''word'': ''three'', ''digits'':
        ''3''}  >>> m.captures("word")  [''one'', ''two'', ''three'']  >>> m.captures("digits")  [''1'',
        ''2'', ''3'']  >>> m.capturesdict()  {''word'': [''one'', ''two'', ''three''],
        ''digits'': [''1'', ''2'', ''3'']}Added ``allcaptures`` and ``allspans`` (`Git
        issue 474 <https://github.com/mrabarnett/mrab-regex/issues/474>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``allcaptures``
        returns a list of all the captures of all the groups.``allspans`` returns
        a list of all the spans of the all captures of all the groups... sourcecode::
        python  >>> m = regex.match(r"(?:(?P<word>\w+) (?P<digits>\d+)\n)+", "one
        1\ntwo 2\nthree 3\n")  >>> m.allcaptures()  ([''one 1\ntwo 2\nthree 3\n''],
        [''one'', ''two'', ''three''], [''1'', ''2'', ''3''])  >>> m.allspans()  ([(0,
        20)], [(0, 3), (6, 9), (12, 17)], [(4, 5), (10, 11), (18, 19)])Allow duplicate
        names of groups (`Hg issue 87 <https://github.com/mrabarnett/mrab-regex/issues/87>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Group
        names can be duplicated... sourcecode:: python  >>> # With optional groups:  >>>  >>>
        # Both groups capture, the second capture ''overwriting'' the first.  >>>
        m = regex.match(r"(?P<item>\w+)? or (?P<item>\w+)?", "first or second")  >>>
        m.group("item")  ''second''  >>> m.captures("item")  [''first'', ''second'']  >>>
        # Only the second group captures.  >>> m = regex.match(r"(?P<item>\w+)? or
        (?P<item>\w+)?", " or second")  >>> m.group("item")  ''second''  >>> m.captures("item")  [''second'']  >>>
        # Only the first group captures.  >>> m = regex.match(r"(?P<item>\w+)? or
        (?P<item>\w+)?", "first or ")  >>> m.group("item")  ''first''  >>> m.captures("item")  [''first'']  >>>  >>>
        # With mandatory groups:  >>>  >>> # Both groups capture, the second capture
        ''overwriting'' the first.  >>> m = regex.match(r"(?P<item>\w*) or (?P<item>\w*)?",
        "first or second")  >>> m.group("item")  ''second''  >>> m.captures("item")  [''first'',
        ''second'']  >>> # Again, both groups capture, the second capture ''overwriting''
        the first.  >>> m = regex.match(r"(?P<item>\w*) or (?P<item>\w*)", " or second")  >>>
        m.group("item")  ''second''  >>> m.captures("item")  ['''', ''second'']  >>>
        # And yet again, both groups capture, the second capture ''overwriting'' the
        first.  >>> m = regex.match(r"(?P<item>\w*) or (?P<item>\w*)", "first or ")  >>>
        m.group("item")  ''''  >>> m.captures("item")  [''first'', '''']Added ``fullmatch``
        (`issue #16203 <https://bugs.python.org/issue16203>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``fullmatch``
        behaves like ``match``, except that it must match all of the string... sourcecode::
        python  >>> print(regex.fullmatch(r"abc", "abc").span())  (0, 3)  >>> print(regex.fullmatch(r"abc",
        "abcx"))  None  >>> print(regex.fullmatch(r"abc", "abcx", endpos=3).span())  (0,
        3)  >>> print(regex.fullmatch(r"abc", "xabcy", pos=1, endpos=4).span())  (1,
        4)  >>>  >>> regex.match(r"a.*?", "abcd").group(0)  ''a''  >>> regex.fullmatch(r"a.*?",
        "abcd").group(0)  ''abcd''Added ``subf`` and ``subfn``^^^^^^^^^^^^^^^^^^^^^^^^^^^^``subf``
        and ``subfn`` are alternatives to ``sub`` and ``subn`` respectively. When
        passed a replacement string, they treat it as a format string... sourcecode::
        python  >>> regex.subf(r"(\w+) (\w+)", "{0} => {2} {1}", "foo bar")  ''foo
        bar => bar foo''  >>> regex.subf(r"(?P<word1>\w+) (?P<word2>\w+)", "{word2}
        {word1}", "foo bar")  ''bar foo''Added ``expandf`` to match object^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``expandf``
        is an alternative to ``expand``. When passed a replacement string, it treats
        it as a format string... sourcecode:: python  >>> m = regex.match(r"(\w+)
        (\w+)", "foo bar")  >>> m.expandf("{0} => {2} {1}")  ''foo bar => bar foo''  >>>  >>>
        m = regex.match(r"(?P<word1>\w+) (?P<word2>\w+)", "foo bar")  >>> m.expandf("{word2}
        {word1}")  ''bar foo''Detach searched string^^^^^^^^^^^^^^^^^^^^^^A match
        object contains a reference to the string that was searched, via its ``string``
        attribute. The ``detach_string`` method will ''detach'' that string, making
        it available for garbage collection, which might save valuable memory if that
        string is very large... sourcecode:: python  >>> m = regex.search(r"\w+",
        "Hello world")  >>> print(m.group())  Hello  >>> print(m.string)  Hello world  >>>
        m.detach_string()  >>> print(m.group())  Hello  >>> print(m.string)  NoneRecursive
        patterns (`Hg issue 27 <https://github.com/mrabarnett/mrab-regex/issues/27>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Recursive
        and repeated patterns are supported.``(?R)`` or ``(?0)`` tries to match the
        entire regex recursively. ``(?1)``, ``(?2)``, etc, try to match the relevant
        group.``(?&name)`` tries to match the named group... sourcecode:: python  >>>
        regex.match(r"(Tarzan|Jane) loves (?1)", "Tarzan loves Jane").groups()  (''Tarzan'',)  >>>
        regex.match(r"(Tarzan|Jane) loves (?1)", "Jane loves Tarzan").groups()  (''Jane'',)  >>>
        m = regex.search(r"(\w)(?:(?R)|(\w?))\1", "kayak")  >>> m.group(0, 1, 2)  (''kayak'',
        ''k'', None)The first two examples show how the subpattern within the group
        is reused, but is _not_ itself a group. In other words, ``"(Tarzan|Jane) loves
        (?1)"`` is equivalent to ``"(Tarzan|Jane) loves (?:Tarzan|Jane)"``.It''s possible
        to backtrack into a recursed or repeated group.You can''t call a group if
        there is more than one group with that group name or group number (``"ambiguous
        group reference"``).The alternative forms ``(?P>name)`` and ``(?P&name)``
        are also supported.Full Unicode case-folding is supported^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^In
        version 1 behaviour, the regex module uses full case-folding when performing
        case-insensitive matches in Unicode... sourcecode:: python  >>> regex.match(r"(?iV1)strasse",
        "stra\N{LATIN SMALL LETTER SHARP S}e").span()  (0, 6)  >>> regex.match(r"(?iV1)stra\N{LATIN
        SMALL LETTER SHARP S}e", "STRASSE").span()  (0, 7)In version 0 behaviour,
        it uses simple case-folding for backward compatibility with the re module.Approximate
        "fuzzy" matching (`Hg issue 12 <https://github.com/mrabarnett/mrab-regex/issues/12>`_,
        `Hg issue 41 <https://github.com/mrabarnett/mrab-regex/issues/41>`_, `Hg issue
        109 <https://github.com/mrabarnett/mrab-regex/issues/109>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Regex
        usually attempts an exact match, but sometimes an approximate, or "fuzzy",
        match is needed, for those cases where the text being searched may contain
        errors in the form of inserted, deleted or substituted characters.A fuzzy
        regex specifies which types of errors are permitted, and, optionally, either
        the minimum and maximum or only the maximum permitted number of each type.
        (You cannot specify only a minimum.)The 3 types of error are:* Insertion,
        indicated by "i"* Deletion, indicated by "d"* Substitution, indicated by "s"In
        addition, "e" indicates any type of error.The fuzziness of a regex item is
        specified between "{" and "}" after the item.Examples:* ``foo`` match "foo"
        exactly* ``(?:foo){i}`` match "foo", permitting insertions* ``(?:foo){d}``
        match "foo", permitting deletions* ``(?:foo){s}`` match "foo", permitting
        substitutions* ``(?:foo){i,s}`` match "foo", permitting insertions and substitutions*
        ``(?:foo){e}`` match "foo", permitting errorsIf a certain type of error is
        specified, then any type not specified will **not** be permitted.In the following
        examples I''ll omit the item and write only the fuzziness:* ``{d<=3}`` permit
        at most 3 deletions, but no other types* ``{i<=1,s<=2}`` permit at most 1
        insertion and at most 2 substitutions, but no deletions* ``{1<=e<=3}`` permit
        at least 1 and at most 3 errors* ``{i<=2,d<=2,e<=3}`` permit at most 2 insertions,
        at most 2 deletions, at most 3 errors in total, but no substitutionsIt''s
        also possible to state the costs of each type of error and the maximum permitted
        total cost.Examples:* ``{2i+2d+1s<=4}`` each insertion costs 2, each deletion
        costs 2, each substitution costs 1, the total cost must not exceed 4* ``{i<=1,d<=1,s<=1,2i+2d+1s<=4}``
        at most 1 insertion, at most 1 deletion, at most 1 substitution; each insertion
        costs 2, each deletion costs 2, each substitution costs 1, the total cost
        must not exceed 4You can also use "<" instead of "<=" if you want an exclusive
        minimum or maximum.You can add a test to perform on a character that''s substituted
        or inserted.Examples:* ``{s<=2:[a-z]}`` at most 2 substitutions, which must
        be in the character set ``[a-z]``.* ``{s<=2,i<=3:\d}`` at most 2 substitutions,
        at most 3 insertions, which must be digits.By default, fuzzy matching searches
        for the first match that meets the given constraints. The ``ENHANCEMATCH``
        flag will cause it to attempt to improve the fit (i.e. reduce the number of
        errors) of the match that it has found.The ``BESTMATCH`` flag will make it
        search for the best match instead.Further examples to note:* ``regex.search("(dog){e}",
        "cat and dog")[1]`` returns ``"cat"`` because that matches ``"dog"`` with
        3 errors (an unlimited number of errors is permitted).* ``regex.search("(dog){e<=1}",
        "cat and dog")[1]`` returns ``" dog"`` (with a leading space) because that
        matches ``"dog"`` with 1 error, which is within the limit.* ``regex.search("(?e)(dog){e<=1}",
        "cat and dog")[1]`` returns ``"dog"`` (without a leading space) because the
        fuzzy search matches ``" dog"`` with 1 error, which is within the limit, and
        the ``(?e)`` then it attempts a better fit.In the first two examples there
        are perfect matches later in the string, but in neither case is it the first
        possible match.The match object has an attribute ``fuzzy_counts`` which gives
        the total number of substitutions, insertions and deletions... sourcecode::
        python  >>> # A ''raw'' fuzzy match:  >>> regex.fullmatch(r"(?:cats|cat){e<=1}",
        "cat").fuzzy_counts  (0, 0, 1)  >>> # 0 substitutions, 0 insertions, 1 deletion.  >>>
        # A better match might be possible if the ENHANCEMATCH flag used:  >>> regex.fullmatch(r"(?e)(?:cats|cat){e<=1}",
        "cat").fuzzy_counts  (0, 0, 0)  >>> # 0 substitutions, 0 insertions, 0 deletions.The
        match object also has an attribute ``fuzzy_changes`` which gives a tuple of
        the positions of the substitutions, insertions and deletions... sourcecode::
        python  >>> m = regex.search(''(fuu){i<=2,d<=2,e<=5}'', ''anaconda foo bar'')  >>>
        m  <regex.Match object; span=(7, 10), match=''a f'', fuzzy_counts=(0, 2, 2)>  >>>
        m.fuzzy_changes  ([], [7, 8], [10, 11])What this means is that if the matched
        part of the string had been:.. sourcecode:: python  ''anacondfuuoo bar''it
        would''ve been an exact match.However, there were insertions at positions
        7 and 8:.. sourcecode:: python  ''anaconda fuuoo bar''          ^^and deletions
        at positions 10 and 11:.. sourcecode:: python  ''anaconda f~~oo bar''             ^^So
        the actual string was:.. sourcecode:: python  ''anaconda foo bar''Named lists
        ``\L<name>`` (`Hg issue 11 <https://github.com/mrabarnett/mrab-regex/issues/11>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^There
        are occasions where you may want to include a list (actually, a set) of options
        in a regex.One way is to build the pattern like this:.. sourcecode:: python  >>>
        p = regex.compile(r"first|second|third|fourth|fifth")but if the list is large,
        parsing the resulting regex can take considerable time, and care must also
        be taken that the strings are properly escaped and properly ordered, for example,
        "cats" before "cat".The new alternative is to use a named list:.. sourcecode::
        python  >>> option_set = ["first", "second", "third", "fourth", "fifth"]  >>>
        p = regex.compile(r"\L<options>", options=option_set)The order of the items
        is irrelevant, they are treated as a set. The named lists are available as
        the ``.named_lists`` attribute of the pattern object :.. sourcecode:: python  >>>
        print(p.named_lists)  {''options'': frozenset({''third'', ''first'', ''fifth'',
        ''fourth'', ''second''})}If there are any unused keyword arguments, ``ValueError``
        will be raised unless you tell it otherwise:.. sourcecode:: python  >>> option_set
        = ["first", "second", "third", "fourth", "fifth"]  >>> p = regex.compile(r"\L<options>",
        options=option_set, other_options=[])  Traceback (most recent call last):    File
        "<stdin>", line 1, in <module>    File "C:\Python310\lib\site-packages\regex\regex.py",
        line 353, in compile      return _compile(pattern, flags, ignore_unused, kwargs,
        cache_pattern)    File "C:\Python310\lib\site-packages\regex\regex.py", line
        500, in _compile      complain_unused_args()    File "C:\Python310\lib\site-packages\regex\regex.py",
        line 483, in complain_unused_args      raise ValueError(''unused keyword argument
        {!a}''.format(any_one))  ValueError: unused keyword argument ''other_options''  >>>
        p = regex.compile(r"\L<options>", options=option_set, other_options=[], ignore_unused=True)  >>>
        p = regex.compile(r"\L<options>", options=option_set, other_options=[], ignore_unused=False)  Traceback
        (most recent call last):    File "<stdin>", line 1, in <module>    File "C:\Python310\lib\site-packages\regex\regex.py",
        line 353, in compile      return _compile(pattern, flags, ignore_unused, kwargs,
        cache_pattern)    File "C:\Python310\lib\site-packages\regex\regex.py", line
        500, in _compile      complain_unused_args()    File "C:\Python310\lib\site-packages\regex\regex.py",
        line 483, in complain_unused_args      raise ValueError(''unused keyword argument
        {!a}''.format(any_one))  ValueError: unused keyword argument ''other_options''  >>>Start
        and end of word^^^^^^^^^^^^^^^^^^^^^``\m`` matches at the start of a word.``\M``
        matches at the end of a word.Compare with ``\b``, which matches at the start
        or end of a word.Unicode line separators^^^^^^^^^^^^^^^^^^^^^^^Normally the
        only line separator is ``\n`` (``\x0A``), but if the ``WORD`` flag is turned
        on then the line separators are ``\x0D\x0A``, ``\x0A``, ``\x0B``, ``\x0C``
        and ``\x0D``, plus ``\x85``, ``\u2028`` and ``\u2029`` when working with Unicode.This
        affects the regex dot ``"."``, which, with the ``DOTALL`` flag turned off,
        matches any character except a line separator. It also affects the line anchors
        ``^`` and ``$`` (in multiline mode).Set operators^^^^^^^^^^^^^**Version 1
        behaviour only**Set operators have been added, and a set ``[...]`` can include
        nested sets.The operators, in order of increasing precedence, are:* ``||``
        for union ("x||y" means "x or y")* ``~~`` (double tilde) for symmetric difference
        ("x~~y" means "x or y, but not both")* ``&&`` for intersection ("x&&y" means
        "x and y")* ``--`` (double dash) for difference ("x--y" means "x but not y")Implicit
        union, ie, simple juxtaposition like in ``[ab]``, has the highest precedence.
        Thus, ``[ab&&cd]`` is the same as ``[[a||b]&&[c||d]]``.Examples:* ``[ab]``
        # Set containing ''a'' and ''b''* ``[a-z]`` # Set containing ''a'' .. ''z''*
        ``[[a-z]--[qw]]`` # Set containing ''a'' .. ''z'', but not ''q'' or ''w''*
        ``[a-z--qw]`` # Same as above* ``[\p{L}--QW]`` # Set containing all letters
        except ''Q'' and ''W''* ``[\p{N}--[0-9]]`` # Set containing all numbers except
        ''0'' .. ''9''* ``[\p{ASCII}&&\p{Letter}]`` # Set containing all characters
        which are ASCII and letterregex.escape (`issue #2650 <https://bugs.python.org/issue2650>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^regex.escape
        has an additional keyword parameter ``special_only``. When True, only ''special''
        regex characters, such as ''?'', are escaped... sourcecode:: python  >>> regex.escape("foo!?",
        special_only=False)  ''foo\\!\\?''  >>> regex.escape("foo!?", special_only=True)  ''foo!\\?''regex.escape
        (`Hg issue 249 <https://github.com/mrabarnett/mrab-regex/issues/249>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^regex.escape
        has an additional keyword parameter ``literal_spaces``. When True, spaces
        are not escaped... sourcecode:: python  >>> regex.escape("foo bar!?", literal_spaces=False)  ''foo\\
        bar!\\?''  >>> regex.escape("foo bar!?", literal_spaces=True)  ''foo bar!\\?''Repeated
        captures (`issue #7132 <https://bugs.python.org/issue7132>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^A
        match object has additional methods which return information on all the successful
        matches of a repeated group. These methods are:* ``matchobject.captures([group1,
        ...])``  * Returns a list of the strings matched in a group or groups. Compare
        with ``matchobject.group([group1, ...])``.* ``matchobject.starts([group])``  *
        Returns a list of the start positions. Compare with ``matchobject.start([group])``.*
        ``matchobject.ends([group])``  * Returns a list of the end positions. Compare
        with ``matchobject.end([group])``.* ``matchobject.spans([group])``  * Returns
        a list of the spans. Compare with ``matchobject.span([group])``... sourcecode::
        python  >>> m = regex.search(r"(\w{3})+", "123456789")  >>> m.group(1)  ''789''  >>>
        m.captures(1)  [''123'', ''456'', ''789'']  >>> m.start(1)  6  >>> m.starts(1)  [0,
        3, 6]  >>> m.end(1)  9  >>> m.ends(1)  [3, 6, 9]  >>> m.span(1)  (6, 9)  >>>
        m.spans(1)  [(0, 3), (3, 6), (6, 9)]Atomic grouping ``(?>...)`` (`issue #433030
        <https://bugs.python.org/issue433030>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^If
        the following pattern subsequently fails, then the subpattern as a whole will
        fail.Possessive quantifiers^^^^^^^^^^^^^^^^^^^^^^``(?:...)?+`` ; ``(?:...)*+``
        ; ``(?:...)++`` ; ``(?:...){min,max}+``The subpattern is matched up to ''max''
        times. If the following pattern subsequently fails, then all the repeated
        subpatterns will fail as a whole. For example, ``(?:...)++`` is equivalent
        to ``(?>(?:...)+)``.Scoped flags (`issue #433028 <https://bugs.python.org/issue433028>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``(?flags-flags:...)``The
        flags will apply only to the subpattern. Flags can be turned on or off.Definition
        of ''word'' character (`issue #1693050 <https://bugs.python.org/issue1693050>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^The
        definition of a ''word'' character has been expanded for Unicode. It conforms
        to the Unicode specification at ``http://www.unicode.org/reports/tr29/``.Variable-length
        lookbehind^^^^^^^^^^^^^^^^^^^^^^^^^^A lookbehind can match a variable-length
        string.Flags argument for regex.split, regex.sub and regex.subn (`issue #3482
        <https://bugs.python.org/issue3482>`_)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``regex.split``,
        ``regex.sub`` and ``regex.subn`` support a ''flags'' argument.Pos and endpos
        arguments for regex.sub and regex.subn^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``regex.sub``
        and ``regex.subn`` support ''pos'' and ''endpos'' arguments.''Overlapped''
        argument for regex.findall and regex.finditer^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``regex.findall``
        and ``regex.finditer`` support an ''overlapped'' flag which permits overlapped
        matches.Splititer^^^^^^^^^``regex.splititer`` has been added. It''s a generator
        equivalent of ``regex.split``.Subscripting match objects for groups^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^A
        match object accepts access to the groups via subscripting and slicing:..
        sourcecode:: python  >>> m = regex.search(r"(?P<before>.*?)(?P<num>\d+)(?P<after>.*)",
        "pqr123stu")  >>> print(m["before"])  pqr  >>> print(len(m))  4  >>> print(m[:])  (''pqr123stu'',
        ''pqr'', ''123'', ''stu'')Named groups^^^^^^^^^^^^Groups can be named with
        ``(?<name>...)`` as well as the existing ``(?P<name>...)``.Group references^^^^^^^^^^^^^^^^Groups
        can be referenced within a pattern with ``\g<name>``. This also allows there
        to be more than 99 groups.Named characters ``\N{name}``^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Named
        characters are supported. Note that only those known by Python''s Unicode
        database will be recognised.Unicode codepoint properties, including scripts
        and blocks^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``\p{property=value}``;
        ``\P{property=value}``; ``\p{value}`` ; ``\P{value}``Many Unicode properties
        are supported, including blocks and scripts. ``\p{property=value}`` or ``\p{property:value}``
        matches a character whose property ``property`` has value ``value``. The inverse
        of ``\p{property=value}`` is ``\P{property=value}`` or ``\p{^property=value}``.If
        the short form ``\p{value}`` is used, the properties are checked in the order:
        ``General_Category``, ``Script``, ``Block``, binary property:* ``Latin``,
        the ''Latin'' script (``Script=Latin``).* ``BasicLatin``, the ''BasicLatin''
        block (``Block=BasicLatin``).* ``Alphabetic``, the ''Alphabetic'' binary property
        (``Alphabetic=Yes``).A short form starting with ``Is`` indicates a script
        or binary property:* ``IsLatin``, the ''Latin'' script (``Script=Latin``).*
        ``IsAlphabetic``, the ''Alphabetic'' binary property (``Alphabetic=Yes``).A
        short form starting with ``In`` indicates a block property:* ``InBasicLatin``,
        the ''BasicLatin'' block (``Block=BasicLatin``).POSIX character classes^^^^^^^^^^^^^^^^^^^^^^^``[[:alpha:]]``;
        ``[[:^alpha:]]``POSIX character classes are supported. These are normally
        treated as an alternative form of ``\p{...}``.The exceptions are ``alnum``,
        ``digit``, ``punct`` and ``xdigit``, whose definitions are different from
        those of Unicode.``[[:alnum:]]`` is equivalent to ``\p{posix_alnum}``.``[[:digit:]]``
        is equivalent to ``\p{posix_digit}``.``[[:punct:]]`` is equivalent to ``\p{posix_punct}``.``[[:xdigit:]]``
        is equivalent to ``\p{posix_xdigit}``.Search anchor ``\G``^^^^^^^^^^^^^^^^^^^^A
        search anchor has been added. It matches at the position where each search
        started/continued and can be used for contiguous matches or in negative variable-length
        lookbehinds to limit how far back the lookbehind goes:.. sourcecode:: python  >>>
        regex.findall(r"\w{2}", "abcd ef")  [''ab'', ''cd'', ''ef'']  >>> regex.findall(r"\G\w{2}",
        "abcd ef")  [''ab'', ''cd'']* The search starts at position 0 and matches
        ''ab''.* The search continues at position 2 and matches ''cd''.* The search
        continues at position 4 and fails to match any letters.* The anchor stops
        the search start position from being advanced, so there are no more results.Reverse
        searching^^^^^^^^^^^^^^^^^Searches can also work backwards:.. sourcecode::
        python  >>> regex.findall(r".", "abc")  [''a'', ''b'', ''c'']  >>> regex.findall(r"(?r).",
        "abc")  [''c'', ''b'', ''a'']Note that the result of a reverse search is not
        necessarily the reverse of a forward search:.. sourcecode:: python  >>> regex.findall(r"..",
        "abcde")  [''ab'', ''cd'']  >>> regex.findall(r"(?r)..", "abcde")  [''de'',
        ''bc'']Matching a single grapheme ``\X``^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^The
        grapheme matcher is supported. It conforms to the Unicode specification at
        ``http://www.unicode.org/reports/tr29/``.Branch reset ``(?|...|...)``^^^^^^^^^^^^^^^^^^^^^^^^^^^^Group
        numbers will be reused across the alternatives, but groups with different
        names will have different group numbers... sourcecode:: python  >>> regex.match(r"(?|(first)|(second))",
        "first").groups()  (''first'',)  >>> regex.match(r"(?|(first)|(second))",
        "second").groups()  (''second'',)Note that there is only one group.Default
        Unicode word boundary^^^^^^^^^^^^^^^^^^^^^^^^^^^^^The ``WORD`` flag changes
        the definition of a ''word boundary'' to that of a default Unicode word boundary.
        This applies to ``\b`` and ``\B``.Timeout^^^^^^^The matching methods and functions
        support timeouts. The timeout (in seconds) applies to the entire operation:..
        sourcecode:: python  >>> from time import sleep  >>>  >>> def fast_replace(m):  ...     return
        ''X''  ...  >>> def slow_replace(m):  ...     sleep(0.5)  ...     return ''X''  ...  >>>
        regex.sub(r''[a-z]'', fast_replace, ''abcde'', timeout=2)  ''XXXXX''  >>>
        regex.sub(r''[a-z]'', slow_replace, ''abcde'', timeout=2)  Traceback (most
        recent call last):    File "<stdin>", line 1, in <module>    File "C:\Python310\lib\site-packages\regex\regex.py",
        line 278, in sub      return pat.sub(repl, string, count, pos, endpos, concurrent,
        timeout)  TimeoutError: regex timed out'
      Package: regex
      Source: pip
      Version: 2023.12.25
      Hash: ''
      licenses:
      - Apache-2.0
      Title: regex
      DownloadURL: https://files.pythonhosted.org/packages/b5/39/31626e7e75b187fae7f121af3c538a991e725c744ac893cc2cfd70ce2853/regex-2023.12.25.tar.gz
  bazaar:
    register: 'no'
    prim: 22/CTX1027508
    community_link: https://pypi.org/project/regex/
    community_name: https://pypi.org/project/regex/
    community_url: https://pypi.org/project/regex/
    component_comment: ''
    component_highlevel_description: ''
    component_name: regex
    component_platform: linux
    component_programing_language: ''
    component_version: 2023.12.25
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://files.pythonhosted.org/packages/b5/39/31626e7e75b187fae7f121af3c538a991e725c744ac893cc2cfd70ce2853/regex-2023.12.25.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078559&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: regex
    target_sw: linux
    vendor: pip
    version: 2023.12.25
    web_url: https://github.com/mrabarnett/mrab-regex
  licenses:
  - Apache-2.0
  name: regex
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 2023.12.25
  mimer:
    linking: Static
    product_number: CTX1027508
    product_version_label: 2023.12.25
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: requests+2.31.0
  additional_info:
    fossa-attribution:
      Description: "# Requests**Requests** is a simple, yet elegant, HTTP library.```python>>>
        import requests>>> r = requests.get('https://httpbin.org/basic-auth/user/pass',
        auth=('user', 'pass'))>>> r.status_code200>>> r.headers['content-type']'application/json;
        charset=utf8'>>> r.encoding'utf-8'>>> r.text'{\"authenticated\": true, ...'>>>
        r.json(){'authenticated': True, ...}```Requests allows you to send HTTP/1.1
        requests extremely easily. There\u2019s no need to manually add query strings
        to your URLs, or to form-encode your `PUT` & `POST` data \u2014 but nowadays,
        just use the `json` method!Requests is one of the most downloaded Python packages
        today, pulling in around `30M downloads / week`\u2014 according to GitHub,
        Requests is currently [depended upon](https://github.com/psf/requests/network/dependents?package_id=UGFja2FnZS01NzA4OTExNg%3D%3D)
        by `1,000,000+` repositories. You may certainly put your trust in this code.[![Downloads](https://pepy.tech/badge/requests/month)](https://pepy.tech/project/requests)[![Supported
        Versions](https://img.shields.io/pypi/pyversions/requests.svg)](https://pypi.org/project/requests)[![Contributors](https://img.shields.io/github/contributors/psf/requests.svg)](https://github.com/psf/requests/graphs/contributors)##
        Installing Requests and Supported VersionsRequests is available on PyPI:```console$
        python -m pip install requests```Requests officially supports Python 3.7+.##
        Supported Features & Best\u2013PracticesRequests is ready for the demands
        of building robust and reliable HTTP\u2013speaking applications, for the needs
        of today.- Keep-Alive & Connection Pooling- International Domains and URLs-
        Sessions with Cookie Persistence- Browser-style TLS/SSL Verification- Basic
        & Digest Authentication- Familiar `dict`\u2013like Cookies- Automatic Content
        Decompression and Decoding- Multi-part File Uploads- SOCKS Proxy Support-
        Connection Timeouts- Streaming Downloads- Automatic honoring of `.netrc`-
        Chunked HTTP Requests## API Reference and User Guide available on [Read the
        Docs](https://requests.readthedocs.io)[![Read the Docs](https://raw.githubusercontent.com/psf/requests/main/ext/ss.png)](https://requests.readthedocs.io)##
        Cloning the repositoryWhen cloning the Requests repository, you may need to
        add the `-cfetch.fsck.badTimezone=ignore` flag to avoid an error about a bad
        commit (see[this issue](https://github.com/psf/requests/issues/2690) for more
        background):```shellgit clone -c fetch.fsck.badTimezone=ignore https://github.com/psf/requests.git```You
        can also apply this setting to your global Git config:```shellgit config --global
        fetch.fsck.badTimezone ignore```---[![Kenneth Reitz](https://raw.githubusercontent.com/psf/requests/main/ext/kr.png)](https://kennethreitz.org)
        [![Python Software Foundation](https://raw.githubusercontent.com/psf/requests/main/ext/psf.png)](https://www.python.org/psf)"
      Package: requests
      Source: pip
      Version: 2.31.0
      Hash: ''
      licenses:
      - Apache-2.0
      Title: requests
      DownloadURL: https://files.pythonhosted.org/packages/9d/be/10918a2eac4ae9f02f6cfe6414b7a155ccd8f7f9d4380d62fd5b955065c3/requests-2.31.0.tar.gz
  bazaar:
    register: 'no'
    prim: 42/CAX1056548
    community_link: https://github.com/psf/requests
    community_name: https://github.com/psf/requests
    community_url: https://github.com/psf/requests
    component_comment: ''
    component_highlevel_description: Requests is the only *Non-GMO* HTTP library for
      Python, safe for human consumption.
    component_name: Requests, Python
    component_platform: linux
    component_programing_language: Python
    component_version: 2.31.0
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://files.pythonhosted.org/packages/9d/be/10918a2eac4ae9f02f6cfe6414b7a155ccd8f7f9d4380d62fd5b955065c3/requests-2.31.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1047952&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: 'NO'
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: requests
    target_sw: linux
    vendor: pip
    version: 2.31.0
    web_url: https://requests.readthedocs.io
  licenses:
  - Apache-2.0
  name: requests
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 2.31.0
  mimer:
    linking: Static
    product_number: CAX1056548
    product_version_label: 2.31.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: requests-oauthlib+1.3.1
  additional_info:
    fossa-attribution:
      Description: 'Requests-OAuthlib |build-status| |coverage-status| |docs|=========================================================This
        project provides first-class OAuth library support for `Requests <https://requests.readthedocs.io>`_.The
        OAuth 1 workflow--------------------OAuth 1 can seem overly complicated and
        it sure has its quirks. Luckily,requests_oauthlib hides most of these and
        let you focus at the task at hand.Accessing protected resources using requests_oauthlib
        is as simple as:.. code-block:: pycon    >>> from requests_oauthlib import
        OAuth1Session    >>> twitter = OAuth1Session(''client_key'',                                client_secret=''client_secret'',                                resource_owner_key=''resource_owner_key'',                                resource_owner_secret=''resource_owner_secret'')    >>>
        url = ''https://api.twitter.com/1/account/settings.json''    >>> r = twitter.get(url)Before
        accessing resources you will need to obtain a few credentials from yourprovider
        (e.g. Twitter) and authorization from the user for whom you wish toretrieve
        resources for. You can read all about this in the full`OAuth 1 workflow guide
        on RTD <https://requests-oauthlib.readthedocs.io/en/latest/oauth1_workflow.html>`_.The
        OAuth 2 workflow--------------------OAuth 2 is generally simpler than OAuth
        1 but comes in more flavours. The mostcommon being the Authorization Code
        Grant, also known as the WebApplicationflow.Fetching a protected resource
        after obtaining an access token can be extremelysimple. However, before accessing
        resources you will need to obtain a fewcredentials from your provider (e.g.
        Google) and authorization from the userfor whom you wish to retrieve resources
        for. You can read all about this in thefull `OAuth 2 workflow guide on RTD
        <https://requests-oauthlib.readthedocs.io/en/latest/oauth2_workflow.html>`_.Installation-------------To
        install requests and requests_oauthlib you can use pip:.. code-block:: bash    pip
        install requests requests-oauthlib.. |build-status| image:: https://github.com/requests/requests-oauthlib/actions/workflows/run-tests.yml/badge.svg   :target:
        https://github.com/requests/requests-oauthlib/actions.. |coverage-status|
        image:: https://img.shields.io/coveralls/requests/requests-oauthlib.svg   :target:
        https://coveralls.io/r/requests/requests-oauthlib.. |docs| image:: https://readthedocs.org/projects/requests-oauthlib/badge/   :alt:
        Documentation Status   :scale: 100%   :target: https://requests-oauthlib.readthedocs.io/History-------v2.0.0
        (22 March 2024)++++++++++++++++++++++++Full set of changes are in [github](https://github.com/requests/requests-oauthlib/milestone/4?closed=1).Additions
        & changes:- ``OAuth2Session`` now correctly uses the ``self.verify`` value
        if ``verify``  is not overridden in ``fetch_token`` and ``refresh_token``.
        Fixes `#404  <https://github.com/requests/requests-oauthlib/issues/404>`_.-
        ``OAuth2Session`` constructor now uses its ``client.scope`` when a ``client``  is
        provided and ``scope`` is not overridden. Fixes `#408  <https://github.com/requests/requests-oauthlib/issues/408>`_-
        Add ``refresh_token_request`` and ``access_token_request`` compliance hooks-
        Add PKCE support and Auth0 example- Add support for Python 3.8-3.12- Remove
        support of Python 2.x, <3.7- Migrated to Github Action- Updated dependencies-
        Cleanup some docs and examplesv1.4.0 (27 Feb 2024)++++++++++++++++++++++++-
        Version 2.0.0 published initially as 1.4.0, it was yanked eventually.v1.3.1
        (21 January 2022)++++++++++++++++++++++++- Add initial support for OAuth Mutual
        TLS (draft-ietf-oauth-mtls)- Removed outdated LinkedIn Compliance Fixes- Add
        eBay compliance fix- Add Spotify OAuth 2 Tutorial- Add support for python
        3.8, 3.9- Fixed LinkedIn Compliance Fixes- Fixed ReadTheDocs Documentation
        and sphinx errors- Moved pipeline to GitHub Actionsv1.3.0 (6 November 2019)++++++++++++++++++++++++-
        Instagram compliance fix- Added ``force_querystring`` argument to fetch_token()
        method on OAuth2Sessionv1.2.0 (14 January 2019)++++++++++++++++++++++++- This
        project now depends on OAuthlib 3.0.0 and above. It does **not** support  versions
        of OAuthlib before 3.0.0.- Updated oauth2 tests to use ''sess'' for an OAuth2Session
        instance instead of `auth`  because OAuth2Session objects and methods acceept
        an `auth` paramether which is  typically an instance of `requests.auth.HTTPBasicAuth`-
        `OAuth2Session.fetch_token` previously tried to guess how and where to provide  "client"
        and "user" credentials incorrectly. This was incompatible with some  OAuth
        servers and incompatible with breaking changes in oauthlib that seek to  correctly
        provide the `client_id`. The older implementation also did not raise  the
        correct exceptions when username and password are not present on Legacy  clients.-
        Avoid automatic netrc authentication for OAuth2Session.v1.1.0 (9 January 2019)+++++++++++++++++++++++-
        Adjusted version specifier for ``oauthlib`` dependency: this project is  not
        yet compatible with ``oauthlib`` 3.0.0.- Dropped dependency on ``nose``.-
        Minor changes to clean up the code and make it more readable/maintainable.v1.0.0
        (4 June 2018)++++++++++++++++++++- **Removed support for Python 2.6 and Python
        3.3.**  This project now supports Python 2.7, and Python 3.4 and above.- Added
        several examples to the documentation.- Added plentymarkets compliance fix.-
        Added a ``token`` property to OAuth1Session, to match the corresponding  ``token``
        property on OAuth2Session.v0.8.0 (14 February 2017)+++++++++++++++++++++++++-
        Added Fitbit compliance fix.- Fixed an issue where newlines in the response
        body for the access token  request would cause errors when trying to extract
        the token.- Fixed an issue introduced in v0.7.0 where users passing ``auth``
        to several  methods would encounter conflicts with the ``client_id`` and  ``client_secret``-derived
        auth. The user-supplied ``auth`` argument is now  used in preference to those
        options.v0.7.0 (22 September 2016)++++++++++++++++++++++++++- Allowed ``OAuth2Session.request``
        to take the ``client_id`` and  ``client_secret`` parameters for the purposes
        of automatic token refresh,  which may need them.v0.6.2 (12 July 2016)+++++++++++++++++++++-
        Use ``client_id`` and ``client_secret`` for the Authorization header if  provided.-
        Allow explicit bypass of the Authorization header by setting ``auth=False``.-
        Pass through the ``proxies`` kwarg when refreshing tokens.- Miscellaneous
        cleanups.v0.6.1 (19 February 2016)+++++++++++++++++++++++++- Fixed a bug when
        sending authorization in headers with no username and  password present.-
        Make sure we clear the session token before obtaining a new one.- Some improvements
        to the Slack compliance fix.- Avoid timing problems around token refresh.-
        Allow passing arbitrary arguments to requests when calling  ``fetch_request_token``
        and ``fetch_access_token``.v0.6.0 (14 December 2015)+++++++++++++++++++++++++-
        Add compliance fix for Slack.- Add compliance fix for Mailchimp.- ``TokenRequestDenied``
        exceptions now carry the entire response, not just the  status code.- Pass
        through keyword arguments when refreshing tokens automatically.- Send authorization
        in headers, not just body, to maximize compatibility.- More getters/setters
        available for OAuth2 session client values.- Allow sending custom headers
        when refreshing tokens, and set some defaults.v0.5.0 (4 May 2015)+++++++++++++++++++-
        Fix ``TypeError`` being raised instead of ``TokenMissing`` error.- Raise requests
        exceptions on 4XX and 5XX responses in the OAuth2 flow.- Avoid ``AttributeError``
        when initializing the ``OAuth2Session`` class  without complete client information.v0.4.2
        (16 October 2014)++++++++++++++++++++++++- New ``authorized`` property on
        OAuth1Session and OAuth2Session, which allows  you to easily determine if
        the session is already authorized with OAuth tokens  or not.- New ``TokenMissing``
        and ``VerifierMissing`` exception classes for OAuth1Session:  this will make
        it easier to catch and identify these exceptions.v0.4.1 (6 June 2014)++++++++++++++++++++-
        New install target ``[rsa]`` for people using OAuth1 RSA-SHA1 signature  method.-
        Fixed bug in OAuth2 where supplied state param was not used in auth url.-
        OAuth2 HTTPS checking can be disabled by setting environment variable  ``OAUTHLIB_INSECURE_TRANSPORT``.-
        OAuth1 now re-authorize upon redirects.- OAuth1 token fetching now raise a
        detailed error message when the  response body is incorrectly encoded or the
        request was denied.- Added support for custom OAuth1 clients.- OAuth2 compliance
        fix for Sina Weibo.- Multiple fixes to facebook compliance fix.- Compliance
        fixes now re-encode body properly as bytes in Python 3.- Logging now properly
        done under ``requests_oauthlib`` namespace instead  of piggybacking on oauthlib
        namespace.- Logging introduced for OAuth1 auth and session.v0.4.0 (29 September
        2013)++++++++++++++++++++++++++- OAuth1Session methods only return unicode
        strings. #55.- Renamed requests_oauthlib.core to requests_oauthlib.oauth1_auth
        for consistency. #79.- Added Facebook compliance fix and access_token_response
        hook to OAuth2Session. #63.- Added LinkedIn compliance fix.- Added refresh_token_response
        compliance hook, invoked before parsing the refresh token.- Correctly limit
        compliance hooks to running only once!- Content type guessing should only
        be done when no content type is given- OAuth1 now updates r.headers instead
        of replacing it with non case insensitive dict- Remove last use of Response.content
        (in OAuth1Session). #44.- State param can now be supplied in OAuth2Session.authorize_url'
      Package: requests-oauthlib
      Source: pip
      Version: 1.3.1
      Hash: ''
      licenses:
      - ISC
      Title: requests-oauthlib
      DownloadURL: https://files.pythonhosted.org/packages/95/52/531ef197b426646f26b53815a7d2a67cb7a331ef098bb276db26a68ac49f/requests-oauthlib-1.3.1.tar.gz
  bazaar:
    register: 'no'
    prim: 4/CTX1025086
    community_link: https://github.com/requests/requests-oauthlib
    community_name: https://github.com/requests/requests-oauthlib
    community_url: https://github.com/requests/requests-oauthlib
    component_comment: ''
    component_highlevel_description: OAuthlib support for Python-Requests! This project
      provides first-class OAuth library support for `Requests <http://python-requests.org>'.
    component_name: Requests-OAuthlib
    component_platform: linux
    component_programing_language: Python
    component_version: 1.3.1
    licenses:
    - FAL1159116 (ISC License (ISC))
    src_download_link: https://github.com/requests/requests-oauthlib/archive/v1.3.1.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1005313&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: N/A
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: requests-oauthlib
    target_sw: linux
    vendor: pip
    version: 1.3.1
    web_url: https://github.com/requests/requests-oauthlib
  licenses:
  - ISC
  name: requests-oauthlib
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 1.3.1
  mimer:
    linking: Static
    product_number: CTX1025086
    product_version_label: v1.3.1
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: responses+0.18.0
  additional_info:
    fossa-attribution:
      Description: 'Responses=========.. image:: https://img.shields.io/pypi/v/responses.svg    :target:
        https://pypi.python.org/pypi/responses/.. image:: https://img.shields.io/pypi/pyversions/responses.svg    :target:
        https://pypi.org/project/responses/.. image:: https://img.shields.io/pypi/dm/responses   :target:
        https://pypi.python.org/pypi/responses/.. image:: https://codecov.io/gh/getsentry/responses/branch/master/graph/badge.svg    :target:
        https://codecov.io/gh/getsentry/responses/A utility library for mocking out
        the ``requests`` Python library...  note::    Responses requires Python 3.8
        or newer, and requests >= 2.30.0Table of Contents-----------------.. contents::Installing----------``pip
        install responses``Deprecations and Migration Path-------------------------------Here
        you will find a list of deprecated functionality and a migration path for
        each.Please ensure to update your code according to the guidance... list-table::
        Deprecation and Migration   :widths: 50 25 50   :header-rows: 1   * - Deprecated
        Functionality     - Deprecated in Version     - Migration Path   * - ``responses.json_params_matcher``     -
        0.14.0     - ``responses.matchers.json_params_matcher``   * - ``responses.urlencoded_params_matcher``     -
        0.14.0     - ``responses.matchers.urlencoded_params_matcher``   * - ``stream``
        argument in ``Response`` and ``CallbackResponse``     - 0.15.0     - Use ``stream``
        argument in request directly.   * - ``match_querystring`` argument in ``Response``
        and ``CallbackResponse``.     - 0.17.0     - Use ``responses.matchers.query_param_matcher``
        or ``responses.matchers.query_string_matcher``   * - ``responses.assert_all_requests_are_fired``,
        ``responses.passthru_prefixes``, ``responses.target``     - 0.20.0     - Use
        ``responses.mock.assert_all_requests_are_fired``,       ``responses.mock.passthru_prefixes``,
        ``responses.mock.target`` instead.BETA Features-------------Below you can
        find a list of BETA features. Although we will try to keep the API backwards
        compatiblewith released version, we reserve the right to change these APIs
        before they are considered stable. Please share your feedback via`GitHub Issues
        <https://github.com/getsentry/responses/issues>`_.Record Responses to files^^^^^^^^^^^^^^^^^^^^^^^^^You
        can perform real requests to the server and ``responses`` will automatically
        record the output to thefile. Recorded data is stored in `YAML <https://yaml.org>`_
        format.Apply ``@responses._recorder.record(file_path="out.yaml")`` decorator
        to any function where you performrequests to record responses to ``out.yaml``
        file.Following code.. code-block:: python    import requests    from responses
        import _recorder    def another():        rsp = requests.get("https://httpstat.us/500")        rsp
        = requests.get("https://httpstat.us/202")    @_recorder.record(file_path="out.yaml")    def
        test_recorder():        rsp = requests.get("https://httpstat.us/404")        rsp
        = requests.get("https://httpbin.org/status/wrong")        another()will produce
        next output:.. code-block:: yaml    responses:    - response:        auto_calculate_content_length:
        false        body: 404 Not Found        content_type: text/plain        method:
        GET        status: 404        url: https://httpstat.us/404    - response:        auto_calculate_content_length:
        false        body: Invalid status code        content_type: text/plain        method:
        GET        status: 400        url: https://httpbin.org/status/wrong    - response:        auto_calculate_content_length:
        false        body: 500 Internal Server Error        content_type: text/plain        method:
        GET        status: 500        url: https://httpstat.us/500    - response:        auto_calculate_content_length:
        false        body: 202 Accepted        content_type: text/plain        method:
        GET        status: 202        url: https://httpstat.us/202Replay responses
        (populate registry) from files^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^You
        can populate your active registry from a ``yaml`` file with recorded responses.(See
        `Record Responses to files`_ to understand how to obtain a file).To do that
        you need to execute ``responses._add_from_file(file_path="out.yaml")`` withinan
        activated decorator or a context manager.The following code example registers
        a ``patch`` response, then all responses present in``out.yaml`` file and a
        ``post`` response at the end... code-block:: python    import responses    @responses.activate    def
        run():        responses.patch("http://httpbin.org")        responses._add_from_file(file_path="out.yaml")        responses.post("http://httpbin.org/form")    run()Basics------The
        core of ``responses`` comes from registering mock responses and covering test
        functionwith ``responses.activate`` decorator. ``responses`` provides similar
        interface as ``requests``.Main Interface^^^^^^^^^^^^^^* responses.add(``Response``
        or ``Response args``) - allows either to register ``Response`` object or directly  provide
        arguments of ``Response`` object. See `Response Parameters`_.. code-block::
        python    import responses    import requests    @responses.activate    def
        test_simple():        # Register via ''Response'' object        rsp1 = responses.Response(            method="PUT",            url="http://example.com",        )        responses.add(rsp1)        #
        register via direct arguments        responses.add(            responses.GET,            "http://twitter.com/api/1/foobar",            json={"error":
        "not found"},            status=404,        )        resp = requests.get("http://twitter.com/api/1/foobar")        resp2
        = requests.put("http://example.com")        assert resp.json() == {"error":
        "not found"}        assert resp.status_code == 404        assert resp2.status_code
        == 200        assert resp2.request.method == "PUT"If you attempt to fetch
        a url which doesn''t hit a match, ``responses`` will raisea ``ConnectionError``:..
        code-block:: python    import responses    import requests    from requests.exceptions
        import ConnectionError    @responses.activate    def test_simple():        with
        pytest.raises(ConnectionError):            requests.get("http://twitter.com/api/1/foobar")Shortcuts^^^^^^^^^Shortcuts
        provide a shorten version of ``responses.add()`` where method argument is
        prefilled* responses.delete(``Response args``) - register DELETE response*
        responses.get(``Response args``) - register GET response* responses.head(``Response
        args``) - register HEAD response* responses.options(``Response args``) - register
        OPTIONS response* responses.patch(``Response args``) - register PATCH response*
        responses.post(``Response args``) - register POST response* responses.put(``Response
        args``) - register PUT response.. code-block:: python    import responses    import
        requests    @responses.activate    def test_simple():        responses.get(            "http://twitter.com/api/1/foobar",            json={"type":
        "get"},        )        responses.post(            "http://twitter.com/api/1/foobar",            json={"type":
        "post"},        )        responses.patch(            "http://twitter.com/api/1/foobar",            json={"type":
        "patch"},        )        resp_get = requests.get("http://twitter.com/api/1/foobar")        resp_post
        = requests.post("http://twitter.com/api/1/foobar")        resp_patch = requests.patch("http://twitter.com/api/1/foobar")        assert
        resp_get.json() == {"type": "get"}        assert resp_post.json() == {"type":
        "post"}        assert resp_patch.json() == {"type": "patch"}Responses as a
        context manager^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Instead of wrapping the whole
        function with decorator you can use a context manager... code-block:: python    import
        responses    import requests    def test_my_api():        with responses.RequestsMock()
        as rsps:            rsps.add(                responses.GET,                "http://twitter.com/api/1/foobar",                body="{}",                status=200,                content_type="application/json",            )            resp
        = requests.get("http://twitter.com/api/1/foobar")            assert resp.status_code
        == 200        # outside the context manager requests will hit the remote server        resp
        = requests.get("http://twitter.com/api/1/foobar")        resp.status_code
        == 404Response Parameters-------------------The following attributes can be
        passed to a Response mock:method (``str``)    The HTTP method (GET, POST,
        etc).url (``str`` or ``compiled regular expression``)    The full resource
        URL.match_querystring (``bool``)    DEPRECATED: Use ``responses.matchers.query_param_matcher``
        or    ``responses.matchers.query_string_matcher``    Include the query string
        when matching requests.    Enabled by default if the response URL contains
        a query string,    disabled if it doesn''t or the URL is a regular expression.body
        (``str`` or ``BufferedReader`` or ``Exception``)    The response body. Read
        more `Exception as Response body`_json    A Python object representing the
        JSON response body. Automatically configures    the appropriate Content-Type.status
        (``int``)    The HTTP status code.content_type (``content_type``)    Defaults
        to ``text/plain``.headers (``dict``)    Response headers.stream (``bool``)    DEPRECATED:
        use ``stream`` argument in request directlyauto_calculate_content_length (``bool``)    Disabled
        by default. Automatically calculates the length of a supplied string or JSON
        body.match (``tuple``)    An iterable (``tuple`` is recommended) of callbacks
        to match requests    based on request attributes.    Current module provides
        multiple matchers that you can use to match:    * body contents in JSON format    *
        body contents in URL encoded data format    * request query parameters    *
        request query string (similar to query parameters but takes string as input)    *
        kwargs provided to request e.g. ``stream``, ``verify``    * ''multipart/form-data''
        content and headers in request    * request headers    * request fragment
        identifier    Alternatively user can create custom matcher.    Read more `Matching
        Requests`_Exception as Response body--------------------------You can pass
        an ``Exception`` as the body to trigger an error on the request:.. code-block::
        python    import responses    import requests    @responses.activate    def
        test_simple():        responses.get("http://twitter.com/api/1/foobar", body=Exception("..."))        with
        pytest.raises(Exception):            requests.get("http://twitter.com/api/1/foobar")Matching
        Requests-----------------Matching Request Body Contents^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^When
        adding responses for endpoints that are sent request data you can addmatchers
        to ensure your code is sending the right parameters and providedifferent responses
        based on the request body contents. ``responses`` providesmatchers for JSON
        and URL-encoded request bodies.URL-encoded data"""""""""""""""".. code-block::
        python    import responses    import requests    from responses import matchers    @responses.activate    def
        test_calc_api():        responses.post(            url="http://calc.com/sum",            body="4",            match=[matchers.urlencoded_params_matcher({"left":
        "1", "right": "3"})],        )        requests.post("http://calc.com/sum",
        data={"left": 1, "right": 3})JSON encoded data"""""""""""""""""Matching JSON
        encoded data can be done with ``matchers.json_params_matcher()``... code-block::
        python    import responses    import requests    from responses import matchers    @responses.activate    def
        test_calc_api():        responses.post(            url="http://example.com/",            body="one",            match=[                matchers.json_params_matcher({"page":
        {"name": "first", "type": "json"}})            ],        )        resp = requests.request(            "POST",            "http://example.com/",            headers={"Content-Type":
        "application/json"},            json={"page": {"name": "first", "type": "json"}},        )Query
        Parameters Matcher^^^^^^^^^^^^^^^^^^^^^^^^Query Parameters as a Dictionary""""""""""""""""""""""""""""""""You
        can use the ``matchers.query_param_matcher`` function to matchagainst the
        ``params`` request parameter. Just use the same dictionary as youwill use
        in ``params`` argument in ``request``.Note, do not use query parameters as
        part of the URL. Avoid using ``match_querystring``deprecated argument... code-block::
        python    import responses    import requests    from responses import matchers    @responses.activate    def
        test_calc_api():        url = "http://example.com/test"        params = {"hello":
        "world", "I am": "a big test"}        responses.get(            url=url,            body="test",            match=[matchers.query_param_matcher(params)],        )        resp
        = requests.get(url, params=params)        constructed_url = r"http://example.com/test?I+am=a+big+test&hello=world"        assert
        resp.url == constructed_url        assert resp.request.url == constructed_url        assert
        resp.request.params == paramsBy default, matcher will validate that all parameters
        match strictly.To validate that only parameters specified in the matcher are
        present in original requestuse ``strict_match=False``.Query Parameters as
        a String""""""""""""""""""""""""""""As alternative, you can use query string
        value in ``matchers.query_string_matcher`` to matchquery parameters in your
        request.. code-block:: python    import requests    import responses    from
        responses import matchers    @responses.activate    def my_func():        responses.get(            "https://httpbin.org/get",            match=[matchers.query_string_matcher("didi=pro&test=1")],        )        resp
        = requests.get("https://httpbin.org/get", params={"test": 1, "didi": "pro"})    my_func()Request
        Keyword Arguments Matcher^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^To validate request
        arguments use the ``matchers.request_kwargs_matcher`` function to matchagainst
        the request kwargs.Only following arguments are supported: ``timeout``, ``verify``,
        ``proxies``, ``stream``, ``cert``.Note, only arguments provided to ``matchers.request_kwargs_matcher``
        will be validated... code-block:: python    import responses    import requests    from
        responses import matchers    with responses.RequestsMock(assert_all_requests_are_fired=False)
        as rsps:        req_kwargs = {            "stream": True,            "verify":
        False,        }        rsps.add(            "GET",            "http://111.com",            match=[matchers.request_kwargs_matcher(req_kwargs)],        )        requests.get("http://111.com",
        stream=True)        # >>>  Arguments don''t match: {stream: True, verify:
        True} doesn''t match {stream: True, verify: False}Request multipart/form-data
        Data Validation^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^To validate request
        body and headers for ``multipart/form-data`` data you can use``matchers.multipart_matcher``.
        The ``data``, and ``files`` parameters provided will be comparedto the request:..
        code-block:: python    import requests    import responses    from responses.matchers
        import multipart_matcher    @responses.activate    def my_func():        req_data
        = {"some": "other", "data": "fields"}        req_files = {"file_name": b"Old
        World!"}        responses.post(            url="http://httpbin.org/post",            match=[multipart_matcher(req_files,
        data=req_data)],        )        resp = requests.post("http://httpbin.org/post",
        files={"file_name": b"New World!"})    my_func()    # >>> raises ConnectionError:
        multipart/form-data doesn''t match. Request body differs.Request Fragment
        Identifier Validation^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^To validate request
        URL fragment identifier you can use ``matchers.fragment_identifier_matcher``.The
        matcher takes fragment string (everything after ``#`` sign) as input for comparison:..
        code-block:: python    import requests    import responses    from responses.matchers
        import fragment_identifier_matcher    @responses.activate    def run():        url
        = "http://example.com?ab=xy&zed=qwe#test=1&foo=bar"        responses.get(            url,            match=[fragment_identifier_matcher("test=1&foo=bar")],            body=b"test",        )        #
        two requests to check reversed order of fragment identifier        resp =
        requests.get("http://example.com?ab=xy&zed=qwe#test=1&foo=bar")        resp
        = requests.get("http://example.com?zed=qwe&ab=xy#foo=bar&test=1")    run()Request
        Headers Validation^^^^^^^^^^^^^^^^^^^^^^^^^^When adding responses you can
        specify matchers to ensure that your code issending the right headers and
        provide different responses based on the requestheaders... code-block:: python    import
        responses    import requests    from responses import matchers    @responses.activate    def
        test_content_type():        responses.get(            url="http://example.com/",            body="hello
        world",            match=[matchers.header_matcher({"Accept": "text/plain"})],        )        responses.get(            url="http://example.com/",            json={"content":
        "hello world"},            match=[matchers.header_matcher({"Accept": "application/json"})],        )        #
        request in reverse order to how they were added!        resp = requests.get("http://example.com/",
        headers={"Accept": "application/json"})        assert resp.json() == {"content":
        "hello world"}        resp = requests.get("http://example.com/", headers={"Accept":
        "text/plain"})        assert resp.text == "hello world"Because ``requests``
        will send several standard headers in addition to what wasspecified by your
        code, request headers that are additional to the onespassed to the matcher
        are ignored by default. You can change this behaviour bypassing ``strict_match=True``
        to the matcher to ensure that only the headersthat you''re expecting are sent
        and no others. Note that you will probably haveto use a ``PreparedRequest``
        in your code to ensure that ``requests`` doesn''tinclude any additional headers...
        code-block:: python    import responses    import requests    from responses
        import matchers    @responses.activate    def test_content_type():        responses.get(            url="http://example.com/",            body="hello
        world",            match=[matchers.header_matcher({"Accept": "text/plain"},
        strict_match=True)],        )        # this will fail because requests adds
        its own headers        with pytest.raises(ConnectionError):            requests.get("http://example.com/",
        headers={"Accept": "text/plain"})        # a prepared request where you overwrite
        the headers before sending will work        session = requests.Session()        prepped
        = session.prepare_request(            requests.Request(                method="GET",                url="http://example.com/",            )        )        prepped.headers
        = {"Accept": "text/plain"}        resp = session.send(prepped)        assert
        resp.text == "hello world"Creating Custom Matcher^^^^^^^^^^^^^^^^^^^^^^^If
        your application requires other encodings or different data validation you
        can buildyour own matcher that returns ``Tuple[matches: bool, reason: str]``.Where
        boolean represents ``True`` or ``False`` if the request parameters match andthe
        string is a reason in case of match failure. Your matcher canexpect a ``PreparedRequest``
        parameter to be provided by ``responses``.Note, ``PreparedRequest`` is customized
        and has additional attributes ``params`` and ``req_kwargs``.Response Registry---------------------------Default
        Registry^^^^^^^^^^^^^^^^By default, ``responses`` will search all registered
        ``Response`` objects andreturn a match. If only one ``Response`` is registered,
        the registry is kept unchanged.However, if multiple matches are found for
        the same request, then first match is returned andremoved from registry.Ordered
        Registry^^^^^^^^^^^^^^^^In some scenarios it is important to preserve the
        order of the requests and responses.You can use ``registries.OrderedRegistry``
        to force all ``Response`` objects to be dependenton the insertion order and
        invocation index.In following example we add multiple ``Response`` objects
        that target the same URL. However,you can see, that status code will depend
        on the invocation order... code-block:: python    import requests    import
        responses    from responses.registries import OrderedRegistry    @responses.activate(registry=OrderedRegistry)    def
        test_invocation_index():        responses.get(            "http://twitter.com/api/1/foobar",            json={"msg":
        "not found"},            status=404,        )        responses.get(            "http://twitter.com/api/1/foobar",            json={"msg":
        "OK"},            status=200,        )        responses.get(            "http://twitter.com/api/1/foobar",            json={"msg":
        "OK"},            status=200,        )        responses.get(            "http://twitter.com/api/1/foobar",            json={"msg":
        "not found"},            status=404,        )        resp = requests.get("http://twitter.com/api/1/foobar")        assert
        resp.status_code == 404        resp = requests.get("http://twitter.com/api/1/foobar")        assert
        resp.status_code == 200        resp = requests.get("http://twitter.com/api/1/foobar")        assert
        resp.status_code == 200        resp = requests.get("http://twitter.com/api/1/foobar")        assert
        resp.status_code == 404Custom Registry^^^^^^^^^^^^^^^Built-in ``registries``
        are suitable for most of use cases, but to handle special conditions, you
        canimplement custom registry which must follow interface of ``registries.FirstMatchRegistry``.Redefining
        the ``find`` method will allow you to create custom search logic and returnappropriate
        ``Response``Example that shows how to set custom registry.. code-block:: python    import
        responses    from responses import registries    class CustomRegistry(registries.FirstMatchRegistry):        pass    print("Before
        tests:", responses.mock.get_registry())    """ Before tests: <responses.registries.FirstMatchRegistry
        object> """    # using function decorator    @responses.activate(registry=CustomRegistry)    def
        run():        print("Within test:", responses.mock.get_registry())        """
        Within test: <__main__.CustomRegistry object> """    run()    print("After
        test:", responses.mock.get_registry())    """ After test: <responses.registries.FirstMatchRegistry
        object> """    # using context manager    with responses.RequestsMock(registry=CustomRegistry)
        as rsps:        print("In context manager:", rsps.get_registry())        """
        In context manager: <__main__.CustomRegistry object> """    print("After exit
        from context manager:", responses.mock.get_registry())    """    After exit
        from context manager: <responses.registries.FirstMatchRegistry object>    """Dynamic
        Responses-----------------You can utilize callbacks to provide dynamic responses.
        The callback must returna tuple of (``status``, ``headers``, ``body``)...
        code-block:: python    import json    import responses    import requests    @responses.activate    def
        test_calc_api():        def request_callback(request):            payload
        = json.loads(request.body)            resp_body = {"value": sum(payload["numbers"])}            headers
        = {"request-id": "728d329e-0e86-11e4-a748-0c84dc037c13"}            return
        (200, headers, json.dumps(resp_body))        responses.add_callback(            responses.POST,            "http://calc.com/sum",            callback=request_callback,            content_type="application/json",        )        resp
        = requests.post(            "http://calc.com/sum",            json.dumps({"numbers":
        [1, 2, 3]}),            headers={"content-type": "application/json"},        )        assert
        resp.json() == {"value": 6}        assert len(responses.calls) == 1        assert
        responses.calls[0].request.url == "http://calc.com/sum"        assert responses.calls[0].response.text
        == ''{"value": 6}''        assert (            responses.calls[0].response.headers["request-id"]            ==
        "728d329e-0e86-11e4-a748-0c84dc037c13"        )You can also pass a compiled
        regex to ``add_callback`` to match multiple urls:.. code-block:: python    import
        re, json    from functools import reduce    import responses    import requests    operators
        = {        "sum": lambda x, y: x + y,        "prod": lambda x, y: x * y,        "pow":
        lambda x, y: x**y,    }    @responses.activate    def test_regex_url():        def
        request_callback(request):            payload = json.loads(request.body)            operator_name
        = request.path_url[1:]            operator = operators[operator_name]            resp_body
        = {"value": reduce(operator, payload["numbers"])}            headers = {"request-id":
        "728d329e-0e86-11e4-a748-0c84dc037c13"}            return (200, headers, json.dumps(resp_body))        responses.add_callback(            responses.POST,            re.compile("http://calc.com/(sum|prod|pow|unsupported)"),            callback=request_callback,            content_type="application/json",        )        resp
        = requests.post(            "http://calc.com/prod",            json.dumps({"numbers":
        [2, 3, 4]}),            headers={"content-type": "application/json"},        )        assert
        resp.json() == {"value": 24}    test_regex_url()If you want to pass extra
        keyword arguments to the callback function, for example when reusinga callback
        function to give a slightly different result, you can use ``functools.partial``:..
        code-block:: python    from functools import partial    def request_callback(request,
        id=None):        payload = json.loads(request.body)        resp_body = {"value":
        sum(payload["numbers"])}        headers = {"request-id": id}        return
        (200, headers, json.dumps(resp_body))    responses.add_callback(        responses.POST,        "http://calc.com/sum",        callback=partial(request_callback,
        id="728d329e-0e86-11e4-a748-0c84dc037c13"),        content_type="application/json",    )Integration
        with unit test frameworks-------------------------------------Responses as
        a ``pytest`` fixture^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^.. code-block:: python    @pytest.fixture    def
        mocked_responses():        with responses.RequestsMock() as rsps:            yield
        rsps    def test_api(mocked_responses):        mocked_responses.get(            "http://twitter.com/api/1/foobar",            body="{}",            status=200,            content_type="application/json",        )        resp
        = requests.get("http://twitter.com/api/1/foobar")        assert resp.status_code
        == 200Add default responses for each test^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^When
        run with ``unittest`` tests, this can be used to set up somegeneric class-level
        responses, that may be complemented by each test.Similar interface could be
        applied in ``pytest`` framework... code-block:: python    class TestMyApi(unittest.TestCase):        def
        setUp(self):            responses.get("https://example.com", body="within
        setup")            # here go other self.responses.add(...)        @responses.activate        def
        test_my_func(self):            responses.get(                "https://httpbin.org/get",                match=[matchers.query_param_matcher({"test":
        "1", "didi": "pro"})],                body="within test",            )            resp
        = requests.get("https://example.com")            resp2 = requests.get(                "https://httpbin.org/get",
        params={"test": "1", "didi": "pro"}            )            print(resp.text)            #
        >>> within setup            print(resp2.text)            # >>> within testRequestMock
        methods: start, stop, reset^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^``responses``
        has ``start``, ``stop``, ``reset`` methods very analogous to`unittest.mock.patch
        <https://docs.python.org/3/library/unittest.mock.html#patch-methods-start-and-stop>`_.These
        make it simpler to do requests mocking in ``setup`` methods or whereyou want
        to do multiple patches without nesting decorators or with statements... code-block::
        python    class TestUnitTestPatchSetup:        def setup(self):            """Creates
        ``RequestsMock`` instance and starts it."""            self.r_mock = responses.RequestsMock(assert_all_requests_are_fired=True)            self.r_mock.start()            #
        optionally some default responses could be registered            self.r_mock.get("https://example.com",
        status=505)            self.r_mock.put("https://example.com", status=506)        def
        teardown(self):            """Stops and resets RequestsMock instance.            If
        ``assert_all_requests_are_fired`` is set to ``True``, will raise an error            if
        some requests were not processed.            """            self.r_mock.stop()            self.r_mock.reset()        def
        test_function(self):            resp = requests.get("https://example.com")            assert
        resp.status_code == 505            resp = requests.put("https://example.com")            assert
        resp.status_code == 506Assertions on declared responses--------------------------------When
        used as a context manager, Responses will, by default, raise an assertionerror
        if a url was registered but not accessed. This can be disabled by passingthe
        ``assert_all_requests_are_fired`` value:.. code-block:: python    import responses    import
        requests    def test_my_api():        with responses.RequestsMock(assert_all_requests_are_fired=False)
        as rsps:            rsps.add(                responses.GET,                "http://twitter.com/api/1/foobar",                body="{}",                status=200,                content_type="application/json",            )Assert
        Request Call Count-------------------------Assert based on ``Response`` object^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Each
        ``Response`` object has ``call_count`` attribute that could be inspectedto
        check how many times each request was matched... code-block:: python    @responses.activate    def
        test_call_count_with_matcher():        rsp = responses.get(            "http://www.example.com",            match=(matchers.query_param_matcher({}),),        )        rsp2
        = responses.get(            "http://www.example.com",            match=(matchers.query_param_matcher({"hello":
        "world"}),),            status=777,        )        requests.get("http://www.example.com")        resp1
        = requests.get("http://www.example.com")        requests.get("http://www.example.com?hello=world")        resp2
        = requests.get("http://www.example.com?hello=world")        assert resp1.status_code
        == 200        assert resp2.status_code == 777        assert rsp.call_count
        == 2        assert rsp2.call_count == 2Assert based on the exact URL^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Assert
        that the request was called exactly n times... code-block:: python    import
        responses    import requests    @responses.activate    def test_assert_call_count():        responses.get("http://example.com")        requests.get("http://example.com")        assert
        responses.assert_call_count("http://example.com", 1) is True        requests.get("http://example.com")        with
        pytest.raises(AssertionError) as excinfo:            responses.assert_call_count("http://example.com",
        1)        assert (            "Expected URL ''http://example.com'' to be called
        1 times. Called 2 times."            in str(excinfo.value)        )    @responses.activate    def
        test_assert_call_count_always_match_qs():        responses.get("http://www.example.com")        requests.get("http://www.example.com")        requests.get("http://www.example.com?hello=world")        #
        One call on each url, querystring is matched by default        responses.assert_call_count("http://www.example.com",
        1) is True        responses.assert_call_count("http://www.example.com?hello=world",
        1) is TrueAssert Request Calls data-------------------------``Request`` object
        has ``calls`` list which elements correspond to ``Call`` objectsin the global
        list of ``Registry``. This can be useful when the order of requests is notguaranteed,
        but you need to check their correctness, for example in multithreadedapplications...
        code-block:: python    import concurrent.futures    import responses    import
        requests    @responses.activate    def test_assert_calls_on_resp():        rsp1
        = responses.patch("http://www.foo.bar/1/", status=200)        rsp2 = responses.patch("http://www.foo.bar/2/",
        status=400)        rsp3 = responses.patch("http://www.foo.bar/3/", status=200)        def
        update_user(uid, is_active):            url = f"http://www.foo.bar/{uid}/"            response
        = requests.patch(url, json={"is_active": is_active})            return response        with
        concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:            future_to_uid
        = {                executor.submit(update_user, uid, is_active): uid                for
        (uid, is_active) in [("3", True), ("2", True), ("1", False)]            }            for
        future in concurrent.futures.as_completed(future_to_uid):                uid
        = future_to_uid[future]                response = future.result()                print(f"{uid}
        updated with {response.status_code} status code")        assert len(responses.calls)
        == 3  # total calls count        assert rsp1.call_count == 1        assert
        rsp1.calls[0] in responses.calls        assert rsp1.calls[0].response.status_code
        == 200        assert json.loads(rsp1.calls[0].request.body) == {"is_active":
        False}        assert rsp2.call_count == 1        assert rsp2.calls[0] in responses.calls        assert
        rsp2.calls[0].response.status_code == 400        assert json.loads(rsp2.calls[0].request.body)
        == {"is_active": True}        assert rsp3.call_count == 1        assert rsp3.calls[0]
        in responses.calls        assert rsp3.calls[0].response.status_code == 200        assert
        json.loads(rsp3.calls[0].request.body) == {"is_active": True}Multiple Responses------------------You
        can also add multiple responses for the same url:.. code-block:: python    import
        responses    import requests    @responses.activate    def test_my_api():        responses.get("http://twitter.com/api/1/foobar",
        status=500)        responses.get(            "http://twitter.com/api/1/foobar",            body="{}",            status=200,            content_type="application/json",        )        resp
        = requests.get("http://twitter.com/api/1/foobar")        assert resp.status_code
        == 500        resp = requests.get("http://twitter.com/api/1/foobar")        assert
        resp.status_code == 200URL Redirection---------------In the following example
        you can see how to create a redirection chain and add custom exception that
        will be raisedin the execution chain and contain the history of redirects...  code-block::    A
        -> 301 redirect -> B    B -> 301 redirect -> C    C -> connection issue..
        code-block:: python    import pytest    import requests    import responses    @responses.activate    def
        test_redirect():        # create multiple Response objects where first two
        contain redirect headers        rsp1 = responses.Response(            responses.GET,            "http://example.com/1",            status=301,            headers={"Location":
        "http://example.com/2"},        )        rsp2 = responses.Response(            responses.GET,            "http://example.com/2",            status=301,            headers={"Location":
        "http://example.com/3"},        )        rsp3 = responses.Response(responses.GET,
        "http://example.com/3", status=200)        # register above generated Responses
        in ``response`` module        responses.add(rsp1)        responses.add(rsp2)        responses.add(rsp3)        #
        do the first request in order to generate genuine ``requests`` response        #
        this object will contain genuine attributes of the response, like ``history``        rsp
        = requests.get("http://example.com/1")        responses.calls.reset()        #
        customize exception with ``response`` attribute        my_error = requests.ConnectionError("custom
        error")        my_error.response = rsp        # update body of the 3rd response
        with Exception, this will be raised during execution        rsp3.body = my_error        with
        pytest.raises(requests.ConnectionError) as exc_info:            requests.get("http://example.com/1")        assert
        exc_info.value.args[0] == "custom error"        assert rsp1.url in exc_info.value.response.history[0].url        assert
        rsp2.url in exc_info.value.response.history[1].urlValidate ``Retry`` mechanism----------------------------If
        you are using the ``Retry`` features of ``urllib3`` and want to cover scenarios
        that test your retry limits, you can test those scenarios with ``responses``
        as well. The best approach will be to use an `Ordered Registry`_.. code-block::
        python    import requests    import responses    from responses import registries    from
        urllib3.util import Retry    @responses.activate(registry=registries.OrderedRegistry)    def
        test_max_retries():        url = "https://example.com"        rsp1 = responses.get(url,
        body="Error", status=500)        rsp2 = responses.get(url, body="Error", status=500)        rsp3
        = responses.get(url, body="Error", status=500)        rsp4 = responses.get(url,
        body="OK", status=200)        session = requests.Session()        adapter
        = requests.adapters.HTTPAdapter(            max_retries=Retry(                total=4,                backoff_factor=0.1,                status_forcelist=[500],                method_whitelist=["GET",
        "POST", "PATCH"],            )        )        session.mount("https://", adapter)        resp
        = session.get(url)        assert resp.status_code == 200        assert rsp1.call_count
        == 1        assert rsp2.call_count == 1        assert rsp3.call_count == 1        assert
        rsp4.call_count == 1Using a callback to modify the response---------------------------------------If
        you use customized processing in ``requests`` via subclassing/mixins, or if
        youhave library tools that interact with ``requests`` at a low level, you
        may needto add extended processing to the mocked Response object to fully
        simulate theenvironment for your tests.  A ``response_callback`` can be used,
        which will bewrapped by the library before being returned to the caller.  The
        callbackaccepts a ``response`` as it''s single argument, and is expected to
        return asingle ``response`` object... code-block:: python    import responses    import
        requests    def response_callback(resp):        resp.callback_processed =
        True        return resp    with responses.RequestsMock(response_callback=response_callback)
        as m:        m.add(responses.GET, "http://example.com", body=b"test")        resp
        = requests.get("http://example.com")        assert resp.text == "test"        assert
        hasattr(resp, "callback_processed")        assert resp.callback_processed
        is TruePassing through real requests-----------------------------In some cases
        you may wish to allow for certain requests to pass through responsesand hit
        a real server. This can be done with the ``add_passthru`` methods:.. code-block::
        python    import responses    @responses.activate    def test_my_api():        responses.add_passthru("https://percy.io")This
        will allow any requests matching that prefix, that is otherwise notregistered
        as a mock response, to passthru using the standard behavior.Pass through endpoints
        can be configured with regex patterns if youneed to allow an entire domain
        or path subtree to send requests:.. code-block:: python    responses.add_passthru(re.compile("https://percy.io/\\w+"))Lastly,
        you can use the ``passthrough`` argument of the ``Response`` objectto force
        a response to behave as a pass through... code-block:: python    # Enable
        passthrough for a single response    response = Response(        responses.GET,        "http://example.com",        body="not
        used",        passthrough=True,    )    responses.add(response)    # Use PassthroughResponse    response
        = PassthroughResponse(responses.GET, "http://example.com")    responses.add(response)Viewing/Modifying
        registered responses--------------------------------------Registered responses
        are available as a public method of the RequestMockinstance. It is sometimes
        useful for debugging purposes to view the stack ofregistered responses which
        can be accessed via ``responses.registered()``.The ``replace`` function allows
        a previously registered ``response`` to bechanged. The method signature is
        identical to ``add``. ``response`` s areidentified using ``method`` and ``url``.
        Only the first matched ``response`` isreplaced... code-block:: python    import
        responses    import requests    @responses.activate    def test_replace():        responses.get("http://example.org",
        json={"data": 1})        responses.replace(responses.GET, "http://example.org",
        json={"data": 2})        resp = requests.get("http://example.org")        assert
        resp.json() == {"data": 2}The ``upsert`` function allows a previously registered
        ``response`` to bechanged like ``replace``. If the response is registered,
        the ``upsert`` functionwill registered it like ``add``.``remove`` takes a
        ``method`` and ``url`` argument and will remove **all**matched responses from
        the registered list.Finally, ``reset`` will reset all registered responses.Coroutines
        and Multithreading-----------------------------``responses`` supports both
        Coroutines and Multithreading out of the box.Note, ``responses`` locks threading
        on ``RequestMock`` object allowing onlysingle thread to access it... code-block::
        python    async def test_async_calls():        @responses.activate        async
        def run():            responses.get(                "http://twitter.com/api/1/foobar",                json={"error":
        "not found"},                status=404,            )            resp = requests.get("http://twitter.com/api/1/foobar")            assert
        resp.json() == {"error": "not found"}            assert responses.calls[0].request.url
        == "http://twitter.com/api/1/foobar"        await run()Contributing------------Environment
        Configuration^^^^^^^^^^^^^^^^^^^^^^^^^Responses uses several linting and autoformatting
        utilities, so it''s important that whensubmitting patches you use the appropriate
        toolchain:Clone the repository:.. code-block:: shell    git clone https://github.com/getsentry/responses.gitCreate
        an environment (e.g. with ``virtualenv``):.. code-block:: shell    virtualenv
        .env && source .env/bin/activateConfigure development requirements:.. code-block::
        shell    make developTests and Code Quality Validation^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^The
        easiest way to validate your code is to run tests via ``tox``.Current ``tox``
        configuration runs the same checks that are used inGitHub Actions CI/CD pipeline.Please
        execute the following command line from the project root to validateyour code
        against:* Unit tests in all Python versions that are supported by this project*
        Type validation via ``mypy``* All ``pre-commit`` hooks.. code-block:: shell    toxAlternatively,
        you can always run a single test. See documentation below.Unit tests""""""""""Responses
        uses `Pytest <https://docs.pytest.org/en/latest/>`_ fortesting. You can run
        all tests by:.. code-block:: shell    tox -e py37    tox -e py310OR manually
        activate required version of Python and run.. code-block:: shell    pytestAnd
        run a single test by:.. code-block:: shell    pytest -k ''<test_function_name>''Type
        Validation"""""""""""""""To verify ``type`` compliance, run `mypy <https://github.com/python/mypy>`_
        linter:.. code-block:: shell    tox -e mypyOR.. code-block:: shell    mypy
        --config-file=./mypy.ini -p responsesCode Quality and Style""""""""""""""""""""""To
        check code style and reformat it run:.. code-block:: shell    tox -e precomOR..
        code-block:: shell    pre-commit run --all-files'
      Package: responses
      Source: pip
      Version: 0.18.0
      Hash: ''
      licenses:
      - Apache-2.0
      Title: responses
      DownloadURL: https://github.com/getsentry/responses/archive/refs/tags/0.18.0.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/getsentry/responses
    community_name: https://github.com/getsentry/responses
    community_url: https://github.com/getsentry/responses
    component_comment: ''
    component_highlevel_description: ''
    component_name: responses
    component_platform: linux
    component_programing_language: ''
    component_version: 0.18.0
    licenses: []
    src_download_link: https://github.com/getsentry/responses/archive/refs/tags/0.18.0.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: responses
    target_sw: linux
    vendor: pip
    version: 0.18.0
    web_url: https://github.com/getsentry/responses
  licenses:
  - Apache-2.0
  name: responses
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 0.18.0
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 0.18.0
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: rsa+4.9
  additional_info:
    fossa-attribution:
      Description: "# Pure Python RSA implementation[![PyPI](https://img.shields.io/pypi/v/rsa.svg)](https://pypi.org/project/rsa/)[![Build
        Status](https://travis-ci.org/sybrenstuvel/python-rsa.svg?branch=master)](https://travis-ci.org/sybrenstuvel/python-rsa)[![Coverage
        Status](https://coveralls.io/repos/github/sybrenstuvel/python-rsa/badge.svg?branch=master)](https://coveralls.io/github/sybrenstuvel/python-rsa?branch=master)[![Code
        Climate](https://api.codeclimate.com/v1/badges/a99a88d28ad37a79dbf6/maintainability)](https://codeclimate.com/github/codeclimate/codeclimate/maintainability)[Python-RSA](https://stuvel.eu/rsa)
        is a pure-Python RSA implementation. It supportsencryption and decryption,
        signing and verifying signatures, and keygeneration according to PKCS#1 version
        1.5. It can be used as a Pythonlibrary as well as on the commandline. The
        code was mostly written bySybren A.  St\xFCvel.Documentation can be found
        at the [Python-RSA homepage](https://stuvel.eu/rsa). For all changes, check
        [the changelog](https://github.com/sybrenstuvel/python-rsa/blob/master/CHANGELOG.md).Download
        and install using:    pip install rsaor download it from the [Python Package
        Index](https://pypi.org/project/rsa/).The source code is maintained at [GitHub](https://github.com/sybrenstuvel/python-rsa/)
        and islicensed under the [Apache License, version 2.0](https://www.apache.org/licenses/LICENSE-2.0)##
        SecurityBecause of how Python internally stores numbers, it is very hard (if
        not impossible) to make a pure-Python program secure against timing attacks.
        This library is no exception, so use it with care. See https://securitypitfalls.wordpress.com/2018/08/03/constant-time-compare-in-python/
        for more info.## Setup of Development Environment```python3 -m venv .venv.
        ./.venv/bin/activatepip install poetrypoetry install```## Publishing a New
        ReleaseSince this project is considered critical on the Python Package Index,two-factor
        authentication is required. For uploading packages to PyPi, an APIkey is required;
        username+password will not work.First, generate an API token at https://pypi.org/manage/account/token/.
        Then,use this token when publishing instead of your username and password.As
        username, use `__token__`.As password, use the token itself, including the
        `pypi-` prefix.See https://pypi.org/help/#apitoken for help using API tokens
        to publish. Thisis what I have in `~/.pypirc`:```[distutils]index-servers
        =    rsa# Use `twine upload -r rsa` to upload with this token.[rsa]  repository
        = https://upload.pypi.org/legacy/  username = __token__  password = pypi-token``````.
        ./.venv/bin/activatepip install twinepoetry buildtwine check dist/rsa-4.9.tar.gz
        dist/rsa-4.9-*.whltwine upload -r rsa dist/rsa-4.9.tar.gz dist/rsa-4.9-*.whl```The
        `pip install twine` is necessary as Python-RSA requires Python >= 3.6, andTwine
        requires at least version 3.7. This means Poetry refuses to add it asdependency."
      Package: rsa
      Source: pip
      Version: '4.9'
      Hash: ''
      licenses:
      - Apache-2.0
      Title: rsa
      DownloadURL: https://files.pythonhosted.org/packages/aa/65/7d973b89c4d2351d7fb232c2e452547ddfa243e93131e7cfa766da627b52/rsa-4.9.tar.gz
  bazaar:
    register: 'no'
    prim: 11/CTX1022383
    community_link: https://github.com/sybrenstuvel/python-rsa
    community_name: https://github.com/sybrenstuvel/python-rsa
    community_url: https://github.com/sybrenstuvel/python-rsa
    component_comment: ''
    component_highlevel_description: Python-RSA is a pure-Python RSA implementation. It supports encryption and decryption, signing and verifying signatures, and key generation according to PKCS#1 version 1.5.
    component_name: Python-RSA
    component_platform: linux
    component_programing_language: ''
    component_version: '4.9'
    licenses: []
    src_download_link: https://github.com/sybrenstuvel/python-rsa/archive/version-4.9.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1023400&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Netherlands
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: rsa
    target_sw: linux
    vendor: pip
    version: '4.9'
    web_url: https://stuvel.eu/rsa
  licenses:
  - Apache-2.0
  name: rsa
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - '4.9'
  mimer:
    linking: Static
    product_number: CTX1022383
    product_version_label: version-4.9
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: safetensors+0.4.2
  additional_info:
    fossa-attribution:
      Description: '## Installation```pip install safetensors```## Usage### Numpy```pythonfrom
        safetensors.numpy import save_file, load_fileimport numpy as nptensors = {   "a":
        np.zeros((2, 2)),   "b": np.zeros((2, 3), dtype=np.uint8)}save_file(tensors,
        "./model.safetensors")# Now loadingloaded = load_file("./model.safetensors")```###
        Torch```pythonfrom safetensors.torch import save_file, load_fileimport torchtensors
        = {   "a": torch.zeros((2, 2)),   "b": torch.zeros((2, 3), dtype=torch.uint8)}save_file(tensors,
        "./model.safetensors")# Now loadingloaded = load_file("./model.safetensors")```###
        Developing```# inside ./safetensors/bindings/pythonpip install .[dev]```Should
        be enough to install this library locally.### Testing```# inside ./safetensors/bindings/pythonpip
        install .[dev]pytest -sv tests/```'
      Package: safetensors
      Source: pip
      Version: 0.4.2
      Hash: ''
      licenses:
      - Apache-2.0
      Title: safetensors
      DownloadURL: https://files.pythonhosted.org/packages/32/b4/24d2855f668c2fbee5855cc6551684bdd3f7b935af324c9c8b20290d8443/safetensors-0.4.2.tar.gz
  bazaar:
    register: 'no'
    prim: 2/CTX1038721
    community_link: https://pypi.org/project/safetensors/
    community_name: https://pypi.org/project/safetensors/
    community_url: https://pypi.org/project/safetensors/
    component_comment: ''
    component_highlevel_description: Simple, safe way to store and distribute tensors
    component_name: safetensors
    component_platform: linux
    component_programing_language: Python
    component_version: 0.4.2
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://files.pythonhosted.org/packages/32/b4/24d2855f668c2fbee5855cc6551684bdd3f7b935af324c9c8b20290d8443/safetensors-0.4.2.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078652&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: safetensors
    target_sw: linux
    vendor: pip
    version: 0.4.2
    web_url: https://pypi.org/project/safetensors/0.4.2/
  licenses:
  - Apache-2.0
  name: safetensors
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 0.4.2
  mimer:
    linking: Static
    product_number: CTX1038721
    product_version_label: 0.4.2
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: sentencepiece+0.2.0
  additional_info:
    fossa-attribution:
      Description: "# SentencePiece Python WrapperPython wrapper for SentencePiece.
        This API will offer the encoding, decoding and training of Sentencepiece.##
        Build and Install SentencePieceFor Linux (x64/i686), macOS, and Windows(win32/x64)
        environment, you can simply use pip command to install SentencePiece python
        module.```% pip install sentencepiece```To build and install the Python wrapper
        from source, try the following commands to build and install wheel package.```%
        git clone https://github.com/google/sentencepiece.git % cd sentencepiece%
        mkdir build% cd build% cmake .. -DSPM_ENABLE_SHARED=OFF -DCMAKE_INSTALL_PREFIX=./root%
        make install% cd ../python% python setup.py bdist_wheel% pip install dist/sentencepiece*.whl```If
        you don\u2019t have write permission to the global site-packages directory
        or don\u2019t want to install into it, please try:```% python setup.py install
        --user```## UsageSee [this google colab page](https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb)
        to run sentencepiece interactively.### Segmentation```% python>>> import sentencepiece
        as spm>>> sp = spm.SentencePieceProcessor(model_file='test/test_model.model')>>>
        sp.encode('This is a test')[284, 47, 11, 4, 15, 400]>>> sp.encode(['This is
        a test', 'Hello world'], out_type=int)[[284, 47, 11, 4, 15, 400], [151, 88,
        21, 887]]>>> sp.encode_as_ids(['This is a test', 'Hello world'])[[284, 47,
        11, 4, 15, 400], [151, 88, 21, 887]]>>> sp.encode('This is a test', out_type=str)['\u2581This',
        '\u2581is', '\u2581a', '\u2581', 't', 'est']>>> sp.encode(['This is a test',
        'Hello world'], out_type=str)[['\u2581This', '\u2581is', '\u2581a', '\u2581',
        't', 'est'], ['\u2581He', 'll', 'o', '\u2581world']]>>> sp.encode_as_pieces(['This
        is a test', 'Hello world'])[['\u2581This', '\u2581is', '\u2581a', '\u2581',
        't', 'est'], ['\u2581He', 'll', 'o', '\u2581world']]>>> proto = sp.encode('This
        is a test', out_type='immutable_proto')>>> for n in proto.pieces:...     print('piece=\"{}\"
        surface=\"{}\" id={} begin={} end={}'.format(n.piece, n.surface, n.id, n.begin,
        n.end))... piece=\"\u2581This\" surface=\"This\" id=284 begin=0 end=4piece=\"\u2581is\"
        surface=\" is\" id=47 begin=4 end=7piece=\"\u2581a\" surface=\" a\" id=11
        begin=7 end=9piece=\"\u2581\" surface=\" \" id=4 begin=9 end=10piece=\"t\"
        surface=\"t\" id=15 begin=10 end=11piece=\"est\" surface=\"est\" id=400 begin=11
        end=14>>> [[x.id for x in proto.pieces], [x.piece for x in proto.pieces],
        [x.begin for x in proto.pieces], [x.end for x in proto.pieces]][[284, 47,
        11, 4, 15, 400], ['\u2581This', '\u2581is', '\u2581a', '\u2581', 't', 'est'],
        [0, 4, 7, 9, 10, 11], [4, 7, 9, 10, 11, 14]]>>> proto2 = sp.encode_as_immutable_proto('This
        is a test')>>> proto2 == protoTrue>>> for _ in range(10):...     sp.encode('This
        is a test', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)...
        ['\u2581', 'This', '\u2581', 'is', '\u2581a', '\u2581', 't', 'e', 'st']['\u2581T',
        'h', 'i', 's', '\u2581is', '\u2581a', '\u2581', 'te', 's', 't']['\u2581T',
        'h', 'is', '\u2581', 'is', '\u2581', 'a', '\u2581', 't', 'est']['\u2581',
        'This', '\u2581is', '\u2581', 'a', '\u2581', 't', 'e', 'st']['\u2581', 'This',
        '\u2581', 'is', '\u2581', 'a', '\u2581', 't', 'e', 's', 't']['\u2581This',
        '\u2581is', '\u2581a', '\u2581', 'te', 's', 't']['\u2581This', '\u2581is',
        '\u2581', 'a', '\u2581', 't', 'e', 'st']['\u2581', 'T', 'h', 'is', '\u2581',
        'is', '\u2581', 'a', '\u2581', 'te', 'st']['\u2581', 'This', '\u2581', 'i',
        's', '\u2581a', '\u2581', 't', 'e', 'st']['\u2581This', '\u2581', 'is', '\u2581a',
        '\u2581', 't', 'est']>> sp.nbest_encode('This is a test', nbest_size=5, out_type=str)[['\u2581This',
        '\u2581is', '\u2581a', '\u2581', 't', 'est'], ['\u2581This', '\u2581is', '\u2581a',
        '\u2581', 'te', 'st'], ['\u2581This', '\u2581is', '\u2581a', '\u2581', 'te',
        's', 't'],['\u2581This', '\u2581is', '\u2581a', '\u2581', 't', 'e', 'st'],['\u2581This',
        '\u2581is', '\u2581a', '\u2581', 't', 'es', 't']]>>> sp.sample_encode_and_score('This
        is a test', num_samples=5, alpha=0.1, out_type=str, wor=True)[(['\u2581This',
        '\u2581', 'i', 's', '\u2581a', '\u2581', 'te', 's', 't'], -3.043105125427246),(['\u2581This',
        '\u2581', 'i', 's', '\u2581a', '\u2581', 'te', 'st'], -2.8475849628448486),(['\u2581',
        'This', '\u2581is', '\u2581', 'a', '\u2581', 'te', 'st'], -3.043248176574707),(['\u2581',
        'This', '\u2581is', '\u2581a', '\u2581', 't', 'e', 'st'], -2.87727689743042),(['\u2581',
        'This', '\u2581', 'i', 's', '\u2581', 'a', '\u2581', 't', 'est'], -3.6284031867980957)]>>>
        sp.decode([284, 47, 11, 4, 15, 400])'This is a test'>>> sp.decode([[284, 47,
        11, 4, 15, 400], [151, 88, 21, 887]])['This is a test', 'Hello world']>>>
        proto = sp.decode([284, 47, 11, 4, 15, 400], out_type='immutable_proto') >>>
        proto.text'This is a test'>>> sp.decode(['\u2581', 'This', '\u2581', 'is',
        '\u2581a', '\u2581', 't', 'e', 'st'])'This is a test'>>> sp.decode([['\u2581This',
        '\u2581is', '\u2581a', '\u2581', 't', 'est'], ['\u2581He', 'll', 'o', '\u2581world']])['This
        is a test', 'Hello world']>>> sp.get_piece_size()1000>>> sp.id_to_piece(2)'</s>'>>>
        sp.id_to_piece([2, 3, 4])['</s>', '\\r', '\u2581']>>> sp.piece_to_id('<s>')1>>>
        sp.piece_to_id(['</s>', '\\r', '\u2581'])[2, 3, 4]>>> len(sp)1000>>> sp['</s>']2```###
        Model TrainingTraining is performed by passing parameters of [spm_train](https://github.com/google/sentencepiece#train-sentencepiece-model)
        to  SentencePieceTrainer.train() function.```>>> import sentencepiece as spm>>>
        spm.SentencePieceTrainer.train(input='test/botchan.txt', model_prefix='m',
        vocab_size=1000, user_defined_symbols=['foo', 'bar'])sentencepiece_trainer.cc(73)
        LOG(INFO) Starts training with : trainer_spec {  input: test/botchan.txt  ..
        snipunigram_model_trainer.cc(500) LOG(INFO) EM sub_iter=1 size=1188 obj=10.2839
        num_tokens=32182 num_tokens/piece=27.0892unigram_model_trainer.cc(500) LOG(INFO)
        EM sub_iter=0 size=1100 obj=10.4269 num_tokens=33001 num_tokens/piece=30.0009unigram_model_trainer.cc(500)
        LOG(INFO) EM sub_iter=1 size=1100 obj=10.4069 num_tokens=33002 num_tokens/piece=30.0018trainer_interface.cc(595)
        LOG(INFO) Saving model: m.modeltrainer_interface.cc(619) LOG(INFO) Saving
        vocabs: m.vocab>>>```### Training without local filesystemSentencepiece trainer
        can receive any iterable object to feed training sentences. You can also pass
        a file object (instance with write() method) to emit the output model to any
        devices. These features are useful to run sentencepiece on environment that
        have limited access to the local file system (e.g., Google colab.)```import
        urllib.requestimport ioimport sentencepiece as spm# Loads model from URL as
        iterator and stores the model to BytesIO.model = io.BytesIO()with urllib.request.urlopen(
        \   'https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt')
        as response:  spm.SentencePieceTrainer.train(      sentence_iterator=response,
        model_writer=model, vocab_size=1000)# Serialize the model as file.# with open('out.model',
        'wb') as f:#   f.write(model.getvalue())# Directly load the model from serialized
        model.sp = spm.SentencePieceProcessor(model_proto=model.getvalue())print(sp.encode('this
        is test'))```"
      Package: sentencepiece
      Source: pip
      Version: 0.2.0
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-3-Clause
      - CC0-1.0
      - MIT
      - Protobuf
      - gutenberg-2020
      - public-domain
      Title: sentencepiece
      DownloadURL: https://github.com/google/sentencepiece/archive/refs/tags/v0.2.0.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/google/sentencepiece
    community_name: https://github.com/google/sentencepiece
    community_url: https://github.com/google/sentencepiece
    component_comment: ''
    component_highlevel_description: ''
    component_name: sentencepiece
    component_platform: linux
    component_programing_language: ''
    component_version: v0.2.0
    licenses: []
    src_download_link: https://github.com/google/sentencepiece/archive/refs/tags/v0.2.0.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: sentencepiece
    target_sw: linux
    vendor: pip
    version: 0.2.0
    web_url: https://github.com/google/sentencepiece
  licenses:
  - Apache-2.0
  - BSD-3-Clause
  - CC0-1.0
  - MIT
  - Protobuf
  - gutenberg-2020
  - public-domain
  name: sentencepiece
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 0.2.0
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 0.2.0
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: setuptools+69.1.1
  additional_info:
    fossa-attribution:
      Description: '.. image:: https://img.shields.io/pypi/v/setuptools.svg   :target:
        https://pypi.org/project/setuptools.. image:: https://img.shields.io/pypi/pyversions/setuptools.svg..
        image:: https://github.com/pypa/setuptools/actions/workflows/main.yml/badge.svg   :target:
        https://github.com/pypa/setuptools/actions?query=workflow%3A%22tests%22   :alt:
        tests.. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json    :target:
        https://github.com/astral-sh/ruff    :alt: Ruff.. image:: https://img.shields.io/readthedocs/setuptools/latest.svg    :target:
        https://setuptools.pypa.io.. image:: https://img.shields.io/badge/skeleton-2024-informational   :target:
        https://blog.jaraco.com/skeleton.. image:: https://img.shields.io/codecov/c/github/pypa/setuptools/master.svg?logo=codecov&logoColor=white   :target:
        https://codecov.io/gh/pypa/setuptools.. image:: https://tidelift.com/badges/github/pypa/setuptools?style=flat   :target:
        https://tidelift.com/subscription/pkg/pypi-setuptools?utm_source=pypi-setuptools&utm_medium=readme..
        image:: https://img.shields.io/discord/803025117553754132   :target: https://discord.com/channels/803025117553754132/815945031150993468   :alt:
        DiscordSee the `Quickstart <https://setuptools.pypa.io/en/latest/userguide/quickstart.html>`_and
        the `User''s Guide <https://setuptools.pypa.io/en/latest/userguide/>`_ forinstructions
        on how to use Setuptools.Questions and comments should be directed to `GitHub
        Discussions<https://github.com/pypa/setuptools/discussions>`_.Bug reports
        and especially tested patches may besubmitted directly to the `bug tracker<https://github.com/pypa/setuptools/issues>`_.Code
        of Conduct===============Everyone interacting in the setuptools project''s
        codebases, issue trackers,chat rooms, and fora is expected to follow the`PSF
        Code of Conduct <https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md>`_.For
        Enterprise==============Available as part of the Tidelift Subscription.Setuptools
        and the maintainers of thousands of other packages are working with Tidelift
        to deliver one enterprise subscription that covers all of the open source
        you use.`Learn more <https://tidelift.com/subscription/pkg/pypi-setuptools?utm_source=pypi-setuptools&utm_medium=referral&utm_campaign=github>`_.'
      Package: setuptools
      Source: pip
      Version: 69.1.1
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-3-Clause
      - MIT
      Title: setuptools
      DownloadURL: https://files.pythonhosted.org/packages/c8/1f/e026746e5885a83e1af99002ae63650b7c577af5c424d4c27edcf729ab44/setuptools-69.1.1.tar.gz
  bazaar:
    register: 'no'
    prim: 180/CAX1054853
    community_link: https://pypi.org/project/setuptools/
    community_name: https://pypi.org/project/setuptools/
    community_url: https://pypi.org/project/setuptools/
    component_comment: ''
    component_highlevel_description: ''
    component_name: setuptools
    component_platform: linux
    component_programing_language: ''
    component_version: V69.1.1
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://files.pythonhosted.org/packages/c8/1f/e026746e5885a83e1af99002ae63650b7c577af5c424d4c27edcf729ab44/setuptools-69.1.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1079832&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: setuptools
    target_sw: linux
    vendor: pip
    version: 69.1.1
    web_url: https://github.com/pypa/setuptools
  licenses:
  - Apache-2.0
  - BSD-3-Clause
  - MIT
  name: setuptools
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 69.1.1
  mimer:
    linking: Static
    product_number: CAX1054853
    product_version_label: V69.1.1
    selected_licenses:
    - MIT
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: six+1.16.0
  additional_info:
    fossa-attribution:
      Description: '.. image:: https://img.shields.io/pypi/v/six.svg   :target: https://pypi.org/project/six/   :alt:
        six on PyPI.. image:: https://travis-ci.org/benjaminp/six.svg?branch=master   :target:
        https://travis-ci.org/benjaminp/six   :alt: six on TravisCI.. image:: https://readthedocs.org/projects/six/badge/?version=latest   :target:
        https://six.readthedocs.io/   :alt: six''s documentation on Read the Docs..
        image:: https://img.shields.io/badge/license-MIT-green.svg   :target: https://github.com/benjaminp/six/blob/master/LICENSE   :alt:
        MIT License badgeSix is a Python 2 and 3 compatibility library.  It provides
        utility functionsfor smoothing over the differences between the Python versions
        with the goal ofwriting Python code that is compatible on both Python versions.  See
        thedocumentation for more information on what is provided.Six supports Python
        2.7 and 3.3+.  It is contained in only one Pythonfile, so it can be easily
        copied into your project. (The copyright and licensenotice must be retained.)Online
        documentation is at https://six.readthedocs.io/.Bugs can be reported to https://github.com/benjaminp/six.  The
        code can alsobe found there.'
      Package: six
      Source: pip
      Version: 1.16.0
      Hash: ''
      licenses:
      - MIT
      Title: six
      DownloadURL: https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl
  bazaar:
    register: 'no'
    prim: 13/CAX1056543
    community_link: https://pythonhosted.org/six/
    community_name: https://pythonhosted.org/six/
    community_url: https://pythonhosted.org/six/
    component_comment: ''
    component_highlevel_description: Six is a Python 2 and 3 compatibility library.  It
      provides utility functions for smoothing over the differences between the Python
      versions with the goal of writing Python code that is compatible on both Python
      versions
    component_name: six
    component_platform: linux
    component_programing_language: Python
    component_version: 1.16.0
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://github.com/benjaminp/six/archive/refs/tags/1.16.0.zip
    stako_decision_reason: allowed
    stako: ESW4
    stako_comment: Inactive community.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=970756&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: N/A
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: six
    target_sw: linux
    vendor: pip
    version: 1.16.0
    web_url: https://github.com/benjaminp/six
  licenses:
  - MIT
  name: six
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 1.16.0
  mimer:
    linking: Static
    product_number: CAX1056543
    product_version_label: 1.16.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: sympy+1.12
  additional_info:
    fossa-attribution:
      Description: "# SymPy[![pypi version](https://img.shields.io/pypi/v/sympy.svg)](https://pypi.python.org/pypi/sympy)[![Join
        the chat at https://gitter.im/sympy/sympy](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/sympy/sympy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)[![Zenodo
        Badge](https://zenodo.org/badge/18918/sympy/sympy.svg)](https://zenodo.org/badge/latestdoi/18918/sympy/sympy)[![Downloads](https://pepy.tech/badge/sympy/month)](https://pepy.tech/project/sympy)[![GitHub
        Issues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/sympy/sympy/issues)[![Git
        Tutorial](https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?)](https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project)[![Powered
        by NumFocus](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)[![Commits
        since last release](https://img.shields.io/github/commits-since/sympy/sympy/latest.svg?longCache=true&style=flat-square&logo=git&logoColor=fff)](https://github.com/sympy/sympy/releases)[![SymPy
        Banner](https://github.com/sympy/sympy/raw/master/banner.svg)](https://sympy.org/)See
        the [AUTHORS](AUTHORS) file for the list of authors.And many more people helped
        on the SymPy mailing list, reported bugs,helped organize SymPy's participation
        in the Google Summer of Code, theGoogle Highly Open Participation Contest,
        Google Code-In, wrote andblogged about SymPy...License: New BSD License (see
        the [LICENSE](LICENSE) file for details) covers allfiles in the sympy repository
        unless stated otherwise.Our mailing list is at<https://groups.google.com/forum/?fromgroups#!forum/sympy>.We
        have a community chat at [Gitter](https://gitter.im/sympy/sympy). Feelfree
        to ask us anything there. We have a very welcoming and helpfulcommunity.##
        DownloadThe recommended installation method is through Anaconda,<https://www.anaconda.com/products/distribution>You
        can also get the latest version of SymPy from<https://pypi.python.org/pypi/sympy/>To
        get the git version do    $ git clone https://github.com/sympy/sympy.gitFor
        other options (tarballs, debs, etc.), see<https://docs.sympy.org/dev/install.html>.##
        Documentation and UsageFor in-depth instructions on installation and building
        thedocumentation, see the [SymPy Documentation Style Guide](https://docs.sympy.org/dev/documentation-style-guide.html).Everything
        is at:<https://docs.sympy.org/>You can generate everything at the above site
        in your local copy ofSymPy by:    $ cd doc    $ make htmlThen the docs will
        be in <span class=\"title-ref\">\\_build/html</span>. Ifyou don't want to
        read that, here is a short usage:From this directory, start Python and:```
        python>>> from sympy import Symbol, cos>>> x = Symbol('x')>>> e = 1/cos(x)>>>
        print(e.series(x, 0, 10))1 + x**2/2 + 5*x**4/24 + 61*x**6/720 + 277*x**8/8064
        + O(x**10)```SymPy also comes with a console that is a simple wrapper around
        theclassic python console (or IPython when available) that loads the SymPynamespace
        and executes some common commands for you.To start it, issue:    $ bin/isympyfrom
        this directory, if SymPy is not installed or simply:    $ isympyif SymPy is
        installed.## InstallationSymPy has a hard dependency on the [mpmath](http://mpmath.org/)
        library(version \\>= 0.19). You should install it first, please refer to thempmath
        installation guide:<https://github.com/fredrik-johansson/mpmath#1-download--installation>To
        install SymPy using PyPI, run the following command:    $ pip install sympyTo
        install SymPy using Anaconda, run the following command:    $ conda install
        -c anaconda sympyTo install SymPy from GitHub source, first clone SymPy using
        `git`:    $ git clone https://github.com/sympy/sympy.gitThen, in the `sympy`
        repository that you cloned, simply run:    $ pip install .See <https://docs.sympy.org/dev/install.html>
        for more information.## ContributingWe welcome contributions from anyone,
        even if you are new to opensource. Please read our [Introduction to Contributing](https://github.com/sympy/sympy/wiki/Introduction-to-contributing)page
        and the [SymPy Documentation Style Guide](https://docs.sympy.org/dev/documentation-style-guide.html).
        If youare new and looking for some way to contribute, a good place to start
        isto look at the issues tagged [Easy to Fix](https://github.com/sympy/sympy/issues?q=is%3Aopen+is%3Aissue+label%3A%22Easy+to+Fix%22).Please
        note that all participants in this project are expected to followour Code
        of Conduct. By participating in this project you agree to abideby its terms.
        See [CODE\\_OF\\_CONDUCT.md](CODE_OF_CONDUCT.md).## TestsTo execute all tests,
        run:    $./setup.py testin the current directory.For the more fine-grained
        running of tests or doctests, use `bin/test`or respectively `bin/doctest`.
        The master branch is automatically testedby GitHub Actions.To test pull requests,
        use[sympy-bot](https://github.com/sympy/sympy-bot).## Regenerate Experimental
        <span class=\"title-ref\">LaTeX</span> Parser/LexerThe parser and lexer were
        generated with the [ANTLR4](http://antlr4.org)toolchain in `sympy/parsing/latex/_antlr`
        and checked into the repo.Presently, most users should not need to regenerate
        these files, butif you plan to work on this feature, you will need the `antlr4`command-line
        tool (and you must ensure that it is in your `PATH`).One way to get it is:
        \   $ conda install -c conda-forge antlr=4.11.1Alternatively, follow the instructions
        on the ANTLR website and downloadthe `antlr-4.11.1-complete.jar`. Then export
        the `CLASSPATH` as instructedand instead of creating `antlr4` as an alias,
        make it an executable filewith the following contents:``` bash#!/bin/bashjava
        -jar /usr/local/lib/antlr-4.11.1-complete.jar \"$@\"```After making changes
        to `sympy/parsing/latex/LaTeX.g4`, run:    $ ./setup.py antlr## CleanTo clean
        everything (thus getting the same tree as in the repository):    $ git clean
        -Xdfwhich will clear everything ignored by `.gitignore`, and:    $ git clean
        -dfto clear all untracked files. You can revert the most recent changes ingit
        with:    $ git reset --hardWARNING: The above commands will all clear changes
        you may have made,and you will lose them forever. Be sure to check things
        with `gitstatus`, `git diff`, `git clean -Xn`, and `git clean -n` before doing
        anyof those.## BugsOur issue tracker is at <https://github.com/sympy/sympy/issues>.
        Pleasereport any bugs that you find. Or, even better, fork the repository
        onGitHub and create a pull request. We welcome all changes, big or small,and
        we will help you make the pull request if you are new to git (justask on our
        mailing list or Gitter Channel). If you further have any queries, you can
        find answerson Stack Overflow using the [sympy](https://stackoverflow.com/questions/tagged/sympy)
        tag.## Brief HistorySymPy was started by Ond\u0159ej \u010Cert\xEDk in 2005,
        he wrote some code duringthe summer, then he wrote some more code during summer
        2006. In February2007, Fabian Pedregosa joined the project and helped fix
        many things,contributed documentation, and made it alive again. 5 students
        (MateuszPaprocki, Brian Jorgensen, Jason Gedge, Robert Schwarz, and Chris
        Wu)improved SymPy incredibly during summer 2007 as part of the GoogleSummer
        of Code. Pearu Peterson joined the development during the summer2007 and he
        has made SymPy much more competitive by rewriting the corefrom scratch, which
        has made it from 10x to 100x faster. Jurjen N.E. Boshas contributed pretty-printing
        and other patches. Fredrik Johansson haswritten mpmath and contributed a lot
        of patches.SymPy has participated in every Google Summer of Code since 2007.
        Youcan see <https://github.com/sympy/sympy/wiki#google-summer-of-code> forfull
        details. Each year has improved SymPy by bounds. Most of SymPy'sdevelopment
        has come from Google Summer of Code students.In 2011, Ond\u0159ej \u010Cert\xEDk
        stepped down as lead developer, with AaronMeurer, who also started as a Google
        Summer of Code student, taking hisplace. Ond\u0159ej \u010Cert\xEDk is still
        active in the community but is too busywith work and family to play a lead
        development role.Since then, a lot more people have joined the development
        and somepeople have also left. You can see the full list in doc/src/aboutus.rst,or
        online at:<https://docs.sympy.org/dev/aboutus.html#sympy-development-team>The
        git history goes back to 2007 when development moved from svn to hg.To see
        the history before that point, look at<https://github.com/sympy/sympy-old>.You
        can use git to see the biggest developers. The command:    $ git shortlog
        -nswill show each developer, sorted by commits to the project. The command:
        \   $ git shortlog -ns --since=\"1 year\"will show the top developers from
        the last year.## CitationTo cite SymPy in publications use> Meurer A, Smith
        CP, Paprocki M, \u010Cert\xEDk O, Kirpichev SB, Rocklin M,> Kumar A, Ivanov
        S, Moore JK, Singh S, Rathnayake T, Vig S, Granger BE,> Muller RP, Bonazzi
        F, Gupta H, Vats S, Johansson F, Pedregosa F, Curry> MJ, Terrel AR, Rou\u010Dka
        \u0160, Saboo A, Fernando I, Kulal S, Cimrman R,> Scopatz A. (2017) SymPy:
        symbolic computing in Python. *PeerJ Computer> Science* 3:e103 <https://doi.org/10.7717/peerj-cs.103>A
        BibTeX entry for LaTeX users is``` bibtex@article{10.7717/peerj-cs.103, title
        = {SymPy: symbolic computing in Python}, author = {Meurer, Aaron and Smith,
        Christopher P. and Paprocki, Mateusz and \\v{C}ert\\'{i}k, Ond\\v{r}ej and
        Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, Amit and Ivanov, Sergiu
        and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean
        and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta,
        Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry,
        Matthew J. and Terrel, Andy R. and Rou\\v{c}ka, \\v{S}t\\v{e}p\\'{a}n and
        Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert
        and Scopatz, Anthony}, year = 2017, month = Jan, keywords = {Python, Computer
        algebra system, Symbolics}, abstract = {            SymPy is an open-source
        computer algebra system written in pure Python. It is built with a focus on
        extensibility and ease of use, through both interactive and programmatic applications.
        These characteristics have led SymPy to become a popular symbolic library
        for the scientific Python ecosystem. This paper presents the architecture
        of SymPy, a description of its features, and a discussion of select submodules.
        The supplementary material provides additional examples and further outlines
        details of the architecture and features of SymPy.         }, volume = 3,
        pages = {e103}, journal = {PeerJ Computer Science}, issn = {2376-5992}, url
        = {https://doi.org/10.7717/peerj-cs.103}, doi = {10.7717/peerj-cs.103}}```SymPy
        is BSD licensed, so you are free to use it whatever you like, beit academic,
        commercial, creating forks or derivatives, as long as youcopy the BSD statement
        if you redistribute it (see the LICENSE file fordetails). That said, although
        not required by the SymPy license, if itis convenient for you, please cite
        SymPy when using it in your work andalso consider contributing all your changes
        back, so that we canincorporate it and all of us will benefit in the end."
      Package: sympy
      Source: pip
      Version: '1.12'
      Hash: ''
      licenses:
      - BSD-3-Clause
      - MIT
      Title: sympy
      DownloadURL: https://files.pythonhosted.org/packages/e5/57/3485a1a3dff51bfd691962768b14310dae452431754bfc091250be50dd29/sympy-1.12.tar.gz
  bazaar:
    register: 'no'
    prim: 2/CTX1031223
    community_link: https://pypi.org/project/sympy/
    community_name: https://pypi.org/project/sympy/
    community_url: https://pypi.org/project/sympy/
    component_comment: ''
    component_highlevel_description: A computer algebra system written in pure Python.
    component_name: sympy
    component_platform: linux
    component_programing_language: Python
    component_version: '1.12'
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/e5/57/3485a1a3dff51bfd691962768b14310dae452431754bfc091250be50dd29/sympy-1.12.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Low community activity.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1065403&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: sympy
    target_sw: linux
    vendor: pip
    version: '1.12'
    web_url: https://sympy.org
  licenses:
  - BSD-3-Clause
  - MIT
  name: sympy
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '1.12'
  mimer:
    linking: Static
    product_number: CTX1031223
    product_version_label: '1.12'
    selected_licenses:
    - BSD-3-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: tensorboard+2.15.2
  additional_info:
    fossa-attribution:
      Description: TensorBoard is a suite of web applications for inspecting and understandingyour
        TensorFlow runs and graphs.Releases prior to 1.6.0 were published under the
        ``tensorflow-tensorboard`` nameand may be found at https://pypi.python.org/pypi/tensorflow-tensorboard.
      Package: tensorboard
      Source: pip
      Version: 2.15.2
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-3-Clause
      - MIT
      Title: tensorboard
      DownloadURL: https://github.com/tensorflow/tensorboard/archive/refs/tags/2.15.2.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/tensorflow/tensorboard
    community_name: https://github.com/tensorflow/tensorboard
    community_url: https://github.com/tensorflow/tensorboard
    component_comment: ''
    component_highlevel_description: ''
    component_name: tensorboard
    component_platform: linux
    component_programing_language: ''
    component_version: 2.15.2
    licenses: []
    src_download_link: https://github.com/tensorflow/tensorboard/archive/refs/tags/2.15.2.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: tensorboard
    target_sw: linux
    vendor: pip
    version: 2.15.2
    web_url: https://github.com/tensorflow/tensorboard
  licenses:
  - Apache-2.0
  - BSD-3-Clause
  - MIT
  name: tensorboard
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 2.15.2
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 2.15.2
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: tensorboard-data-server+0.7.2
  additional_info:
    fossa-attribution:
      Description: Fast data loading for TensorBoard
      Package: tensorboard-data-server
      Source: pip
      Version: 0.7.2
      Hash: ''
      licenses:
      - Apache-2.0
      Title: tensorboard-data-server
      DownloadURL: https://github.com/tensorflow/tensorboard/archive/data-server-v0.7.2.tar.gz
  bazaar:
    register: 'no'
    prim: 22/CTX1022230
    community_link: https://github.com/tensorflow/tensorboard
    community_name: https://github.com/tensorflow/tensorboard
    community_url: https://github.com/tensorflow/tensorboard
    component_comment: ''
    component_highlevel_description: TensorBoard is a suite of web applications for inspecting and understanding your TensorFlow runs and graphs.
    component_name: tensorboard-data-server
    component_platform: linux
    component_programing_language: ''
    component_version: DATA-SERVER-V0.7.2
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/tensorflow/tensorboard/archive/data-server-v0.7.2.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1072239&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: tensorboard-data-server
    target_sw: linux
    vendor: pip
    version: 0.7.2
    web_url: https://github.com/tensorflow/tensorboard/tree/master/tensorboard/data/server
  licenses:
  - Apache-2.0
  name: tensorboard-data-server
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 0.7.2
  mimer:
    linking: Static
    product_number: CTX1022230
    product_version_label: data-server-v0.7.2
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: tensorflow+2.15.0
  additional_info:
    fossa-attribution:
      Description: '[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)TensorFlow
        is an open source software library for high performance numericalcomputation.
        Its flexible architecture allows easy deployment of computationacross a variety
        of platforms (CPUs, GPUs, TPUs), and from desktops to clustersof servers to
        mobile and edge devices.Originally developed by researchers and engineers
        from the Google Brain teamwithin Google''s AI organization, it comes with
        strong support for machinelearning and deep learning and the flexible numerical
        computation core is usedacross many other scientific domains. TensorFlow is
        licensed under [Apache2.0](https://github.com/tensorflow/tensorflow/blob/master/LICENSE).'
      Package: tensorflow
      Source: pip
      Version: 2.15.0
      Hash: ''
      licenses:
      - 0BSD
      - Apache-2.0
      - BSD-2-Clause
      - BSD-2-Clause-FreeBSD
      - BSD-3-Clause
      - BSD-4-Clause-UC
      - BSD-Source-Code
      - BSL-1.0
      - CC-BY-3.0
      - CC0-1.0
      - GPL-2.0-only
      - GPL-2.0-or-later
      - GPL-2.0-with-autoconf-exception
      - ICU
      - IJG
      - ISC
      - Libpng
      - MIT
      - MPL-2.0
      - MatPlotLib
      - NCSA
      - OpenSSL
      - PIL
      - PSF-2.0
      - Protobuf
      - Python-2.0
      - Spencer-94
      - SunPro
      - Unicode-DFS-2015
      - Unicode-DFS-2016
      - Zlib
      - apache-2.0 WITH llvm-exception
      - bsd-2-clause-views
      - bsd-3-clause-open-mpi
      - curl
      - libpng-v2
      - naist-2003
      - openssl-nokia-psk-contribution
      - openssl-ssleay
      - proprietary-license
      - public-domain
      Title: tensorflow
      DownloadURL: https://files.pythonhosted.org/packages/20/ba/7560053dbd0b2f5e3654c6d0ad57267558d2d17aec6604a9f6a3fdfb4d5c/tensorflow-2.15.0-cp310-cp310-macosx_10_15_x86_64.whl
  bazaar:
    register: 'no'
    prim: 45/CTX1022158
    community_link: https://github.com/tensorflow/tensorflow
    community_name: https://github.com/tensorflow/tensorflow
    community_url: https://github.com/tensorflow/tensorflow
    component_comment: ''
    component_highlevel_description: TensorFlow is an open source software library
      for numerical computation using data flow graphs.
    component_name: tensorflow
    component_platform: linux
    component_programing_language: C++
    component_version: V2.15.0
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/tensorflow/tensorflow/archive/v2.15.0.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Copyleft license found.
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1082286&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: C++
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: tensorflow
    target_sw: linux
    vendor: pip
    version: 2.15.0
    web_url: https://www.tensorflow.org/
  licenses:
  - 0BSD
  - Apache-2.0
  - BSD-2-Clause
  - BSD-2-Clause-FreeBSD
  - BSD-3-Clause
  - BSD-4-Clause-UC
  - BSD-Source-Code
  - BSL-1.0
  - CC-BY-3.0
  - CC0-1.0
  - GPL-2.0-only
  - GPL-2.0-or-later
  - GPL-2.0-with-autoconf-exception
  - ICU
  - IJG
  - ISC
  - Libpng
  - MIT
  - MPL-2.0
  - MatPlotLib
  - NCSA
  - OpenSSL
  - PIL
  - PSF-2.0
  - Protobuf
  - Python-2.0
  - Spencer-94
  - SunPro
  - Unicode-DFS-2015
  - Unicode-DFS-2016
  - Zlib
  - apache-2.0 WITH llvm-exception
  - bsd-2-clause-views
  - bsd-3-clause-open-mpi
  - curl
  - libpng-v2
  - naist-2003
  - openssl-nokia-psk-contribution
  - openssl-ssleay
  - proprietary-license
  - public-domain
  name: tensorflow
  primary:
  - this
  subcomponent: false
  type: FOSS
  versions:
  - 2.15.0
  mimer:
    linking: Static
    product_number: CTX1022158
    product_version_label: V2.15.0
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'True'
- ID: tensorflow-estimator+2.15.0
  additional_info:
    fossa-attribution:
      Description: TensorFlow Estimator is a high-level API that encapsulates model
        training,evaluation, prediction, and exporting.
      Package: tensorflow-estimator
      Source: pip
      Version: 2.15.0
      Hash: ''
      licenses:
      - Apache-2.0
      Title: tensorflow-estimator
      DownloadURL: https://github.com/tensorflow/estimator/archive/refs/tags/v2.15.0.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/tensorflow/estimator
    community_name: https://github.com/tensorflow/estimator
    community_url: https://github.com/tensorflow/estimator
    component_comment: ''
    component_highlevel_description: ''
    component_name: tensorflow-estimator
    component_platform: linux
    component_programing_language: ''
    component_version: v2.15.0
    licenses: []
    src_download_link: https://github.com/tensorflow/estimator/archive/refs/tags/v2.15.0.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: tensorflow-estimator
    target_sw: linux
    vendor: pip
    version: 2.15.0
    web_url: https://www.tensorflow.org/
  licenses:
  - Apache-2.0
  name: tensorflow-estimator
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 2.15.0
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 2.15.0
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: tensorflow-io-gcs-filesystem+0.36.0
  additional_info:
    fossa-attribution:
      Description: '<div align="center">  <img src="https://github.com/tensorflow/community/blob/master/sigs/logos/SIGIO.png"
        width="60%"><br><br></div>-----------------# TensorFlow I/O[![GitHub CI](https://github.com/tensorflow/io/workflows/GitHub%20CI/badge.svg?branch=master)](https://github.com/tensorflow/io/actions?query=branch%3Amaster)[![PyPI](https://badge.fury.io/py/tensorflow-io.svg)](https://pypi.org/project/tensorflow-io/)[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/tensorflow/io/blob/master/LICENSE)[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/io)TensorFlow
        I/O is a collection of file systems and file formats that are notavailable
        in TensorFlow''s built-in support. A full list of supported file systemsand
        file formats by TensorFlow I/O can be found [here](https://www.tensorflow.org/io/api_docs/python/tfio).The
        use of tensorflow-io is straightforward with keras. Below is an exampleto
        [Get Started with TensorFlow](https://www.tensorflow.org/tutorials/quickstart/beginner)
        withthe data processing aspect replaced by tensorflow-io:```pythonimport tensorflow
        as tfimport tensorflow_io as tfio# Read the MNIST data into the IODataset.dataset_url
        = "https://storage.googleapis.com/cvdf-datasets/mnist/"d_train = tfio.IODataset.from_mnist(    dataset_url
        + "train-images-idx3-ubyte.gz",    dataset_url + "train-labels-idx1-ubyte.gz",)#
        Shuffle the elements of the dataset.d_train = d_train.shuffle(buffer_size=1024)#
        By default image data is uint8, so convert to float32 using map().d_train
        = d_train.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y))#
        prepare batches the data just like any other tf.data.Datasetd_train = d_train.batch(32)#
        Build the model.model = tf.keras.models.Sequential(    [        tf.keras.layers.Flatten(input_shape=(28,
        28)),        tf.keras.layers.Dense(512, activation=tf.nn.relu),        tf.keras.layers.Dropout(0.2),        tf.keras.layers.Dense(10,
        activation=tf.nn.softmax),    ])# Compile the model.model.compile(    optimizer="adam",
        loss="sparse_categorical_crossentropy", metrics=["accuracy"])# Fit the model.model.fit(d_train,
        epochs=5, steps_per_epoch=200)```In the above [MNIST](http://yann.lecun.com/exdb/mnist/)
        example, the URL''sto access the dataset files are passed directly to the
        `tfio.IODataset.from_mnist` API call.This is due to the inherent support that
        `tensorflow-io` provides for `HTTP`/`HTTPS` file system,thus eliminating the
        need for downloading and saving datasets on a local directory.NOTE: Since
        `tensorflow-io` is able to detect and uncompress the MNIST dataset automatically
        if needed,we can pass the URL''s for the compressed files (gzip) to the API
        call as is.Please check the official [documentation](https://www.tensorflow.org/io)
        for moredetailed and interesting usages of the package.## Installation###
        Python PackageThe `tensorflow-io` Python package can be installed with pip
        directly using:```sh$ pip install tensorflow-io```People who are a little
        more adventurous can also try our nightly binaries:```sh$ pip install tensorflow-io-nightly```To
        ensure you have a version of TensorFlow that is compatible with TensorFlow-IO,you
        can specify the `tensorflow` extra requirement during install:```pip install
        tensorflow-io[tensorflow]```Similar extras exist for the `tensorflow-gpu`,
        `tensorflow-cpu` and `tensorflow-rocm`packages.### Docker ImagesIn addition
        to the pip packages, the docker images can be used to quickly get started.For
        stable builds:```sh$ docker pull tfsigio/tfio:latest$ docker run -it --rm
        --name tfio-latest tfsigio/tfio:latest```For nightly builds:```sh$ docker
        pull tfsigio/tfio:nightly$ docker run -it --rm --name tfio-nightly tfsigio/tfio:nightly```###
        R PackageOnce the `tensorflow-io` Python package has been successfully installed,
        youcan install the development version of the R package from GitHub via the
        following:```rif (!require("remotes")) install.packages("remotes")remotes::install_github("tensorflow/io",
        subdir = "R-package")```### TensorFlow Version CompatibilityTo ensure compatibility
        with TensorFlow, it is recommended to install a matchingversion of TensorFlow
        I/O according to the table below. You can find the listof releases [here](https://github.com/tensorflow/io/releases).|
        TensorFlow I/O Version | TensorFlow Compatibility | Release Date || --- |
        --- | --- || 0.36.0 | 2.15.x | Feb 02, 2024 || 0.35.0 | 2.14.x | Dec 18, 2023
        || 0.34.0 | 2.13.x | Sep 08, 2023 || 0.33.0 | 2.13.x | Aug 01, 2023 || 0.32.0
        | 2.12.x | Mar 28, 2023 || 0.31.0 | 2.11.x | Feb 25, 2023 || 0.30.0 | 2.11.x
        | Jan 20, 2023 || 0.29.0 | 2.11.x | Dec 18, 2022 || 0.28.0 | 2.11.x | Nov
        21, 2022 || 0.27.0 | 2.10.x | Sep 08, 2022 || 0.26.0 | 2.9.x | May 17, 2022
        || 0.25.0 | 2.8.x | Apr 19, 2022 || 0.24.0 | 2.8.x | Feb 04, 2022 || 0.23.1
        | 2.7.x | Dec 15, 2021 || 0.23.0 | 2.7.x | Dec 14, 2021 || 0.22.0 | 2.7.x
        | Nov 10, 2021 || 0.21.0 | 2.6.x | Sep 12, 2021 || 0.20.0 | 2.6.x | Aug 11,
        2021 || 0.19.1 | 2.5.x | Jul 25, 2021 || 0.19.0 | 2.5.x | Jun 25, 2021 ||
        0.18.0 | 2.5.x | May 13, 2021 || 0.17.1 | 2.4.x | Apr 16, 2021 || 0.17.0 |
        2.4.x | Dec 14, 2020 || 0.16.0 | 2.3.x | Oct 23, 2020 || 0.15.0 | 2.3.x |
        Aug 03, 2020 || 0.14.0 | 2.2.x | Jul 08, 2020 || 0.13.0 | 2.2.x | May 10,
        2020 || 0.12.0 | 2.1.x | Feb 28, 2020 || 0.11.0 | 2.1.x | Jan 10, 2020 ||
        0.10.0 | 2.0.x | Dec 05, 2019 || 0.9.1 | 2.0.x | Nov 15, 2019 || 0.9.0 | 2.0.x
        | Oct 18, 2019 || 0.8.1 | 1.15.x | Nov 15, 2019 || 0.8.0 | 1.15.x | Oct 17,
        2019 || 0.7.2 | 1.14.x | Nov 15, 2019 || 0.7.1 | 1.14.x | Oct 18, 2019 ||
        0.7.0 | 1.14.x | Jul 14, 2019 || 0.6.0 | 1.13.x | May 29, 2019 || 0.5.0 |
        1.13.x | Apr 12, 2019 || 0.4.0 | 1.13.x | Mar 01, 2019 || 0.3.0 | 1.12.0 |
        Feb 15, 2019 || 0.2.0 | 1.12.0 | Jan 29, 2019 || 0.1.0 | 1.12.0 | Dec 16,
        2018 |## Performance BenchmarkingWe use [github-pages](https://tensorflow.github.io/io/dev/bench/)
        to document the results of API performance benchmarks. The benchmark job is
        triggered on every commit to `master` branch andfacilitates tracking performance
        w.r.t commits.## ContributingTensorflow I/O is a community led open source
        project. As such, the projectdepends on public contributions, bug-fixes, and
        documentation. Please see:- [contribution guidelines](CONTRIBUTING.md) for
        a guide on how to contribute.- [development doc](docs/development.md) for
        instructions on the development environment setup.- [tutorials](docs/tutorials)
        for a list of tutorial notebooks and instructions on how to write one.###
        Build Status and CI| Build | Status || --- | --- || Linux CPU Python 2 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-py2.html)
        || Linux CPU Python 3 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-py3.html)
        || Linux GPU Python 2| [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-gpu-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-gpu-py2.html)
        || Linux GPU Python 3| [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/io/ubuntu-gpu-py3.html)
        |Because of manylinux2010 requirement, TensorFlow I/O is built withUbuntu:16.04
        + Developer Toolset 7 (GCC 7.3) on Linux. Configurationwith Ubuntu 16.04 with
        Developer Toolset 7 is not exactly straightforward.If the system have docker
        installed, then the following commandwill automatically build manylinux2010
        compatible whl package:```sh#!/usr/bin/env bashls dist/*for f in dist/*.whl;
        do  docker run -i --rm -v $PWD:/v -w /v --net=host quay.io/pypa/manylinux2010_x86_64
        bash -x -e /v/tools/build/auditwheel repair --plat manylinux2010_x86_64 $fdonesudo
        chown -R $(id -nu):$(id -ng) .ls wheelhouse/*```It takes some time to build,
        but once complete, there will be python`3.5`, `3.6`, `3.7` compatible whl
        packages available in `wheelhouse`directory.On macOS, the same command could
        be used. However, the script expects `python` in shelland will only generate
        a whl package that matches the version of `python` in shell. Ifyou want to
        build a whl package for a specific python then you have to alias this versionof
        python to `python` in shell. See [.github/workflows/build.yml](.github/workflows/build.yml)Auditwheel
        step for instructions how to do that.Note the above command is also the command
        we use when releasing packages for Linux and macOS.TensorFlow I/O uses both
        GitHub Workflows and Google CI (Kokoro) for continuous integration.GitHub
        Workflows is used for macOS build and test. Kokoro is used for Linux build
        and test.Again, because of the manylinux2010 requirement, on Linux whl packages
        are alwaysbuilt with Ubuntu 16.04 + Developer Toolset 7. Tests are done on
        a variatiy of systemswith different python3 versions to ensure a good coverage:|
        Python | Ubuntu 18.04| Ubuntu 20.04 | macOS + osx9 | Windows-2019 || -------
        | ----- | ------- | ------- | --------- || 2.7 |  :heavy_check_mark: | :heavy_check_mark:
        | :heavy_check_mark: | N/A || 3.7 |  :heavy_check_mark: | :heavy_check_mark:
        | :heavy_check_mark: | :heavy_check_mark: || 3.8 |  :heavy_check_mark: | :heavy_check_mark:
        | :heavy_check_mark: | :heavy_check_mark: |TensorFlow I/O has integrations
        with many systems and cloud vendors such asPrometheus, Apache Kafka, Apache
        Ignite, Google Cloud PubSub, AWS Kinesis,Microsoft Azure Storage, Alibaba
        Cloud OSS etc.We tried our best to test against those systems in our continuous
        integrationwhenever possible. Some tests such as Prometheus, Kafka, and Igniteare
        done with live systems, meaning we install Prometheus/Kafka/Ignite on CI machine
        beforethe test is run. Some tests such as Kinesis, PubSub, and Azure Storage
        are donethrough official or non-official emulators. Offline tests are also
        performed wheneverpossible, though systems covered through offine tests may
        not have the samelevel of coverage as live systems or emulators.|  | Live
        System | Emulator| CI Integration |  Offline || ------- | ----- | ----- |
        ----- | ----- || Apache Kafka | :heavy_check_mark:  | | :heavy_check_mark:|
        || Apache Ignite |  :heavy_check_mark: | |:heavy_check_mark:| || Prometheus
        |  :heavy_check_mark: | |:heavy_check_mark:| || Google PubSub |   | :heavy_check_mark:
        |:heavy_check_mark:| || Azure Storage |   | :heavy_check_mark: |:heavy_check_mark:|
        || AWS Kinesis |   | :heavy_check_mark: |:heavy_check_mark:| || Alibaba Cloud
        OSS |   | | |  :heavy_check_mark: || Google BigTable/BigQuery |   | to be
        added | | || Elasticsearch (experimental) |  :heavy_check_mark: | |:heavy_check_mark:|
        || MongoDB (experimental) |  :heavy_check_mark: | |:heavy_check_mark:| |References
        for emulators:- Official [PubSub Emulator](https://cloud.google.com/sdk/gcloud/reference/beta/emulators/pubsub/)
        by Google Cloud for Cloud PubSub.- Official [Azurite Emulator](https://github.com/Azure/Azurite)
        by Azure for Azure Storage.- None-official [LocalStack emulator](https://github.com/localstack/localstack)
        by LocalStack for AWS Kinesis.## Community* SIG IO [Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/io)
        and mailing list: [io@tensorflow.org](io@tensorflow.org)* SIG IO [Monthly
        Meeting Notes](https://docs.google.com/document/d/1CB51yJxns5WA4Ylv89D-a5qReiGTC0GYum6DU-9nKGo/edit)*
        Gitter room: [tensorflow/sig-io](https://gitter.im/tensorflow/sig-io)## Additional
        Information* [Streaming Machine Learning with Tiered Storage and Without a
        Data Lake](https://www.confluent.io/blog/streaming-machine-learning-with-tiered-storage/)
        - [Kai Waehner](https://github.com/kaiwaehner)* [TensorFlow with Apache Arrow
        Datasets](https://medium.com/tensorflow/tensorflow-with-apache-arrow-datasets-cdbcfe80a59f)
        - [Bryan Cutler](https://github.com/BryanCutler)* [How to build a custom Dataset
        for Tensorflow](https://towardsdatascience.com/how-to-build-a-custom-dataset-for-tensorflow-1fe3967544d8)
        - [Ivelin Ivanov](https://github.com/ivelin)* [TensorFlow on Apache Ignite](https://medium.com/tensorflow/tensorflow-on-apache-ignite-99f1fc60efeb)
        - [Anton Dmitriev](https://github.com/dmitrievanthony)## License[Apache License
        2.0](LICENSE)'
      Package: tensorflow-io-gcs-filesystem
      Source: pip
      Version: 0.36.0
      Hash: ''
      licenses:
      - Apache-2.0
      Title: tensorflow-io-gcs-filesystem
      DownloadURL: https://github.com/tensorflow/io/archive/v0.36.0.tar.gz
  bazaar:
    register: 'no'
    prim: 6/CTX1033706
    community_link: https://github.com/tensorflow/io
    community_name: https://github.com/tensorflow/io
    community_url: https://github.com/tensorflow/io
    component_comment: ''
    component_highlevel_description: Dataset, streaming, and file system extensions maintained by TensorFlow SIG-IO
    component_name: tensorflow-io
    component_platform: linux
    component_programing_language: ''
    component_version: V0.36.0
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/tensorflow/io/archive/v0.36.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1082929&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: tensorflow-io-gcs-filesystem
    target_sw: linux
    vendor: pip
    version: 0.36.0
    web_url: https://github.com/tensorflow/io
  licenses:
  - Apache-2.0
  name: tensorflow-io-gcs-filesystem
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 0.36.0
  mimer:
    linking: Static
    product_number: CTX1033706
    product_version_label: v0.36.0
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: termcolor+2.4.0
  additional_info:
    fossa-attribution:
      Description: '# termcolor[![PyPI version](https://img.shields.io/pypi/v/termcolor.svg?logo=pypi&logoColor=FFE873)](https://pypi.org/project/termcolor)[![Supported
        Python versions](https://img.shields.io/pypi/pyversions/termcolor.svg?logo=python&logoColor=FFE873)](https://pypi.org/project/termcolor)[![PyPI
        downloads](https://img.shields.io/pypi/dm/termcolor.svg)](https://pypistats.org/packages/termcolor)[![GitHub
        Actions status](https://github.com/termcolor/termcolor/workflows/Test/badge.svg)](https://github.com/termcolor/termcolor/actions)[![Codecov](https://codecov.io/gh/termcolor/termcolor/branch/main/graph/badge.svg)](https://codecov.io/gh/termcolor/termcolor)[![Licence](https://img.shields.io/github/license/termcolor/termcolor.svg)](COPYING.txt)[![Code
        style: Black](https://img.shields.io/badge/code%20style-Black-000000.svg)](https://github.com/psf/black)[![Tidelift](https://tidelift.com/badges/package/pypi/termcolor)](https://tidelift.com/subscription/pkg/pypi-termcolor?utm_source=pypi-termcolor&utm_medium=referral&utm_campaign=readme)##
        Installation### From PyPI```bashpython3 -m pip install --upgrade termcolor```###
        From source```bashgit clone https://github.com/termcolor/termcolorcd termcolorpython3
        -m pip install .```### DemoTo see demo output, run:```bashpython3 -m termcolor```##
        Example```pythonimport sysfrom termcolor import colored, cprinttext = colored("Hello,
        World!", "red", attrs=["reverse", "blink"])print(text)cprint("Hello, World!",
        "green", "on_red")print_red_on_cyan = lambda x: cprint(x, "red", "on_cyan")print_red_on_cyan("Hello,
        World!")print_red_on_cyan("Hello, Universe!")for i in range(10):    cprint(i,
        "magenta", end=" ")cprint("Attention!", "red", attrs=["bold"], file=sys.stderr)```##
        Text properties| Text colors     | Text highlights    | Attributes  || ---------------
        | ------------------ | ----------- || `black`         | `on_black`         |
        `bold`      || `red`           | `on_red`           | `dark`      || `green`         |
        `on_green`         | `underline` || `yellow`        | `on_yellow`        |
        `blink`     || `blue`          | `on_blue`          | `reverse`   || `magenta`       |
        `on_magenta`       | `concealed` || `cyan`          | `on_cyan`          |             ||
        `white`         | `on_white`         |             || `light_grey`    | `on_light_grey`    |             ||
        `dark_grey`     | `on_dark_grey`     |             || `light_red`     | `on_light_red`     |             ||
        `light_green`   | `on_light_green`   |             || `light_yellow`  | `on_light_yellow`  |             ||
        `light_blue`    | `on_light_blue`    |             || `light_magenta` | `on_light_magenta`
        |             || `light_cyan`    | `on_light_cyan`    |             |## Terminal
        properties| Terminal     | bold    | dark | underline | blink      | reverse
        | concealed || ------------ | ------- | ---- | --------- | ---------- | -------
        | --------- || xterm        | yes     | no   | yes       | bold       | yes     |
        yes       || linux        | yes     | yes  | bold      | yes        | yes     |
        no        || rxvt         | yes     | no   | yes       | bold/black | yes     |
        no        || dtterm       | yes     | yes  | yes       | reverse    | yes     |
        yes       || teraterm     | reverse | no   | yes       | rev/red    | yes     |
        no        || aixterm      | normal  | no   | yes       | no         | yes     |
        yes       || PuTTY        | color   | no   | yes       | no         | yes     |
        no        || Windows      | no      | no   | no        | no         | yes     |
        no        || Cygwin SSH   | yes     | no   | color     | color      | color   |
        yes       || Mac Terminal | yes     | no   | yes       | yes        | yes     |
        yes       |## OverridesTerminal colour detection can be disabled or enabled
        in several ways.In order of precedence:1. Calling `colored` or `cprint` with
        a truthy `no_color` disables colour.2. Calling `colored` or `cprint` with
        a truthy `force_color` forces colour.3. Setting the `ANSI_COLORS_DISABLED`
        environment variable to any value disables colour.4. Setting the [`NO_COLOR`](https://no-color.org/)
        environment variable to any value   disables colour.5. Setting the [`FORCE_COLOR`](https://force-color.org/)
        environment variable to any   value forces colour.6. Setting the `TERM` environment
        variable to `dumb`, or using such a   [dumb terminal](https://en.wikipedia.org/wiki/Computer_terminal#Character-oriented_terminal),   disables
        colour.7. Finally, termcolor will attempt to detect whether the terminal supports
        colour.'
      Package: termcolor
      Source: pip
      Version: 2.4.0
      Hash: ''
      licenses:
      - MIT
      Title: termcolor
      DownloadURL: https://files.pythonhosted.org/packages/10/56/d7d66a84f96d804155f6ff2873d065368b25a07222a6fd51c4f24ef6d764/termcolor-2.4.0.tar.gz
  bazaar:
    register: 'no'
    prim: 6/CAX1056953
    community_link: https://github.com/termcolor/termcolor
    community_name: https://github.com/termcolor/termcolor
    community_url: https://github.com/termcolor/termcolor
    component_comment: ''
    component_highlevel_description: ANSII Color formatting for output in terminal.
    component_name: termcolor
    component_platform: linux
    component_programing_language: Python
    component_version: 2.4.0
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://github.com/termcolor/termcolor/archive/2.4.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1082932&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: termcolor
    target_sw: linux
    vendor: pip
    version: 2.4.0
    web_url: https://pypi.org/project/termcolor/2.4.0/
  licenses:
  - MIT
  name: termcolor
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 2.4.0
  mimer:
    linking: Static
    product_number: CAX1056953
    product_version_label: 2.4.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: tokenizers+0.15.2
  additional_info:
    fossa-attribution:
      Description: '<p align="center">    <br>    <img src="https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png"
        width="600"/>    <br><p><p align="center">    <a href="https://badge.fury.io/py/tokenizers">         <img
        alt="Build" src="https://badge.fury.io/py/tokenizers.svg">    </a>    <a href="https://github.com/huggingface/tokenizers/blob/master/LICENSE">        <img
        alt="GitHub" src="https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue">    </a></p><br>#
        TokenizersProvides an implementation of today''s most used tokenizers, with
        a focus on performance andversatility.Bindings over the [Rust](https://github.com/huggingface/tokenizers/tree/master/tokenizers)
        implementation.If you are interested in the High-level design, you can go
        check it there.Otherwise, let''s dive in!## Main features: - Train new vocabularies
        and tokenize using 4 pre-made tokenizers (Bert WordPiece and the 3   most
        common BPE versions). - Extremely fast (both training and tokenization), thanks
        to the Rust implementation. Takes   less than 20 seconds to tokenize a GB
        of text on a server''s CPU. - Easy to use, but also extremely versatile. -
        Designed for research and production. - Normalization comes with alignments
        tracking. It''s always possible to get the part of the   original sentence
        that corresponds to a given token. - Does all the pre-processing: Truncate,
        Pad, add the special tokens your model needs.### Installation#### With pip:```bashpip
        install tokenizers```#### From sources:To use this method, you need to have
        the Rust installed:```bash# Install with:curl https://sh.rustup.rs -sSf |
        sh -s -- -yexport PATH="$HOME/.cargo/bin:$PATH"```Once Rust is installed,
        you can compile doing the following```bashgit clone https://github.com/huggingface/tokenizerscd
        tokenizers/bindings/python# Create a virtual env (you can use yours as well)python
        -m venv .envsource .env/bin/activate# Install `tokenizers` in the current
        virtual envpip install -e .```### Load a pretrained tokenizer from the Hub```pythonfrom
        tokenizers import Tokenizertokenizer = Tokenizer.from_pretrained("bert-base-cased")```###
        Using the provided TokenizersWe provide some pre-build tokenizers to cover
        the most common cases. You can easily load one ofthese using some `vocab.json`
        and `merges.txt` files:```pythonfrom tokenizers import CharBPETokenizer# Initialize
        a tokenizervocab = "./path/to/vocab.json"merges = "./path/to/merges.txt"tokenizer
        = CharBPETokenizer(vocab, merges)# And then encode:encoded = tokenizer.encode("I
        can feel the magic, can you?")print(encoded.ids)print(encoded.tokens)```And
        you can train them just as simply:```pythonfrom tokenizers import CharBPETokenizer#
        Initialize a tokenizertokenizer = CharBPETokenizer()# Then train it!tokenizer.train([
        "./path/to/files/1.txt", "./path/to/files/2.txt" ])# Now, let''s use it:encoded
        = tokenizer.encode("I can feel the magic, can you?")# And finally save it
        somewheretokenizer.save("./path/to/directory/my-bpe.tokenizer.json")```####
        Provided Tokenizers - `CharBPETokenizer`: The original BPE - `ByteLevelBPETokenizer`:
        The byte level version of the BPE - `SentencePieceBPETokenizer`: A BPE implementation
        compatible with the one used by SentencePiece - `BertWordPieceTokenizer`:
        The famous Bert tokenizer, using WordPieceAll of these can be used and trained
        as explained above!### Build your ownWhenever these provided tokenizers don''t
        give you enough freedom, you can build your own tokenizer,by putting all the
        different parts you need together.You can check how we implemented the [provided
        tokenizers](https://github.com/huggingface/tokenizers/tree/master/bindings/python/py_src/tokenizers/implementations)
        and adapt them easily to your own needs.#### Building a byte-level BPEHere
        is an example showing how to build your own byte-level BPE by putting all
        the different piecestogether, and then saving it to a single file:```pythonfrom
        tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors#
        Initialize a tokenizertokenizer = Tokenizer(models.BPE())# Customize pre-tokenization
        and decodingtokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)tokenizer.decoder
        = decoders.ByteLevel()tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)#
        And then traintrainer = trainers.BpeTrainer(    vocab_size=20000,    min_frequency=2,    initial_alphabet=pre_tokenizers.ByteLevel.alphabet())tokenizer.train([    "./path/to/dataset/1.txt",    "./path/to/dataset/2.txt",    "./path/to/dataset/3.txt"],
        trainer=trainer)# And Save ittokenizer.save("byte-level-bpe.tokenizer.json",
        pretty=True)```Now, when you want to use this tokenizer, this is as simple
        as:```pythonfrom tokenizers import Tokenizertokenizer = Tokenizer.from_file("byte-level-bpe.tokenizer.json")encoded
        = tokenizer.encode("I can feel the magic, can you?")```'
      Package: tokenizers
      Source: pip
      Version: 0.15.2
      Hash: ''
      licenses:
      - Apache-2.0
      Title: tokenizers
      DownloadURL: https://files.pythonhosted.org/packages/c0/44/625db94e91c6196b6574359fa70bfe28e8eabf57a1b894f8f0ec69727fd1/tokenizers-0.15.2.tar.gz
  bazaar:
    register: 'no'
    prim: 4/CTX1031949
    community_link: https://pypi.org/project/tokenizers/
    community_name: https://pypi.org/project/tokenizers/
    community_url: https://pypi.org/project/tokenizers/
    component_comment: ''
    component_highlevel_description: ???? Fast State-of-the-Art Tokenizers optimized
      for Research and Production
    component_name: tokenizers
    component_platform: linux
    component_programing_language: Rust
    component_version: 0.15.2
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://files.pythonhosted.org/packages/c0/44/625db94e91c6196b6574359fa70bfe28e8eabf57a1b894f8f0ec69727fd1/tokenizers-0.15.2.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078658&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Rust
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: tokenizers
    target_sw: linux
    vendor: pip
    version: 0.15.2
    web_url: https://pypi.org/project/tokenizers/0.15.2/
  licenses:
  - Apache-2.0
  name: tokenizers
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 0.15.2
  mimer:
    linking: Static
    product_number: CTX1031949
    product_version_label: 0.15.2
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: torch+2.2.1
  additional_info:
    fossa-attribution:
      Description: "![PyTorch Logo](https://github.com/pytorch/pytorch/blob/main/docs/source/_static/img/pytorch-logo-dark.png)--------------------------------------------------------------------------------PyTorch
        is a Python package that provides two high-level features:- Tensor computation
        (like NumPy) with strong GPU acceleration- Deep neural networks built on a
        tape-based autograd systemYou can reuse your favorite Python packages such
        as NumPy, SciPy, and Cython to extend PyTorch when needed.Our trunk health
        (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).<!--
        toc -->- [More About PyTorch](#more-about-pytorch)  - [A GPU-Ready Tensor
        Library](#a-gpu-ready-tensor-library)  - [Dynamic Neural Networks: Tape-Based
        Autograd](#dynamic-neural-networks-tape-based-autograd)  - [Python First](#python-first)
        \ - [Imperative Experiences](#imperative-experiences)  - [Fast and Lean](#fast-and-lean)
        \ - [Extensions Without Pain](#extensions-without-pain)- [Installation](#installation)
        \ - [Binaries](#binaries)    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)
        \ - [From Source](#from-source)    - [Prerequisites](#prerequisites)    -
        [Install Dependencies](#install-dependencies)    - [Get the PyTorch Source](#get-the-pytorch-source)
        \   - [Install PyTorch](#install-pytorch)      - [Adjust Build Options (Optional)](#adjust-build-options-optional)
        \ - [Docker Image](#docker-image)    - [Using pre-built images](#using-pre-built-images)
        \   - [Building the image yourself](#building-the-image-yourself)  - [Building
        the Documentation](#building-the-documentation)  - [Previous Versions](#previous-versions)-
        [Getting Started](#getting-started)- [Resources](#resources)- [Communication](#communication)-
        [Releases and Contributing](#releases-and-contributing)- [The Team](#the-team)-
        [License](#license)<!-- tocstop -->## More About PyTorch[Learn the basics
        of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)At a
        granular level, PyTorch is a library that consists of the following components:|
        Component | Description || ---- | --- || [**torch**](https://pytorch.org/docs/stable/torch.html)
        | A Tensor library like NumPy, with strong GPU support || [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html)
        | A tape-based automatic differentiation library that supports all differentiable
        Tensor operations in torch || [**torch.jit**](https://pytorch.org/docs/stable/jit.html)
        | A compilation stack (TorchScript) to create serializable and optimizable
        models from PyTorch code  || [**torch.nn**](https://pytorch.org/docs/stable/nn.html)
        | A neural networks library deeply integrated with autograd designed for maximum
        flexibility || [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html)
        | Python multiprocessing, but with magical memory sharing of torch Tensors
        across processes. Useful for data loading and Hogwild training || [**torch.utils**](https://pytorch.org/docs/stable/data.html)
        | DataLoader and other utility functions for convenience |Usually, PyTorch
        is used either as:- A replacement for NumPy to use the power of GPUs.- A deep
        learning research platform that provides maximum flexibility and speed.Elaborating
        Further:### A GPU-Ready Tensor LibraryIf you use NumPy, then you have used
        Tensors (a.k.a. ndarray).![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)PyTorch
        provides Tensors that can live either on the CPU or the GPU and accelerates
        thecomputation by a huge amount.We provide a wide variety of tensor routines
        to accelerate and fit your scientific computation needssuch as slicing, indexing,
        mathematical operations, linear algebra, reductions.And they are fast!###
        Dynamic Neural Networks: Tape-Based AutogradPyTorch has a unique way of building
        neural networks: using and replaying a tape recorder.Most frameworks such
        as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.One
        has to build a neural network and reuse the same structure again and again.Changing
        the way the network behaves means that one has to start from scratch.With
        PyTorch, we use a technique called reverse-mode auto-differentiation, which
        allows you tochange the way your network behaves arbitrarily with zero lag
        or overhead. Our inspiration comesfrom several research papers on this topic,
        as well as current and past work such as[torch-autograd](https://github.com/twitter/torch-autograd),[autograd](https://github.com/HIPS/autograd),[Chainer](https://chainer.org),
        etc.While this technique is not unique to PyTorch, it's one of the fastest
        implementations of it to date.You get the best of speed and flexibility for
        your crazy research.![Dynamic graph](https://github.com/pytorch/pytorch/blob/main/docs/source/_static/img/dynamic_graph.gif)###
        Python FirstPyTorch is not a Python binding into a monolithic C++ framework.It
        is built to be deeply integrated into Python.You can use it naturally like
        you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/)
        / [scikit-learn](https://scikit-learn.org) etc.You can write your new neural
        network layers in Python itself, using your favorite librariesand use packages
        such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).Our
        goal is to not reinvent the wheel where appropriate.### Imperative ExperiencesPyTorch
        is designed to be intuitive, linear in thought, and easy to use.When you execute
        a line of code, it gets executed. There isn't an asynchronous view of the
        world.When you drop into a debugger or receive error messages and stack traces,
        understanding them is straightforward.The stack trace points to exactly where
        your code was defined.We hope you never spend hours debugging your code because
        of bad stack traces or asynchronous and opaque execution engines.### Fast
        and LeanPyTorch has minimal framework overhead. We integrate acceleration
        librariessuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn),
        [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.At the core,
        its CPU and GPU Tensor and neural network backendsare mature and have been
        tested for years.Hence, PyTorch is quite fast \u2014 whether you run small
        or large neural networks.The memory usage in PyTorch is extremely efficient
        compared to Torch or some of the alternatives.We've written custom memory
        allocators for the GPU to make sure thatyour deep learning models are maximally
        memory efficient.This enables you to train bigger deep learning models than
        before.### Extensions Without PainWriting new neural network modules, or interfacing
        with PyTorch's Tensor API was designed to be straightforwardand with minimal
        abstractions.You can write new neural network layers in Python using the torch
        API[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).If
        you want to write your layers in C/C++, we provide a convenient extension
        API that is efficient and with minimal boilerplate.No wrapper code needs to
        be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html)
        and [an example here](https://github.com/pytorch/extension-cpp).## Installation###
        BinariesCommands to install binaries via Conda or pip wheels are on our website:
        [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)####
        NVIDIA Jetson PlatformsPython wheels for NVIDIA's Jetson Nano, Jetson TX1/TX2,
        Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048)
        and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)They
        require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv)
        and [@ptrblck](https://github.com/ptrblck) are maintaining them.### From Source####
        PrerequisitesIf you are installing from source, you will need:- Python 3.8
        or later (for Linux, Python 3.8.1+ is needed)- A compiler that fully supports
        C++17, such as clang or gcc (gcc 9.4.0 or newer is required)We highly recommend
        installing an [Anaconda](https://www.anaconda.com/download) environment. You
        will get a high-quality BLAS library (MKL) and you get controlled dependency
        versions regardless of your Linux distro.If you want to compile with CUDA
        support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/),
        then install the following:- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)-
        [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v7 or above- [Compiler](https://gist.github.com/ax3l/9489132)
        compatible with CUDANote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf)
        for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA
        hardwareIf you want to disable CUDA support, export the environment variable
        `USE_CUDA=0`.Other potentially useful environment variables may be found in
        `setup.py`.If you are building for NVIDIA's Jetson platforms (Jetson Nano,
        TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are
        [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)If
        you want to compile with ROCm support, install- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html)
        4.0 and above installation- ROCm is currently supported only for Linux systems.If
        you want to disable ROCm support, export the environment variable `USE_ROCM=0`.Other
        potentially useful environment variables may be found in `setup.py`.#### Install
        Dependencies**Common**```bashconda install cmake ninja# Run this command from
        the PyTorch directory after cloning the source code using the \u201CGet the
        PyTorch Source\u201C section belowpip install -r requirements.txt```**On Linux**```bashconda
        install mkl mkl-include# CUDA only: Add LAPACK support for the GPU if neededconda
        install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA
        version from https://anaconda.org/pytorch/repo# (optional) If using torch.compile
        with inductor/triton, install the matching version of triton# Run from the
        pytorch directory after cloningmake triton```**On MacOS**```bash# Add this
        package on intel x86 processor machines onlyconda install mkl mkl-include#
        Add these packages if torch.distributed is neededconda install pkg-config
        libuv```**On Windows**```bashconda install mkl mkl-include# Add these packages
        if torch.distributed is needed.# Distributed package support on Windows is
        a prototype feature and is subject to changes.conda install -c conda-forge
        libuv=1.39```#### Get the PyTorch Source```bashgit clone --recursive https://github.com/pytorch/pytorchcd
        pytorch# if you are updating an existing checkoutgit submodule syncgit submodule
        update --init --recursive```#### Install PyTorch**On Linux**If you would like
        to compile PyTorch with [new C++ ABI](https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html)
        enabled, then first run this command:```bashexport _GLIBCXX_USE_CXX11_ABI=1```If
        you're compiling for AMD ROCm then first run this command:```bash# Only run
        this if you're compiling for ROCmpython tools/amd_build/build_amd.py```Install
        PyTorch```bashexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which
        conda))/../\"}python setup.py develop```> _Aside:_ If you are using [Anaconda](https://www.anaconda.com/distribution/#download-section),
        you may experience an error caused by the linker:>> ```plaintext> build/temp.linux-x86_64-3.7/torch/csrc/stub.o:
        file not recognized: file format not recognized> collect2: error: ld returned
        1 exit status> error: command 'g++' failed with exit status 1> ```>> This
        is caused by `ld` from the Conda environment shadowing the system `ld`. You
        should use a newer version of Python that fixes this issue. The recommended
        Python version is 3.8.1+.**On macOS**```bashpython3 setup.py develop```**On
        Windows**Choose Correct Visual Studio Version.PyTorch CI uses Visual C++ BuildTools,
        which come with Visual Studio Enterprise,Professional, or Community Editions.
        You can also install the build tools fromhttps://visualstudio.microsoft.com/visual-cpp-build-tools/.
        The build tools *do not*come with Visual Studio Code by default.If you want
        to build legacy python code, please refer to [Building on legacy code and
        CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)**CPU-only
        builds**In this mode PyTorch computations will run on your CPU, not your GPU```cmdconda
        activatepython setup.py develop```Note on OpenMP: The desired OpenMP implementation
        is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually
        download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH`
        and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source)
        is an example for setting up both MKL and Intel OpenMP. Without these configurations
        for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.**CUDA based
        build**In this mode PyTorch computations will leverage your GPU via CUDA for
        faster number crunching[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm)
        is needed to build Pytorch with CUDA.NVTX is a part of CUDA distributive,
        where it is called \"Nsight Compute\". To install it onto an already installed
        CUDA run CUDA installation once again and check the corresponding checkbox.Make
        sure that CUDA with Nsight Compute is installed after Visual Studio.Currently,
        VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe`
        is detected in `PATH`, then Ninja will be used as the default generator, otherwise,
        it will use VS 2017 / 2019.<br/> If Ninja is selected as the generator, the
        latest MSVC will get selected as the underlying toolchain.Additional libraries
        such as[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN
        or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache)
        are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers)
        to install them.You can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat)
        script for some other environment variables configurations```cmdcmd:: Set
        the environment variables after you have downloaded and unzipped the mkl package,::
        else CMake would throw an error as `Could NOT find OpenMP`.set CMAKE_INCLUDE_PATH={Your
        directory}\\mkl\\includeset LIB={Your directory}\\mkl\\lib;%LIB%:: Read the
        content in the previous section carefully before you proceed.:: [Optional]
        If you want to override the underlying toolset used by Ninja and Visual Studio
        with CUDA, please run the following script block.:: \"Visual Studio 2019 Developer
        Command Prompt\" will be run automatically.:: Make sure you have CMake >=
        3.12 before you do this when you use the Visual Studio generator.set CMAKE_GENERATOR_TOOLSET_VERSION=14.27set
        DISTUTILS_USE_SDK=1for /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft
        Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest
        -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\"
        x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%:: [Optional] If you want
        to override the CUDA host compilerset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft
        Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exepython
        setup.py develop```##### Adjust Build Options (Optional)You can adjust the
        configuration of cmake variables optionally (without building first), by doingthe
        following. For example, adjusting the pre-detected directories for CuDNN or
        BLAS can be donewith such a step.On Linux```bashexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname
        $(which conda))/../\"}python setup.py build --cmake-onlyccmake build  # or
        cmake-gui build```On macOS```bashexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname
        $(which conda))/../\"}MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python
        setup.py build --cmake-onlyccmake build  # or cmake-gui build```### Docker
        Image#### Using pre-built imagesYou can also pull a pre-built docker image
        from Docker Hub and run with docker v19.03+```bashdocker run --gpus all --rm
        -ti --ipc=host pytorch/pytorch:latest```Please note that PyTorch uses shared
        memory to share data between processes, so if torch multiprocessing is used
        (e.g.for multithreaded data loaders) the default shared memory segment size
        that container runs with is not enough, and youshould increase shared memory
        size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker
        run`.#### Building the image yourself**NOTE:** Must be built with a docker
        version > 18.06The `Dockerfile` is supplied to build images with CUDA 11.1
        support and cuDNN v8.You can pass `PYTHON_VERSION=x.y` make variable to specify
        which Python version is to be used by Miniconda, or leave itunset to use the
        default.```bashmake -f docker.Makefile# images are tagged as docker.io/${your_docker_username}/pytorch```You
        can also pass the `CMAKE_VARS=\"...\"` environment variable to specify additional
        CMake variables to be passed to CMake during the build.See [setup.py](./setup.py)
        for the list of available variables.```bashCMAKE_VARS=\"BUILD_CAFFE2=ON BUILD_CAFFE2_OPS=ON\"
        make -f docker.Makefile```### Building the DocumentationTo build documentation
        in various formats, you will need [Sphinx](http://www.sphinx-doc.org) and
        thereadthedocs theme.```bashcd docs/pip install -r requirements.txt```You
        can then build the documentation by running `make <format>` from the`docs/`
        folder. Run `make` to get a list of all available output formats.If you get
        a katex error run `npm install katex`.  If it persists, try`npm install -g
        katex`> Note: if you installed `nodejs` with a different package manager (e.g.,`conda`)
        then `npm` will probably install a version of `katex` that is notcompatible
        with your version of `nodejs` and doc builds will fail.A combination of versions
        that is known to work is `node@6.13.1` and`katex@0.13.18`. To install the
        latter with `npm` you can run```npm install -g katex@0.13.18```### Previous
        VersionsInstallation instructions and binaries for previous PyTorch versions
        may be foundon [our website](https://pytorch.org/previous-versions).## Getting
        StartedThree-pointers to get you started:- [Tutorials: get you started with
        understanding and using PyTorch](https://pytorch.org/tutorials/)- [Examples:
        easy to understand PyTorch code across all domains](https://github.com/pytorch/examples)-
        [The API Reference](https://pytorch.org/docs/)- [Glossary](https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md)##
        Resources* [PyTorch.org](https://pytorch.org/)* [PyTorch Tutorials](https://pytorch.org/tutorials/)*
        [PyTorch Examples](https://github.com/pytorch/examples)* [PyTorch Models](https://pytorch.org/hub/)*
        [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)*
        [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)*
        [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)*
        [PyTorch Twitter](https://twitter.com/PyTorch)* [PyTorch Blog](https://pytorch.org/blog/)*
        [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)##
        Communication* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org*
        GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts,
        etc.* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary
        audience of moderate to experienced PyTorch users and developers for general
        chat, online discussions, collaboration, etc. If you are a beginner looking
        for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org).
        If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1*
        Newsletter: No-noise, a one-way email newsletter with important announcements
        about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv* Facebook Page:
        Important announcements about PyTorch. https://www.facebook.com/pytorch* For
        brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)##
        Releases and ContributingTypically, PyTorch has three minor releases a year.
        Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).We
        appreciate all contributions. If you are planning to contribute back bug-fixes,
        please do so without any further discussion.If you plan to contribute new
        features, utility functions, or extensions to the core, please first open
        an issue and discuss the feature with us.Sending a PR without discussion might
        end up resulting in a rejected PR because we might be taking the core in a
        different direction than you might be aware of.To learn more about making
        a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md).
        For more information about PyTorch releases, see [Release page](RELEASE.md).##
        The TeamPyTorch is a community-driven project with several skillful engineers
        and researchers contributing to it.PyTorch is currently maintained by [Soumith
        Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan),
        [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang),
        and [Nikita Shulga](https://github.com/malfet) with major contributions coming
        from hundreds of talented individuals in various forms and means.A non-exhaustive
        but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey
        Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban
        Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume
        Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison,
        Edward Yang, Zachary Devito.Note: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch)
        with the same name. Hugh is a valuable contributor to the Torch community
        and has helped with many things Torch and PyTorch.## LicensePyTorch has a
        BSD-style license, as found in the [LICENSE](LICENSE) file."
      Package: torch
      Source: pip
      Version: 2.2.1
      Hash: ''
      licenses:
      - 0BSD
      - Apache-2.0
      - BSD-2-Clause
      - BSD-3-Clause
      - BSL-1.0
      - GPL-3.0-only
      - MIT
      - NCSA
      - PIL
      - Protobuf
      - Zlib
      - apache-2.0 WITH llvm-exception
      - bsd-3-clause-no-trademark
      - bzip2-1.0.6
      Title: torch
      DownloadURL: https://github.com/pytorch/pytorch/archive/refs/tags/v2.2.1.zip
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/pytorch/pytorch
    community_name: https://github.com/pytorch/pytorch
    community_url: https://github.com/pytorch/pytorch
    component_comment: ''
    component_highlevel_description: ''
    component_name: torch
    component_platform: linux
    component_programing_language: ''
    component_version: v2.2.1
    licenses: []
    src_download_link: https://github.com/pytorch/pytorch/archive/refs/tags/v2.2.1.zip
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: torch
    target_sw: linux
    vendor: pip
    version: 2.2.1
    web_url: https://pytorch.org/
  licenses:
  - 0BSD
  - Apache-2.0
  - BSD-2-Clause
  - BSD-3-Clause
  - BSL-1.0
  - GPL-3.0-only
  - MIT
  - NCSA
  - PIL
  - Protobuf
  - Zlib
  - apache-2.0 WITH llvm-exception
  - bsd-3-clause-no-trademark
  - bzip2-1.0.6
  name: torch
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 2.2.1
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 2.2.1
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: tqdm+4.66.2
  additional_info:
    fossa-attribution:
      Description: "|Logo|tqdm====|Py-Versions| |Versions| |Conda-Forge-Status| |Docker|
        |Snapcraft||Build-Status| |Coverage-Status| |Branch-Coverage-Status| |Codacy-Grade|
        |Libraries-Rank| |PyPI-Downloads||LICENCE| |OpenHub-Status| |binder-demo|
        |awesome-python|``tqdm`` derives from the Arabic word *taqaddum* (\u062A\u0642\u062F\u0651\u0645)
        which can mean \"progress,\"and is an abbreviation for \"I love you so much\"
        in Spanish (*te quiero demasiado*).Instantly make your loops show a smart
        progress meter - just wrap anyiterable with ``tqdm(iterable)``, and you're
        done!.. code:: python    from tqdm import tqdm    for i in tqdm(range(10000)):
        \       ...``76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588
        \_\_\_\_\_\_ | 7568/10000 [00:33<00:10, 229.00it/s]````trange(N)`` can be
        also used as a convenient shortcut for``tqdm(range(N))``.|Screenshot|    |Video|
        |Slides| |Merch|It can also be executed as a module with pipes:.. code:: sh
        \   $ seq 9999999 | tqdm --bytes | wc -l    75.2MB [00:00, 217MB/s]    9999999
        \   $ tar -zcf - docs/ | tqdm --bytes --total `du -sb docs/ | cut -f1` \\
        \       > backup.tgz     32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258D
        \                     | 8.89G/27.9G [00:42<01:31, 223MB/s]Overhead is low
        -- about 60ns per iteration (80ns with ``tqdm.gui``), and isunit tested against
        performance regression.By comparison, the well-established`ProgressBar <https://github.com/niltonvolpato/python-progressbar>`__
        hasan 800ns/iter overhead.In addition to its low overhead, ``tqdm`` uses smart
        algorithms to predictthe remaining time and to skip unnecessary iteration
        displays, which allowsfor a negligible overhead in most cases.``tqdm`` works
        on any platform(Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS),in any
        console or in a GUI, and is also friendly with IPython/Jupyter notebooks.``tqdm``
        does not require any dependencies (not even ``curses``!), justPython and an
        environment supporting ``carriage return \\r`` and``line feed \\n`` control
        characters.------------------------------------------.. contents:: Table of
        contents   :backlinks: top   :local:Installation------------Latest PyPI stable
        release~~~~~~~~~~~~~~~~~~~~~~~~~~|Versions| |PyPI-Downloads| |Libraries-Dependents|..
        code:: sh    pip install tqdmLatest development release on GitHub~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|GitHub-Status|
        |GitHub-Stars| |GitHub-Commits| |GitHub-Forks| |GitHub-Updated|Pull and install
        pre-release ``devel`` branch:.. code:: sh    pip install \"git+https://github.com/tqdm/tqdm.git@devel#egg=tqdm\"Latest
        Conda release~~~~~~~~~~~~~~~~~~~~|Conda-Forge-Status|.. code:: sh    conda
        install -c conda-forge tqdmLatest Snapcraft release~~~~~~~~~~~~~~~~~~~~~~~~|Snapcraft|There
        are 3 channels to choose from:.. code:: sh    snap install tqdm  # implies
        --stable, i.e. latest tagged release    snap install tqdm  --candidate  #
        master branch    snap install tqdm  --edge  # devel branchNote that ``snap``
        binaries are purely for CLI use (not ``import``-able), andautomatically set
        up ``bash`` tab-completion.Latest Docker release~~~~~~~~~~~~~~~~~~~~~|Docker|..
        code:: sh    docker pull tqdm/tqdm    docker run -i --rm tqdm/tqdm --helpOther~~~~~There
        are other (unofficial) places where ``tqdm`` may be downloaded, particularly
        for CLI use:|Repology|.. |Repology| image:: https://repology.org/badge/tiny-repos/python:tqdm.svg
        \  :target: https://repology.org/project/python:tqdm/versionsChangelog---------The
        list of all changes is available either on GitHub's Releases:|GitHub-Status|,
        on the`wiki <https://github.com/tqdm/tqdm/wiki/Releases>`__, or on the`website
        <https://tqdm.github.io/releases>`__.Usage-----``tqdm`` is very versatile
        and can be used in a number of ways.The three main ones are given below.Iterable-based~~~~~~~~~~~~~~Wrap
        ``tqdm()`` around any iterable:.. code:: python    from tqdm import tqdm    from
        time import sleep    text = \"\"    for char in tqdm([\"a\", \"b\", \"c\",
        \"d\"]):        sleep(0.25)        text = text + char``trange(i)`` is a special
        optimised instance of ``tqdm(range(i))``:.. code:: python    from tqdm import
        trange    for i in trange(100):        sleep(0.01)Instantiation outside of
        the loop allows for manual control over ``tqdm()``:.. code:: python    pbar
        = tqdm([\"a\", \"b\", \"c\", \"d\"])    for char in pbar:        sleep(0.25)
        \       pbar.set_description(\"Processing %s\" % char)Manual~~~~~~Manual control
        of ``tqdm()`` updates using a ``with`` statement:.. code:: python    with
        tqdm(total=100) as pbar:        for i in range(10):            sleep(0.1)
        \           pbar.update(10)If the optional variable ``total`` (or an iterable
        with ``len()``) isprovided, predictive stats are displayed.``with`` is also
        optional (you can just assign ``tqdm()`` to a variable,but in this case don't
        forget to ``del`` or ``close()`` at the end:.. code:: python    pbar = tqdm(total=100)
        \   for i in range(10):        sleep(0.1)        pbar.update(10)    pbar.close()Module~~~~~~Perhaps
        the most wonderful use of ``tqdm`` is in a script or on the commandline. Simply
        inserting ``tqdm`` (or ``python -m tqdm``) between pipes will passthrough
        all ``stdin`` to ``stdout`` while printing progress to ``stderr``.The example
        below demonstrate counting the number of lines in all Python filesin the current
        directory, with timing information included... code:: sh    $ time find .
        -name '*.py' -type f -exec cat \\{} \\; | wc -l    857365    real    0m3.458s
        \   user    0m0.274s    sys     0m3.325s    $ time find . -name '*.py' -type
        f -exec cat \\{} \\; | tqdm | wc -l    857366it [00:03, 246471.31it/s]    857365
        \   real    0m3.585s    user    0m0.862s    sys     0m3.358sNote that the
        usual arguments for ``tqdm`` can also be specified... code:: sh    $ find
        . -name '*.py' -type f -exec cat \\{} \\; |        tqdm --unit loc --unit_scale
        --total 857366 >> /dev/null    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|
        857K/857K [00:04<00:00, 246Kloc/s]Backing up a large directory?.. code:: sh
        \   $ tar -zcf - docs/ | tqdm --bytes --total `du -sb docs/ | cut -f1` \\
        \     > backup.tgz     44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A
        \                  | 153M/352M [00:14<00:18, 11.0MB/s]This can be beautified
        further:.. code:: sh    $ BYTES=\"$(du -sb docs/ | cut -f1)\"    $ tar -cf
        - docs/ \\      | tqdm --bytes --total \"$BYTES\" --desc Processing | gzip
        \\      | tqdm --bytes --total \"$BYTES\" --desc Compressed --position 1 \\
        \     > ~/backup.tgz    Processing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|
        352M/352M [00:14<00:00, 30.2MB/s]    Compressed:  42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258E
        \           | 148M/352M [00:14<00:19, 10.9MB/s]Or done on a file level using
        7-zip:.. code:: sh    $ 7z a -bd -r backup.7z docs/ | grep Compressing \\
        \     | tqdm --total $(find docs/ -type f | wc -l) --unit files \\      |
        grep -v Compressing    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589|
        15327/15327 [01:00<00:00, 712.96files/s]Pre-existing CLI programs already
        outputting basic progress information willbenefit from ``tqdm``'s ``--update``
        and ``--update_to`` flags:.. code:: sh    $ seq 3 0.1 5 | tqdm --total 5 --update_to
        --null    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|
        5.0/5 [00:00<00:00, 9673.21it/s]    $ seq 10 | tqdm --update --null  # 1 +
        2 + ... + 10 = 55 iterations    55it [00:00, 90006.52it/s]FAQ and Known Issues--------------------|GitHub-Issues|The
        most common issues relate to excessive output on multiple lines, insteadof
        a neat one-line progress bar.- Consoles in general: require support for carriage
        return (``CR``, ``\\r``).- Nested progress bars:  * Consoles in general: require
        support for moving cursors up to the    previous line. For example,    `IDLE
        <https://github.com/tqdm/tqdm/issues/191#issuecomment-230168030>`__,    `ConEmu
        <https://github.com/tqdm/tqdm/issues/254>`__ and    `PyCharm <https://github.com/tqdm/tqdm/issues/203>`__
        (also    `here <https://github.com/tqdm/tqdm/issues/208>`__,    `here <https://github.com/tqdm/tqdm/issues/307>`__,
        and    `here <https://github.com/tqdm/tqdm/issues/454#issuecomment-335416815>`__)
        \   lack full support.  * Windows: additionally may require the Python module
        ``colorama``    to ensure nested bars stay within their respective lines.-
        Unicode:  * Environments which report that they support unicode will have
        solid smooth    progressbars. The fallback is an ``ascii``-only bar.  * Windows
        consoles often only partially support unicode and thus    `often require explicit
        ascii=True <https://github.com/tqdm/tqdm/issues/454#issuecomment-335416815>`__
        \   (also `here <https://github.com/tqdm/tqdm/issues/499>`__). This is due
        to    either normal-width unicode characters being incorrectly displayed as
        \   \"wide\", or some unicode characters not rendering.- Wrapping generators:
        \ * Generator wrapper functions tend to hide the length of iterables.    ``tqdm``
        does not.  * Replace ``tqdm(enumerate(...))`` with ``enumerate(tqdm(...))``
        or    ``tqdm(enumerate(x), total=len(x), ...)``.    The same applies to ``numpy.ndenumerate``.
        \ * Replace ``tqdm(zip(a, b))`` with ``zip(tqdm(a), b)`` or even    ``zip(tqdm(a),
        tqdm(b))``.  * The same applies to ``itertools``.  * Some useful convenience
        functions can be found under ``tqdm.contrib``.- `Hanging pipes in python2
        <https://github.com/tqdm/tqdm/issues/359>`__:  when using ``tqdm`` on the
        CLI, you may need to use Python 3.5+ for correct  buffering.- `No intermediate
        output in docker-compose <https://github.com/tqdm/tqdm/issues/771>`__:  use
        ``docker-compose run`` instead of ``docker-compose up`` and ``tty: true``.If
        you come across any other difficulties, browse and file |GitHub-Issues|.Documentation-------------|Py-Versions|
        |README-Hits| (Since 19 May 2016).. code:: python    class tqdm():      \"\"\"
        \     Decorate an iterable object, returning an iterator which acts exactly
        \     like the original iterable, but prints a dynamically updating      progressbar
        every time a value is requested.      \"\"\"      def __init__(self, iterable=None,
        desc=None, total=None, leave=True,                   file=None, ncols=None,
        mininterval=0.1,                   maxinterval=10.0, miniters=None, ascii=None,
        disable=False,                   unit='it', unit_scale=False, dynamic_ncols=False,
        \                  smoothing=0.3, bar_format=None, initial=0, position=None,
        \                  postfix=None, unit_divisor=1000):Parameters~~~~~~~~~~*
        iterable  : iterable, optional      Iterable to decorate with a progressbar.
        \   Leave blank to manually manage the updates.* desc  : str, optional      Prefix
        for the progressbar.* total  : int or float, optional      The number of expected
        iterations. If unspecified,    len(iterable) is used if possible. If float(\"inf\")
        or as a last    resort, only basic progress statistics are displayed    (no
        ETA, no progressbar).    If ``gui`` is True and this parameter needs subsequent
        updating,    specify an initial arbitrary large positive number,    e.g. 9e9.*
        leave  : bool, optional      If [default: True], keeps all traces of the progressbar
        \   upon termination of iteration.    If ``None``, will leave only if ``position``
        is ``0``.* file  : ``io.TextIOWrapper`` or ``io.StringIO``, optional      Specifies
        where to output the progress messages    (default: sys.stderr). Uses ``file.write(str)``
        and ``file.flush()``    methods.  For encoding, see ``write_bytes``.* ncols
        \ : int, optional      The width of the entire output message. If specified,
        \   dynamically resizes the progressbar to stay within this bound.    If unspecified,
        attempts to use environment width. The    fallback is a meter width of 10
        and no limit for the counter and    statistics. If 0, will not print any meter
        (only stats).* mininterval  : float, optional      Minimum progress display
        update interval [default: 0.1] seconds.* maxinterval  : float, optional      Maximum
        progress display update interval [default: 10] seconds.    Automatically adjusts
        ``miniters`` to correspond to ``mininterval``    after long display update
        lag. Only works if ``dynamic_miniters``    or monitor thread is enabled.*
        miniters  : int or float, optional      Minimum progress display update interval,
        in iterations.    If 0 and ``dynamic_miniters``, will automatically adjust
        to equal    ``mininterval`` (more CPU efficient, good for tight loops).    If
        > 0, will skip display of specified number of iterations.    Tweak this and
        ``mininterval`` to get very efficient loops.    If your progress is erratic
        with both fast and slow iterations    (network, skipping items, etc) you should
        set miniters=1.* ascii  : bool or str, optional      If unspecified or False,
        use unicode (smooth blocks) to fill    the meter. The fallback is to use ASCII
        characters \" 123456789#\".* disable  : bool, optional      Whether to disable
        the entire progressbar wrapper    [default: False]. If set to None, disable
        on non-TTY.* unit  : str, optional      String that will be used to define
        the unit of each iteration    [default: it].* unit_scale  : bool or int or
        float, optional      If 1 or True, the number of iterations will be reduced/scaled
        \   automatically and a metric prefix following the    International System
        of Units standard will be added    (kilo, mega, etc.) [default: False]. If
        any other non-zero    number, will scale ``total`` and ``n``.* dynamic_ncols
        \ : bool, optional      If set, constantly alters ``ncols`` and ``nrows``
        to the    environment (allowing for window resizes) [default: False].* smoothing
        \ : float, optional      Exponential moving average smoothing factor for speed
        estimates    (ignored in GUI mode). Ranges from 0 (average speed) to 1    (current/instantaneous
        speed) [default: 0.3].* bar_format  : str, optional      Specify a custom
        bar string formatting. May impact performance.    [default: '{l_bar}{bar}{r_bar}'],
        where    l_bar='{desc}: {percentage:3.0f}%|' and    r_bar='| {n_fmt}/{total_fmt}
        [{elapsed}<{remaining}, '    '{rate_fmt}{postfix}]'    Possible vars: l_bar,
        bar, r_bar, n, n_fmt, total, total_fmt,    percentage, elapsed, elapsed_s,
        ncols, nrows, desc, unit,    rate, rate_fmt, rate_noinv, rate_noinv_fmt,    rate_inv,
        rate_inv_fmt, postfix, unit_divisor,    remaining, remaining_s, eta.    Note
        that a trailing \": \" is automatically removed after {desc}    if the latter
        is empty.* initial  : int or float, optional      The initial counter value.
        Useful when restarting a progress    bar [default: 0]. If using float, consider
        specifying ``{n:.3f}``    or similar in ``bar_format``, or specifying ``unit_scale``.*
        position  : int, optional      Specify the line offset to print this bar (starting
        from 0)    Automatic if unspecified.    Useful to manage multiple bars at
        once (eg, from threads).* postfix  : dict or ``*``, optional      Specify
        additional stats to display at the end of the bar.    Calls ``set_postfix(**postfix)``
        if possible (dict).* unit_divisor  : float, optional      [default: 1000],
        ignored unless ``unit_scale`` is True.* write_bytes  : bool, optional      If
        (default: None) and ``file`` is unspecified,    bytes will be written in Python
        2. If ``True`` will also write    bytes. In all other cases will default to
        unicode.* lock_args  : tuple, optional      Passed to ``refresh`` for intermediate
        output    (initialisation, iterating, and updating).* nrows  : int, optional
        \     The screen height. If specified, hides nested bars outside this    bound.
        If unspecified, attempts to use environment height.    The fallback is 20.*
        colour  : str, optional      Bar colour (e.g. 'green', '#00ff00').* delay
        \ : float, optional      Don't display until [default: 0] seconds have elapsed.Extra
        CLI Options~~~~~~~~~~~~~~~~~* delim  : chr, optional      Delimiting character
        [default: '\\n']. Use '\\0' for null.    N.B.: on Windows systems, Python
        converts '\\n' to '\\r\\n'.* buf_size  : int, optional      String buffer
        size in bytes [default: 256]    used when ``delim`` is specified.* bytes  :
        bool, optional      If true, will count bytes, ignore ``delim``, and default
        \   ``unit_scale`` to True, ``unit_divisor`` to 1024, and ``unit`` to 'B'.*
        tee  : bool, optional      If true, passes ``stdin`` to both ``stderr`` and
        ``stdout``.* update  : bool, optional      If true, will treat input as newly
        elapsed iterations,    i.e. numbers to pass to ``update()``. Note that this
        is slow    (~2e5 it/s) since every input must be decoded as a number.* update_to
        \ : bool, optional      If true, will treat input as total elapsed iterations,
        \   i.e. numbers to assign to ``self.n``. Note that this is slow    (~2e5
        it/s) since every input must be decoded as a number.* null  : bool, optional
        \     If true, will discard input (no stdout).* manpath  : str, optional      Directory
        in which to install tqdm man pages.* comppath  : str, optional      Directory
        in which to place tqdm completion.* log  : str, optional      CRITICAL|FATAL|ERROR|WARN(ING)|[default:
        'INFO']|DEBUG|NOTSET.Returns~~~~~~~* out  : decorated iterator.  .. code::
        python    class tqdm():      def update(self, n=1):          \"\"\"          Manually
        update the progress bar, useful for streams          such as reading files.
        \         E.g.:          >>> t = tqdm(total=filesize) # Initialise          >>>
        for current_buffer in stream:          ...    ...          ...    t.update(len(current_buffer))
        \         >>> t.close()          The last line is highly recommended, but
        possibly not necessary if          ``t.update()`` will be called in such a
        way that ``filesize`` will be          exactly reached and printed.          Parameters
        \         ----------          n  : int or float, optional              Increment
        to add to the internal counter of iterations              [default: 1]. If
        using float, consider specifying ``{n:.3f}``              or similar in ``bar_format``,
        or specifying ``unit_scale``.          Returns          -------          out
        \ : bool or None              True if a ``display()`` was triggered.          \"\"\"
        \     def close(self):          \"\"\"Cleanup and (if leave=False) close the
        progressbar.\"\"\"      def clear(self, nomove=False):          \"\"\"Clear
        current bar display.\"\"\"      def refresh(self):          \"\"\"          Force
        refresh the display of this bar.          Parameters          ----------          nolock
        \ : bool, optional              If ``True``, does not lock.              If
        [default: ``False``]: calls ``acquire()`` on internal lock.          lock_args
        \ : tuple, optional              Passed to internal lock's ``acquire()``.
        \             If specified, will only ``display()`` if ``acquire()`` returns
        ``True``.          \"\"\"      def unpause(self):          \"\"\"Restart tqdm
        timer from last print time.\"\"\"      def reset(self, total=None):          \"\"\"
        \         Resets to 0 iterations for repeated use.          Consider combining
        with ``leave=True``.          Parameters          ----------          total
        \ : int or float, optional. Total to use for the new bar.          \"\"\"
        \     def set_description(self, desc=None, refresh=True):          \"\"\"
        \         Set/modify description of the progress bar.          Parameters
        \         ----------          desc  : str, optional          refresh  : bool,
        optional              Forces refresh [default: True].          \"\"\"      def
        set_postfix(self, ordered_dict=None, refresh=True, **tqdm_kwargs):          \"\"\"
        \         Set/modify postfix (additional stats)          with automatic formatting
        based on datatype.          Parameters          ----------          ordered_dict
        \ : dict or OrderedDict, optional          refresh  : bool, optional              Forces
        refresh [default: True].          kwargs  : dict, optional          \"\"\"
        \     @classmethod      def write(cls, s, file=sys.stdout, end=\"\\n\"):          \"\"\"Print
        a message via tqdm (without overlap with bars).\"\"\"      @property      def
        format_dict(self):          \"\"\"Public API for read-only member access.\"\"\"
        \     def display(self, msg=None, pos=None):          \"\"\"          Use
        ``self.sp`` to display ``msg`` in the specified ``pos``.          Consider
        overloading this function when inheriting to use e.g.:          ``self.some_frontend(**self.format_dict)``
        instead of ``self.sp``.          Parameters          ----------          msg
        \ : str, optional. What to display (default: ``repr(self)``).          pos
        \ : int, optional. Position to ``moveto``            (default: ``abs(self.pos)``).
        \         \"\"\"      @classmethod      @contextmanager      def wrapattr(cls,
        stream, method, total=None, bytes=True, **tqdm_kwargs):          \"\"\"          stream
        \ : file-like object.          method  : str, \"read\" or \"write\". The result
        of ``read()`` and              the first argument of ``write()`` should have
        a ``len()``.          >>> with tqdm.wrapattr(file_obj, \"read\", total=file_obj.size)
        as fobj:          ...     while True:          ...         chunk = fobj.read(chunk_size)
        \         ...         if not chunk:          ...             break          \"\"\"
        \     @classmethod      def pandas(cls, *targs, **tqdm_kwargs):          \"\"\"Registers
        the current `tqdm` class with `pandas`.\"\"\"    def trange(*args, **tqdm_kwargs):
        \       \"\"\"        A shortcut for `tqdm(xrange(*args), **tqdm_kwargs)`.
        \       On Python3+, `range` is used instead of `xrange`.        \"\"\"Convenience
        Functions~~~~~~~~~~~~~~~~~~~~~.. code:: python    def tqdm.contrib.tenumerate(iterable,
        start=0, total=None,                                tqdm_class=tqdm.auto.tqdm,
        **tqdm_kwargs):        \"\"\"Equivalent of `numpy.ndenumerate` or builtin
        `enumerate`.\"\"\"    def tqdm.contrib.tzip(iter1, *iter2plus, **tqdm_kwargs):
        \       \"\"\"Equivalent of builtin `zip`.\"\"\"    def tqdm.contrib.tmap(function,
        *sequences, **tqdm_kwargs):        \"\"\"Equivalent of builtin `map`.\"\"\"Submodules~~~~~~~~~~..
        code:: python    class tqdm.notebook.tqdm(tqdm.tqdm):        \"\"\"IPython/Jupyter
        Notebook widget.\"\"\"    class tqdm.auto.tqdm(tqdm.tqdm):        \"\"\"Automatically
        chooses beween `tqdm.notebook` and `tqdm.tqdm`.\"\"\"    class tqdm.asyncio.tqdm(tqdm.tqdm):
        \     \"\"\"Asynchronous version.\"\"\"      @classmethod      def as_completed(cls,
        fs, *, loop=None, timeout=None, total=None,                       **tqdm_kwargs):
        \         \"\"\"Wrapper for `asyncio.as_completed`.\"\"\"    class tqdm.gui.tqdm(tqdm.tqdm):
        \       \"\"\"Matplotlib GUI version.\"\"\"    class tqdm.tk.tqdm(tqdm.tqdm):
        \       \"\"\"Tkinter GUI version.\"\"\"    class tqdm.rich.tqdm(tqdm.tqdm):
        \       \"\"\"`rich.progress` version.\"\"\"    class tqdm.keras.TqdmCallback(keras.callbacks.Callback):
        \       \"\"\"Keras callback for epoch and batch progress.\"\"\"    class
        tqdm.dask.TqdmCallback(dask.callbacks.Callback):        \"\"\"Dask callback
        for task progress.\"\"\"``contrib``+++++++++++The ``tqdm.contrib`` package
        also contains experimental modules:- ``tqdm.contrib.itertools``: Thin wrappers
        around ``itertools``- ``tqdm.contrib.concurrent``: Thin wrappers around ``concurrent.futures``-
        ``tqdm.contrib.discord``: Posts to `Discord <https://discord.com>`__ bots-
        ``tqdm.contrib.telegram``: Posts to `Telegram <https://telegram.org>`__ bots-
        ``tqdm.contrib.bells``: Automagically enables all optional features  * ``auto``,
        ``pandas``, ``discord``, ``telegram``Examples and Advanced Usage----------------------------
        See the `examples <https://github.com/tqdm/tqdm/tree/master/examples>`__  folder;-
        import the module and run ``help()``;- consult the `wiki <https://github.com/tqdm/tqdm/wiki>`__;
        \ * this has an    `excellent article <https://github.com/tqdm/tqdm/wiki/How-to-make-a-great-Progress-Bar>`__
        \   on how to make a **great** progressbar;- check out the `slides from PyData
        London <https://tqdm.github.io/PyData2019/slides.html>`__, or- run the |binder-demo|.Description
        and additional stats~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Custom information can
        be displayed and updated dynamically on ``tqdm`` barswith the ``desc`` and
        ``postfix`` arguments:.. code:: python    from tqdm import tqdm, trange    from
        random import random, randint    from time import sleep    with trange(10)
        as t:        for i in t:            # Description will be displayed on the
        left            t.set_description('GEN %i' % i)            # Postfix will
        be displayed on the right,            # formatted automatically based on argument's
        datatype            t.set_postfix(loss=random(), gen=randint(1,999), str='h',
        \                         lst=[1, 2])            sleep(0.1)    with tqdm(total=10,
        bar_format=\"{postfix[0]} {postfix[1][value]:>8.2g}\",              postfix=[\"Batch\",
        dict(value=0)]) as t:        for i in range(10):            sleep(0.1)            t.postfix[1][\"value\"]
        = i / 2            t.update()Points to remember when using ``{postfix[...]}``
        in the ``bar_format`` string:- ``postfix`` also needs to be passed as an initial
        argument in a compatible  format, and- ``postfix`` will be auto-converted
        to a string if it is a ``dict``-like  object. To prevent this behaviour, insert
        an extra item into the dictionary  where the key is not a string.Additional
        ``bar_format`` parameters may also be defined by overriding``format_dict``,
        and the bar itself may be modified using ``ascii``:.. code:: python    from
        tqdm import tqdm    class TqdmExtraFormat(tqdm):        \"\"\"Provides a `total_time`
        format parameter\"\"\"        @property        def format_dict(self):            d
        = super(TqdmExtraFormat, self).format_dict            total_time = d[\"elapsed\"]
        * (d[\"total\"] or 0) / max(d[\"n\"], 1)            d.update(total_time=self.format_interval(total_time)
        + \" in total\")            return d    for i in TqdmExtraFormat(          range(9),
        ascii=\" .oO0\",          bar_format=\"{total_time}: {percentage:.0f}%|{bar}{r_bar}\"):
        \       if i == 4:            break.. code::    00:00 in total: 44%|0000.
        \    | 4/9 [00:00<00:00, 962.93it/s]Note that ``{bar}`` also supports a format
        specifier ``[width][type]``.- ``width``  * unspecified (default): automatic
        to fill ``ncols``  * ``int >= 0``: fixed width overriding ``ncols`` logic
        \ * ``int < 0``: subtract from the automatic default- ``type``  * ``a``: ascii
        (``ascii=True`` override)  * ``u``: unicode (``ascii=False`` override)  *
        ``b``: blank (``ascii=\"  \"`` override)This means a fixed bar with right-justified
        text may be created by using:``bar_format=\"{l_bar}{bar:10}|{bar:-10b}right-justified\"``Nested
        progress bars~~~~~~~~~~~~~~~~~~~~``tqdm`` supports nested progress bars. Here's
        an example:.. code:: python    from tqdm.auto import trange    from time import
        sleep    for i in trange(4, desc='1st loop'):        for j in trange(5, desc='2nd
        loop'):            for k in trange(50, desc='3rd loop', leave=False):                sleep(0.01)For
        manual control over positioning (e.g. for multi-processing use),you may specify
        ``position=n`` where ``n=0`` for the outermost bar,``n=1`` for the next, and
        so on.However, it's best to check if ``tqdm`` can work without manual ``position``first...
        code:: python    from time import sleep    from tqdm import trange, tqdm    from
        multiprocessing import Pool, RLock, freeze_support    L = list(range(9))    def
        progresser(n):        interval = 0.001 / (n + 2)        total = 5000        text
        = \"#{}, est. {:<04.2}s\".format(n, interval * total)        for _ in trange(total,
        desc=text, position=n):            sleep(interval)    if __name__ == '__main__':
        \       freeze_support()  # for Windows support        tqdm.set_lock(RLock())
        \ # for managing output contention        p = Pool(initializer=tqdm.set_lock,
        initargs=(tqdm.get_lock(),))        p.map(progresser, L)Note that in Python
        3, ``tqdm.write`` is thread-safe:.. code:: python    from time import sleep
        \   from tqdm import tqdm, trange    from concurrent.futures import ThreadPoolExecutor
        \   L = list(range(9))    def progresser(n):        interval = 0.001 / (n
        + 2)        total = 5000        text = \"#{}, est. {:<04.2}s\".format(n, interval
        * total)        for _ in trange(total, desc=text):            sleep(interval)
        \       if n == 6:            tqdm.write(\"n == 6 completed.\")            tqdm.write(\"`tqdm.write()`
        is thread-safe in py3!\")    if __name__ == '__main__':        with ThreadPoolExecutor()
        as p:            p.map(progresser, L)Hooks and callbacks~~~~~~~~~~~~~~~~~~~``tqdm``
        can easily support callbacks/hooks and manual updates.Here's an example with
        ``urllib``:**``urllib.urlretrieve`` documentation**    | [...]    | If present,
        the hook function will be called once    | on establishment of the network
        connection and once after each block read    | thereafter. The hook will be
        passed three arguments; a count of blocks    | transferred so far, a block
        size in bytes, and the total size of the file.    | [...].. code:: python
        \   import urllib, os    from tqdm import tqdm    urllib = getattr(urllib,
        'request', urllib)    class TqdmUpTo(tqdm):        \"\"\"Provides `update_to(n)`
        which uses `tqdm.update(delta_n)`.\"\"\"        def update_to(self, b=1, bsize=1,
        tsize=None):            \"\"\"            b  : int, optional                Number
        of blocks transferred so far [default: 1].            bsize  : int, optional
        \               Size of each block (in tqdm units) [default: 1].            tsize
        \ : int, optional                Total size (in tqdm units). If [default:
        None] remains unchanged.            \"\"\"            if tsize is not None:
        \               self.total = tsize            return self.update(b * bsize
        - self.n)  # also sets self.n = b * bsize    eg_link = \"https://caspersci.uk.to/matryoshka.zip\"
        \   with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, miniters=1,
        \                 desc=eg_link.split('/')[-1]) as t:  # all optional kwargs
        \       urllib.urlretrieve(eg_link, filename=os.devnull,                           reporthook=t.update_to,
        data=None)        t.total = t.nInspired by `twine#242 <https://github.com/pypa/twine/pull/242>`__.Functional
        alternative in`examples/tqdm_wget.py <https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py>`__.It
        is recommend to use ``miniters=1`` whenever there is potentiallylarge differences
        in iteration speed (e.g. downloading a file overa patchy connection).**Wrapping
        read/write methods**To measure throughput through a file-like object's ``read``
        or ``write``methods, use ``CallbackIOWrapper``:.. code:: python    from tqdm.auto
        import tqdm    from tqdm.utils import CallbackIOWrapper    with tqdm(total=file_obj.size,
        \             unit='B', unit_scale=True, unit_divisor=1024) as t:        fobj
        = CallbackIOWrapper(t.update, file_obj, \"read\")        while True:            chunk
        = fobj.read(chunk_size)            if not chunk:                break        t.reset()
        \       # ... continue to use `t` for something elseAlternatively, use the
        even simpler ``wrapattr`` convenience function,which would condense both the
        ``urllib`` and ``CallbackIOWrapper`` examplesdown to:.. code:: python    import
        urllib, os    from tqdm import tqdm    eg_link = \"https://caspersci.uk.to/matryoshka.zip\"
        \   response = getattr(urllib, 'request', urllib).urlopen(eg_link)    with
        tqdm.wrapattr(open(os.devnull, \"wb\"), \"write\",                       miniters=1,
        desc=eg_link.split('/')[-1],                       total=getattr(response,
        'length', None)) as fout:        for chunk in response:            fout.write(chunk)The
        ``requests`` equivalent is nearly identical:.. code:: python    import requests,
        os    from tqdm import tqdm    eg_link = \"https://caspersci.uk.to/matryoshka.zip\"
        \   response = requests.get(eg_link, stream=True)    with tqdm.wrapattr(open(os.devnull,
        \"wb\"), \"write\",                       miniters=1, desc=eg_link.split('/')[-1],
        \                      total=int(response.headers.get('content-length', 0)))
        as fout:        for chunk in response.iter_content(chunk_size=4096):            fout.write(chunk)**Custom
        callback**``tqdm`` is known for intelligently skipping unnecessary displays.
        To make acustom callback take advantage of this, simply use the return value
        of``update()``. This is set to ``True`` if a ``display()`` was triggered...
        code:: python    from tqdm.auto import tqdm as std_tqdm    def external_callback(*args,
        **kwargs):        ...    class TqdmExt(std_tqdm):        def update(self,
        n=1):            displayed = super(TqdmExt, self).update(n):            if
        displayed:                external_callback(**self.format_dict)            return
        displayed``asyncio``~~~~~~~~~~~Note that ``break`` isn't currently caught
        by asynchronous iterators.This means that ``tqdm`` cannot clean up after itself
        in this case:.. code:: python    from tqdm.asyncio import tqdm    async for
        i in tqdm(range(9)):        if i == 2:            breakInstead, either call
        ``pbar.close()`` manually or use the context manager syntax:.. code:: python
        \   from tqdm.asyncio import tqdm    with tqdm(range(9)) as pbar:        async
        for i in pbar:            if i == 2:                breakPandas Integration~~~~~~~~~~~~~~~~~~Due
        to popular demand we've added support for ``pandas`` -- here's an examplefor
        ``DataFrame.progress_apply`` and ``DataFrameGroupBy.progress_apply``:.. code::
        python    import pandas as pd    import numpy as np    from tqdm import tqdm
        \   df = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))    # Register
        `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`    # (can
        use `tqdm.gui.tqdm`, `tqdm.notebook.tqdm`, optional kwargs, etc.)    tqdm.pandas(desc=\"my
        bar!\")    # Now you can use `progress_apply` instead of `apply`    # and
        `progress_map` instead of `map`    df.progress_apply(lambda x: x**2)    #
        can also groupby:    # df.groupby(0).progress_apply(lambda x: x**2)In case
        you're interested in how this works (and how to modify it for yourown callbacks),
        see the`examples <https://github.com/tqdm/tqdm/tree/master/examples>`__folder
        or import the module and run ``help()``.Keras Integration~~~~~~~~~~~~~~~~~A
        ``keras`` callback is also available:.. code:: python    from tqdm.keras import
        TqdmCallback    ...    model.fit(..., verbose=0, callbacks=[TqdmCallback()])Dask
        Integration~~~~~~~~~~~~~~~~A ``dask`` callback is also available:.. code::
        python    from tqdm.dask import TqdmCallback    with TqdmCallback(desc=\"compute\"):
        \       ...        arr.compute()    # or use callback globally    cb = TqdmCallback(desc=\"global\")
        \   cb.register()    arr.compute()IPython/Jupyter Integration~~~~~~~~~~~~~~~~~~~~~~~~~~~~IPython/Jupyter
        is supported via the ``tqdm.notebook`` submodule:.. code:: python    from
        tqdm.notebook import trange, tqdm    from time import sleep    for i in trange(3,
        desc='1st loop'):        for j in tqdm(range(100), desc='2nd loop'):            sleep(0.01)In
        addition to ``tqdm`` features, the submodule provides a native Jupyterwidget
        (compatible with IPython v1-v4 and Jupyter), fully working nested barsand
        colour hints (blue: normal, green: completed, red: error/interrupt,light blue:
        no ETA); as demonstrated below.|Screenshot-Jupyter1||Screenshot-Jupyter2||Screenshot-Jupyter3|The
        ``notebook`` version supports percentage or pixels for overall width(e.g.:
        ``ncols='100%'`` or ``ncols='480px'``).It is also possible to let ``tqdm``
        automatically choose betweenconsole or notebook versions by using the ``autonotebook``
        submodule:.. code:: python    from tqdm.autonotebook import tqdm    tqdm.pandas()Note
        that this will issue a ``TqdmExperimentalWarning`` if run in a notebooksince
        it is not meant to be possible to distinguish between ``jupyter notebook``and
        ``jupyter console``. Use ``auto`` instead of ``autonotebook`` to suppressthis
        warning.Note that notebooks will display the bar in the cell where it was
        created.This may be a different cell from the one where it is used.If this
        is not desired, either- delay the creation of the bar to the cell where it
        must be displayed, or- create the bar with ``display=False``, and in a later
        cell call  ``display(bar.container)``:.. code:: python    from tqdm.notebook
        import tqdm    pbar = tqdm(..., display=False).. code:: python    # different
        cell    display(pbar.container)The ``keras`` callback has a ``display()``
        method which can be used likewise:.. code:: python    from tqdm.keras import
        TqdmCallback    cbk = TqdmCallback(display=False).. code:: python    # different
        cell    cbk.display()    model.fit(..., verbose=0, callbacks=[cbk])Another
        possibility is to have a single bar (near the top of the notebook)which is
        constantly re-used (using ``reset()`` rather than ``close()``).For this reason,
        the notebook version (unlike the CLI version) does notautomatically call ``close()``
        upon ``Exception``... code:: python    from tqdm.notebook import tqdm    pbar
        = tqdm().. code:: python    # different cell    iterable = range(100)    pbar.reset(total=len(iterable))
        \ # initialise with new `total`    for i in iterable:        pbar.update()
        \   pbar.refresh()  # force print final status but don't `close()`Custom Integration~~~~~~~~~~~~~~~~~~To
        change the default arguments (such as making ``dynamic_ncols=True``),simply
        use built-in Python magic:.. code:: python    from functools import partial
        \   from tqdm import tqdm as std_tqdm    tqdm = partial(std_tqdm, dynamic_ncols=True)For
        further customisation,``tqdm`` may be inherited from to create custom callbacks
        (as with the``TqdmUpTo`` example `above <#hooks-and-callbacks>`__) or for
        custom frontends(e.g. GUIs such as notebook or plotting packages). In the
        latter case:1. ``def __init__()`` to call ``super().__init__(..., gui=True)``
        to disable   terminal ``status_printer`` creation.2. Redefine: ``close()``,
        ``clear()``, ``display()``.Consider overloading ``display()`` to use e.g.``self.frontend(**self.format_dict)``
        instead of ``self.sp(repr(self))``.Some submodule examples of inheritance:-
        `tqdm/notebook.py <https://github.com/tqdm/tqdm/blob/master/tqdm/notebook.py>`__-
        `tqdm/gui.py <https://github.com/tqdm/tqdm/blob/master/tqdm/gui.py>`__- `tqdm/tk.py
        <https://github.com/tqdm/tqdm/blob/master/tqdm/tk.py>`__- `tqdm/contrib/telegram.py
        <https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/telegram.py>`__- `tqdm/contrib/discord.py
        <https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/discord.py>`__Dynamic
        Monitor/Meter~~~~~~~~~~~~~~~~~~~~~You can use a ``tqdm`` as a meter which
        is not monotonically increasing.This could be because ``n`` decreases (e.g.
        a CPU usage monitor) or ``total``changes.One example would be recursively
        searching for files. The ``total`` is thenumber of objects found so far, while
        ``n`` is the number of those objects whichare files (rather than folders):..
        code:: python    from tqdm import tqdm    import os.path    def find_files_recursively(path,
        show_progress=True):        files = []        # total=1 assumes `path` is
        a file        t = tqdm(total=1, unit=\"file\", disable=not show_progress)
        \       if not os.path.exists(path):            raise IOError(\"Cannot find:\"
        + path)        def append_found_file(f):            files.append(f)            t.update()
        \       def list_found_dir(path):            \"\"\"returns os.listdir(path)
        assuming os.path.isdir(path)\"\"\"            listing = os.listdir(path)            #
        subtract 1 since a \"file\" we found was actually this directory            t.total
        += len(listing) - 1            # fancy way to give info without forcing a
        refresh            t.set_postfix(dir=path[-10:], refresh=False)            t.update(0)
        \ # may trigger a refresh            return listing        def recursively_search(path):
        \           if os.path.isdir(path):                for f in list_found_dir(path):
        \                   recursively_search(os.path.join(path, f))            else:
        \               append_found_file(path)        recursively_search(path)        t.set_postfix(dir=path)
        \       t.close()        return filesUsing ``update(0)`` is a handy way to
        let ``tqdm`` decide when to trigger adisplay refresh to avoid console spamming.Writing
        messages~~~~~~~~~~~~~~~~This is a work in progress (see`#737 <https://github.com/tqdm/tqdm/issues/737>`__).Since
        ``tqdm`` uses a simple printing mechanism to display progress bars,you should
        not write any message in the terminal using ``print()`` whilea progressbar
        is open.To write messages in the terminal without any collision with ``tqdm``
        bardisplay, a ``.write()`` method is provided:.. code:: python    from tqdm.auto
        import tqdm, trange    from time import sleep    bar = trange(10)    for i
        in bar:        # Print using tqdm class method .write()        sleep(0.1)
        \       if not (i % 3):            tqdm.write(\"Done task %i\" % i)        #
        Can also use bar.write()By default, this will print to standard output ``sys.stdout``.
        but you canspecify any file-like object using the ``file`` argument. For example,
        thiscan be used to redirect the messages writing to a log file or class.Redirecting
        writing~~~~~~~~~~~~~~~~~~~If using a library that can print messages to the
        console, editing the libraryby  replacing ``print()`` with ``tqdm.write()``
        may not be desirable.In that case, redirecting ``sys.stdout`` to ``tqdm.write()``
        is an option.To redirect ``sys.stdout``, create a file-like class that will
        writeany input string to ``tqdm.write()``, and supply the arguments``file=sys.stdout,
        dynamic_ncols=True``.A reusable canonical example is given below:.. code::
        python    from time import sleep    import contextlib    import sys    from
        tqdm import tqdm    from tqdm.contrib import DummyTqdmFile    @contextlib.contextmanager
        \   def std_out_err_redirect_tqdm():        orig_out_err = sys.stdout, sys.stderr
        \       try:            sys.stdout, sys.stderr = map(DummyTqdmFile, orig_out_err)
        \           yield orig_out_err[0]        # Relay exceptions        except
        Exception as exc:            raise exc        # Always restore sys.stdout/err
        if necessary        finally:            sys.stdout, sys.stderr = orig_out_err
        \   def some_fun(i):        print(\"Fee, fi, fo,\".split()[i])    # Redirect
        stdout to tqdm.write() (don't forget the `as save_stdout`)    with std_out_err_redirect_tqdm()
        as orig_stdout:        # tqdm needs the original stdout        # and dynamic_ncols=True
        to autodetect console width        for i in tqdm(range(3), file=orig_stdout,
        dynamic_ncols=True):            sleep(.5)            some_fun(i)    # After
        the `with`, printing is restored    print(\"Done!\")Redirecting ``logging``~~~~~~~~~~~~~~~~~~~~~~~Similar
        to ``sys.stdout``/``sys.stderr`` as detailed above, console ``logging``may
        also be redirected to ``tqdm.write()``.Warning: if also redirecting ``sys.stdout``/``sys.stderr``,
        make sure toredirect ``logging`` first if needed.Helper methods are available
        in ``tqdm.contrib.logging``. For example:.. code:: python    import logging
        \   from tqdm import trange    from tqdm.contrib.logging import logging_redirect_tqdm
        \   LOG = logging.getLogger(__name__)    if __name__ == '__main__':        logging.basicConfig(level=logging.INFO)
        \       with logging_redirect_tqdm():            for i in trange(9):                if
        i == 4:                    LOG.info(\"console logging redirected to `tqdm.write()`\")
        \       # logging restoredMonitoring thread, intervals and miniters~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~``tqdm``
        implements a few tricks to increase efficiency and reduce overhead.- Avoid
        unnecessary frequent bar refreshing: ``mininterval`` defines how long  to
        wait between each refresh. ``tqdm`` always gets updated in the background,
        \ but it will display only every ``mininterval``.- Reduce number of calls
        to check system clock/time.- ``mininterval`` is more intuitive to configure
        than ``miniters``.  A clever adjustment system ``dynamic_miniters`` will automatically
        adjust  ``miniters`` to the amount of iterations that fit into time ``mininterval``.
        \ Essentially, ``tqdm`` will check if it's time to print without actually
        \ checking time. This behaviour can be still be bypassed by manually setting
        \ ``miniters``.However, consider a case with a combination of fast and slow
        iterations.After a few fast iterations, ``dynamic_miniters`` will set ``miniters``
        to alarge number. When iteration rate subsequently slows, ``miniters`` willremain
        large and thus reduce display update frequency. To address this:- ``maxinterval``
        defines the maximum time between display refreshes.  A concurrent monitoring
        thread checks for overdue updates and forces one  where necessary.The monitoring
        thread should not have a noticeable overhead, and guaranteesupdates at least
        every 10 seconds by default.This value can be directly changed by setting
        the ``monitor_interval`` ofany ``tqdm`` instance (i.e. ``t = tqdm.tqdm(...);
        t.monitor_interval = 2``).The monitor thread may be disabled application-wide
        by setting``tqdm.tqdm.monitor_interval = 0`` before instantiation of any ``tqdm``
        bar.Merch-----You can buy `tqdm branded merch <https://tqdm.github.io/merch>`__
        now!Contributions-------------|GitHub-Commits| |GitHub-Issues| |GitHub-PRs|
        |OpenHub-Status| |GitHub-Contributions| |CII Best Practices|All source code
        is hosted on `GitHub <https://github.com/tqdm/tqdm>`__.Contributions are welcome.See
        the`CONTRIBUTING <https://github.com/tqdm/tqdm/blob/master/CONTRIBUTING.md>`__file
        for more information.Developers who have made significant contributions, ranked
        by *SLoC*(surviving lines of code,`git fame <https://github.com/casperdcl/git-fame>`__
        ``-wMC --excl '\\.(png|gif|jpg)$'``),are:==================== ========================================================
        ==== ================================Name                 ID                                                       SLoC
        Notes==================== ========================================================
        ==== ================================Casper da Costa-Luis `casperdcl <https://github.com/casperdcl>`__
        \            ~78% primary maintainer |Gift-Casper|Stephen Larroque     `lrq3000
        <https://github.com/lrq3000>`__                 ~10% team memberMartin Zugnoni
        \      `martinzugnoni <https://github.com/martinzugnoni>`__     ~4%Daniel
        Ecer          `de-code <https://github.com/de-code>`__                 ~2%Richard
        Sheridan     `richardsheridan <https://github.com/richardsheridan>`__ ~1%Guangshuo
        Chen       `chengs <https://github.com/chengs>`__                   ~1%Kyle
        Altendorf       `altendky <https://github.com/altendky>`__               <1%Matthew
        Stevens      `mjstevens777 <https://github.com/mjstevens777>`__       <1%Hadrien
        Mary         `hadim <https://github.com/hadim>`__                     <1%
        \ team memberNoam Yorav-Raphael   `noamraph <https://github.com/noamraph>`__
        \              <1%  original authorMikhail Korobov      `kmike <https://github.com/kmike>`__
        \                    <1%  team member==================== ========================================================
        ==== ================================Ports to Other Languages~~~~~~~~~~~~~~~~~~~~~~~~A
        list is available on`this wiki page <https://github.com/tqdm/tqdm/wiki/tqdm-ports>`__.LICENCE-------Open
        Source (OSI approved): |LICENCE|Citation information: |DOI||README-Hits| (Since
        19 May 2016).. |Logo| image:: https://img.tqdm.ml/logo.gif.. |Screenshot|
        image:: https://img.tqdm.ml/tqdm.gif.. |Video| image:: https://img.tqdm.ml/video.jpg
        \  :target: https://tqdm.github.io/video.. |Slides| image:: https://img.tqdm.ml/slides.jpg
        \  :target: https://tqdm.github.io/PyData2019/slides.html.. |Merch| image::
        https://img.tqdm.ml/merch.jpg   :target: https://tqdm.github.io/merch.. |Build-Status|
        image:: https://img.shields.io/github/workflow/status/tqdm/tqdm/Test/master?logo=GitHub
        \  :target: https://github.com/tqdm/tqdm/actions?query=workflow%3ATest.. |Coverage-Status|
        image:: https://img.shields.io/coveralls/github/tqdm/tqdm/master?logo=coveralls
        \  :target: https://coveralls.io/github/tqdm/tqdm.. |Branch-Coverage-Status|
        image:: https://codecov.io/gh/tqdm/tqdm/branch/master/graph/badge.svg   :target:
        https://codecov.io/gh/tqdm/tqdm.. |Codacy-Grade| image:: https://app.codacy.com/project/badge/Grade/3f965571598f44549c7818f29cdcf177
        \  :target: https://www.codacy.com/gh/tqdm/tqdm/dashboard.. |CII Best Practices|
        image:: https://bestpractices.coreinfrastructure.org/projects/3264/badge   :target:
        https://bestpractices.coreinfrastructure.org/projects/3264.. |GitHub-Status|
        image:: https://img.shields.io/github/tag/tqdm/tqdm.svg?maxAge=86400&logo=github&logoColor=white
        \  :target: https://github.com/tqdm/tqdm/releases.. |GitHub-Forks| image::
        https://img.shields.io/github/forks/tqdm/tqdm.svg?logo=github&logoColor=white
        \  :target: https://github.com/tqdm/tqdm/network.. |GitHub-Stars| image::
        https://img.shields.io/github/stars/tqdm/tqdm.svg?logo=github&logoColor=white
        \  :target: https://github.com/tqdm/tqdm/stargazers.. |GitHub-Commits| image::
        https://img.shields.io/github/commit-activity/y/tqdm/tqdm.svg?logo=git&logoColor=white
        \  :target: https://github.com/tqdm/tqdm/graphs/commit-activity.. |GitHub-Issues|
        image:: https://img.shields.io/github/issues-closed/tqdm/tqdm.svg?logo=github&logoColor=white
        \  :target: https://github.com/tqdm/tqdm/issues?q=.. |GitHub-PRs| image::
        https://img.shields.io/github/issues-pr-closed/tqdm/tqdm.svg?logo=github&logoColor=white
        \  :target: https://github.com/tqdm/tqdm/pulls.. |GitHub-Contributions| image::
        https://img.shields.io/github/contributors/tqdm/tqdm.svg?logo=github&logoColor=white
        \  :target: https://github.com/tqdm/tqdm/graphs/contributors.. |GitHub-Updated|
        image:: https://img.shields.io/github/last-commit/tqdm/tqdm/master.svg?logo=github&logoColor=white&label=pushed
        \  :target: https://github.com/tqdm/tqdm/pulse.. |Gift-Casper| image:: https://img.shields.io/badge/dynamic/json.svg?color=ff69b4&label=gifts%20received&prefix=%C2%A3&query=%24..sum&url=https%3A%2F%2Fcaspersci.uk.to%2Fgifts.json
        \  :target: https://cdcl.ml/sponsor.. |Versions| image:: https://img.shields.io/pypi/v/tqdm.svg
        \  :target: https://tqdm.github.io/releases.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/tqdm.svg?label=pypi%20downloads&logo=PyPI&logoColor=white
        \  :target: https://pepy.tech/project/tqdm.. |Py-Versions| image:: https://img.shields.io/pypi/pyversions/tqdm.svg?logo=python&logoColor=white
        \  :target: https://pypi.org/project/tqdm.. |Conda-Forge-Status| image:: https://img.shields.io/conda/v/conda-forge/tqdm.svg?label=conda-forge&logo=conda-forge
        \  :target: https://anaconda.org/conda-forge/tqdm.. |Snapcraft| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft
        \  :target: https://snapcraft.io/tqdm.. |Docker| image:: https://img.shields.io/badge/docker-pull-blue.svg?logo=docker&logoColor=white
        \  :target: https://hub.docker.com/r/tqdm/tqdm.. |Libraries-Rank| image::
        https://img.shields.io/librariesio/sourcerank/pypi/tqdm.svg?logo=koding&logoColor=white
        \  :target: https://libraries.io/pypi/tqdm.. |Libraries-Dependents| image::
        https://img.shields.io/librariesio/dependent-repos/pypi/tqdm.svg?logo=koding&logoColor=white
        \   :target: https://github.com/tqdm/tqdm/network/dependents.. |OpenHub-Status|
        image:: https://www.openhub.net/p/tqdm/widgets/project_thin_badge?format=gif
        \  :target: https://www.openhub.net/p/tqdm?ref=Thin+badge.. |awesome-python|
        image:: https://awesome.re/mentioned-badge.svg   :target: https://github.com/vinta/awesome-python..
        |LICENCE| image:: https://img.shields.io/pypi/l/tqdm.svg   :target: https://raw.githubusercontent.com/tqdm/tqdm/master/LICENCE..
        |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.595120-blue.svg
        \  :target: https://doi.org/10.5281/zenodo.595120.. |binder-demo| image::
        https://mybinder.org/badge_logo.svg   :target: https://mybinder.org/v2/gh/tqdm/tqdm/master?filepath=DEMO.ipynb..
        |Screenshot-Jupyter1| image:: https://img.tqdm.ml/jupyter-1.gif.. |Screenshot-Jupyter2|
        image:: https://img.tqdm.ml/jupyter-2.gif.. |Screenshot-Jupyter3| image::
        https://img.tqdm.ml/jupyter-3.gif.. |README-Hits| image:: https://caspersci.uk.to/cgi-bin/hits.cgi?q=tqdm&style=social&r=https://github.com/tqdm/tqdm&l=https://img.tqdm.ml/favicon.png&f=https://img.tqdm.ml/logo.gif
        \  :target: https://caspersci.uk.to/cgi-bin/hits.cgi?q=tqdm&a=plot&r=https://github.com/tqdm/tqdm&l=https://img.tqdm.ml/favicon.png&f=https://img.tqdm.ml/logo.gif&style=social"
      Package: tqdm
      Source: pip
      Version: 4.66.2
      Hash: ''
      licenses:
      - MIT
      - MPL-2.0
      - Python-2.0
      Title: tqdm
      DownloadURL: https://files.pythonhosted.org/packages/ea/85/3ce0f9f7d3f596e7ea57f4e5ce8c18cb44e4a9daa58ddb46ee0d13d6bff8/tqdm-4.66.2.tar.gz
  bazaar:
    register: 'no'
    prim: 27/CTX1023020
    community_link: https://pypi.org/project/tqdm/
    community_name: https://pypi.org/project/tqdm/
    community_url: https://pypi.org/project/tqdm/
    component_comment: ''
    component_highlevel_description: A Fast, Extensible Progress Bar for Python and
      CLI
    component_name: tqdm
    component_platform: linux
    component_programing_language: Python
    component_version: 4.66.2
    licenses:
    - FAL1159005/20 (Mozilla Public License 2.0 (MPL-2.0))
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://files.pythonhosted.org/packages/ea/85/3ce0f9f7d3f596e7ea57f4e5ce8c18cb44e4a9daa58ddb46ee0d13d6bff8/tqdm-4.66.2.tar.gz
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Low community activity
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078659&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: tqdm
    target_sw: linux
    vendor: pip
    version: 4.66.2
    web_url: https://tqdm.github.io
  licenses:
  - MIT
  - MPL-2.0
  - Python-2.0
  name: tqdm
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 4.66.2
  mimer:
    linking: Static
    product_number: CTX1023020
    product_version_label: 4.66.2
    selected_licenses:
    - MIT
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: transformers+4.38.2
  additional_info:
    fossa-attribution:
      Description: "<!---Copyright 2020 The HuggingFace Team. All rights reserved.Licensed
        under the Apache License, Version 2.0 (the \"License\");you may not use this
        file except in compliance with the License.You may obtain a copy of the License
        at    http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable
        law or agreed to in writing, softwaredistributed under the License is distributed
        on an \"AS IS\" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
        express or implied.See the License for the specific language governing permissions
        andlimitations under the License.--><p align=\"center\">  <picture>    <source
        media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">
        \   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">
        \   <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\"
        width=\"352\" height=\"59\" style=\"max-width: 100%;\">  </picture>  <br/>
        \ <br/></p><p align=\"center\">    <a href=\"https://circleci.com/gh/huggingface/transformers\">
        \       <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">
        \   </a>    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">
        \       <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">
        \   </a>    <a href=\"https://huggingface.co/docs/transformers/index\">        <img
        alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">
        \   </a>    <a href=\"https://github.com/huggingface/transformers/releases\">
        \       <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">
        \   </a>    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">
        \       <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">
        \   </a>    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img
        src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a></p><h4 align=\"center\">
        \   <p>        <b>English</b> |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hans.md\">\u7B80\u4F53\u4E2D\u6587</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">\u7E41\u9AD4\u4E2D\u6587</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">\uD55C\uAD6D\uC5B4</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Espa\xF1ol</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">\u65E5\u672C\u8A9E</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">\u0939\u093F\u0928\u094D\u0926\u0940</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ru.md\">\u0420\u0443\u0441\u0441\u043A\u0438\u0439</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_pt-br.md\">\u0420ortugu\xEAs</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_te.md\">\u0C24\u0C46\u0C32\u0C41\u0C17\u0C41</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_fr.md\">Fran\xE7ais</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_de.md\">Deutsch</a>
        |        <a href=\"https://github.com/huggingface/transformers/blob/main/README_vi.md\">Ti\u1EBFng
        Vi\u1EC7t</a> |    </p></h4><h3 align=\"center\">    <p>State-of-the-art Machine
        Learning for JAX, PyTorch and TensorFlow</p></h3><h3 align=\"center\">    <a
        href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a></h3>\U0001F917
        Transformers provides thousands of pretrained models to perform tasks on different
        modalities such as text, vision, and audio.These models can be applied on:*
        \U0001F4DD Text, for tasks like text classification, information extraction,
        question answering, summarization, translation, and text generation, in over
        100 languages.* \U0001F5BC\uFE0F Images, for tasks like image classification,
        object detection, and segmentation.* \U0001F5E3\uFE0F Audio, for tasks like
        speech recognition and audio classification.Transformer models can also perform
        tasks on **several modalities combined**, such as table question answering,
        optical character recognition, information extraction from scanned documents,
        video classification, and visual question answering.\U0001F917 Transformers
        provides APIs to quickly download and use those pretrained models on a given
        text, fine-tune them on your own datasets and then share them with the community
        on our [model hub](https://huggingface.co/models). At the same time, each
        python module defining an architecture is fully standalone and can be modified
        to enable quick research experiments.\U0001F917 Transformers is backed by
        the three most popular deep learning libraries \u2014 [Jax](https://jax.readthedocs.io/en/latest/),
        [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/)
        \u2014 with a seamless integration between them. It's straightforward to train
        your models with one before loading them for inference with the other.## Online
        demosYou can test most of our models directly on their pages from the [model
        hub](https://huggingface.co/models). We also offer [private model hosting,
        versioning, & an inference API](https://huggingface.co/pricing) for public
        and private models.Here are a few examples:In Natural Language Processing:-
        [Masked word completion with BERT](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)-
        [Named Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)-
        [Text generation with Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)-
        [Natural Language Inference with RoBERTa](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)-
        [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)-
        [Question answering with DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)-
        [Translation with T5](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)In
        Computer Vision:- [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224)-
        [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50)-
        [Semantic Segmentation with SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)-
        [Panoptic Segmentation with Mask2Former](https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic)-
        [Depth Estimation with Depth Anything](https://huggingface.co/docs/transformers/main/model_doc/depth_anything)-
        [Video Classification with VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)-
        [Universal Segmentation with OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)In
        Audio:- [Automatic Speech Recognition with Whisper](https://huggingface.co/openai/whisper-large-v3)-
        [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)-
        [Audio Classification with Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)In
        Multimodal tasks:- [Table Question Answering with TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)-
        [Visual Question Answering with ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)-
        [Image captioning with LLaVa](https://huggingface.co/llava-hf/llava-1.5-7b-hf)-
        [Zero-shot Image Classification with SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384)-
        [Document Question Answering with LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)-
        [Zero-shot Video Classification with X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)-
        [Zero-shot Object Detection with OWLv2](https://huggingface.co/docs/transformers/en/model_doc/owlv2)-
        [Zero-shot Image Segmentation with CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)-
        [Automatic Mask Generation with SAM](https://huggingface.co/docs/transformers/model_doc/sam)##
        100 projects using TransformersTransformers is more than a toolkit to use
        pretrained models: it's a community of projects built around it and theHugging
        Face Hub. We want Transformers to enable developers, researchers, students,
        professors, engineers, and anyoneelse to build their dream projects.In order
        to celebrate the 100,000 stars of transformers, we have decided to put the
        spotlight on thecommunity, and we have created the [awesome-transformers](./awesome-transformers.md)
        page which lists 100incredible projects built in the vicinity of transformers.If
        you own or use a project that you believe should be part of the list, please
        open a PR to add it!## If you are looking for custom support from the Hugging
        Face team<a target=\"_blank\" href=\"https://huggingface.co/support\">    <img
        alt=\"HuggingFace Expert Acceleration Program\" src=\"https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png\"
        style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow:
        0 1px 2px 0 rgba(0, 0, 0, 0.05);\"></a><br>## Quick tourTo immediately use
        a model on a given input (text, image, audio, ...), we provide the `pipeline`
        API. Pipelines group together a pretrained model with the preprocessing that
        was used during that model's training. Here is how to quickly use a pipeline
        to classify positive versus negative texts:```python>>> from transformers
        import pipeline# Allocate a pipeline for sentiment-analysis>>> classifier
        = pipeline('sentiment-analysis')>>> classifier('We are very happy to introduce
        pipeline to the transformers repository.')[{'label': 'POSITIVE', 'score':
        0.9996980428695679}]```The second line of code downloads and caches the pretrained
        model used by the pipeline, while the third evaluates it on the given text.
        Here, the answer is \"positive\" with a confidence of 99.97%.Many tasks have
        a pre-trained `pipeline` ready to go, in NLP but also in computer vision and
        speech. For example, we can easily extract detected objects in an image:```
        python>>> import requests>>> from PIL import Image>>> from transformers import
        pipeline# Download an image with cute cats>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\">>>
        image_data = requests.get(url, stream=True).raw>>> image = Image.open(image_data)#
        Allocate a pipeline for object detection>>> object_detector = pipeline('object-detection')>>>
        object_detector(image)[{'score': 0.9982201457023621,  'label': 'remote',  'box':
        {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, {'score': 0.9960021376609802,
        \ 'label': 'remote',  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax':
        187}}, {'score': 0.9954745173454285,  'label': 'couch',  'box': {'xmin': 0,
        'ymin': 1, 'xmax': 639, 'ymax': 473}}, {'score': 0.9988006353378296,  'label':
        'cat',  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}}, {'score':
        0.9986783862113953,  'label': 'cat',  'box': {'xmin': 345, 'ymin': 23, 'xmax':
        640, 'ymax': 368}}]```Here, we get a list of objects detected in the image,
        with a box surrounding the object and a confidence score. Here is the original
        image on the left, with the predictions displayed on the right:<h3 align=\"center\">
        \   <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"
        width=\"400\"></a>    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png\"
        width=\"400\"></a></h3>You can learn more about the tasks supported by the
        `pipeline` API in [this tutorial](https://huggingface.co/docs/transformers/task_summary).In
        addition to `pipeline`, to download and use any of the pretrained models on
        your given task, all it takes is three lines of code. Here is the PyTorch
        version:```python>>> from transformers import AutoTokenizer, AutoModel>>>
        tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")>>>
        model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")>>> inputs
        = tokenizer(\"Hello world!\", return_tensors=\"pt\")>>> outputs = model(**inputs)```And
        here is the equivalent code for TensorFlow:```python>>> from transformers
        import AutoTokenizer, TFAutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")>>>
        model = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\")>>>
        inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")>>> outputs = model(**inputs)```The
        tokenizer is responsible for all the preprocessing the pretrained model expects
        and can be called directly on a single string (as in the above examples) or
        a list. It will output a dictionary that you can use in downstream code or
        simply directly pass to your model using the ** argument unpacking operator.The
        model itself is a regular [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
        or a [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
        (depending on your backend) which you can use as usual. [This tutorial](https://huggingface.co/docs/transformers/training)
        explains how to integrate such a model into a classic PyTorch or TensorFlow
        training loop, or how to use our `Trainer` API to quickly fine-tune on a new
        dataset.## Why should I use transformers?1. Easy-to-use state-of-the-art models:
        \   - High performance on natural language understanding & generation, computer
        vision, and audio tasks.    - Low barrier to entry for educators and practitioners.
        \   - Few user-facing abstractions with just three classes to learn.    -
        A unified API for using all our pretrained models.1. Lower compute costs,
        smaller carbon footprint:    - Researchers can share trained models instead
        of always retraining.    - Practitioners can reduce compute time and production
        costs.    - Dozens of architectures with over 400,000 pretrained models across
        all modalities.1. Choose the right framework for every part of a model's lifetime:
        \   - Train state-of-the-art models in 3 lines of code.    - Move a single
        model between TF2.0/PyTorch/JAX frameworks at will.    - Seamlessly pick the
        right framework for training, evaluation, and production.1. Easily customize
        a model or an example to your needs:    - We provide examples for each architecture
        to reproduce the results published by its original authors.    - Model internals
        are exposed as consistently as possible.    - Model files can be used independently
        of the library for quick experiments.## Why shouldn't I use transformers?-
        This library is not a modular toolbox of building blocks for neural nets.
        The code in the model files is not refactored with additional abstractions
        on purpose, so that researchers can quickly iterate on each of the models
        without diving into additional abstractions/files.- The training API is not
        intended to work on any model but is optimized to work with the models provided
        by the library. For generic machine learning loops, you should use another
        library (possibly, [Accelerate](https://huggingface.co/docs/accelerate)).-
        While we strive to present as many use cases as possible, the scripts in our
        [examples folder](https://github.com/huggingface/transformers/tree/main/examples)
        are just that: examples. It is expected that they won't work out-of-the-box
        on your specific problem and that you will be required to change a few lines
        of code to adapt them to your needs.## Installation### With pipThis repository
        is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.11+, and TensorFlow 2.6+.You
        should install \U0001F917 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html).
        If you're unfamiliar with Python virtual environments, check out the [user
        guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).First,
        create a virtual environment with the version of Python you're going to use
        and activate it.Then, you will need to install at least one of Flax, PyTorch,
        or TensorFlow.Please refer to [TensorFlow installation page](https://www.tensorflow.org/install/),
        [PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally)
        and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation)
        installation pages regarding the specific installation command for your platform.When
        one of those backends has been installed, \U0001F917 Transformers can be installed
        using pip as follows:```bashpip install transformers```If you'd like to play
        with the examples or need the bleeding edge of the code and can't wait for
        a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).###
        With conda\U0001F917 Transformers can be installed using conda as follows:```shell
        scriptconda install conda-forge::transformers```> **_NOTE:_** Installing `transformers`
        from the `huggingface` channel is deprecated.Follow the installation pages
        of Flax, PyTorch or TensorFlow to see how to install them with conda.> **_NOTE:_**
        \ On Windows, you may be prompted to activate Developer Mode in order to benefit
        from caching. If this is not an option for you, please let us know in [this
        issue](https://github.com/huggingface/huggingface_hub/issues/1062).## Model
        architectures**[All the model checkpoints](https://huggingface.co/models)**
        provided by \U0001F917 Transformers are seamlessly integrated from the huggingface.co
        [model hub](https://huggingface.co/models), where they are uploaded directly
        by [users](https://huggingface.co/users) and [organizations](https://huggingface.co/organizations).Current
        number of checkpoints: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\U0001F917
        Transformers currently provides the following architectures (see [here](https://huggingface.co/docs/transformers/model_summary)
        for a high-level summary of each them):1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)**
        (from Google Research and the Toyota Technological Institute at Chicago) released
        with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language
        Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda
        Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.1. **[ALIGN](https://huggingface.co/docs/transformers/model_doc/align)**
        (from Google Research) released with the paper [Scaling Up Visual and Vision-Language
        Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918)
        by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham,
        Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.1. **[AltCLIP](https://huggingface.co/docs/transformers/model_doc/altclip)**
        (from BAAI) released with the paper [AltCLIP: Altering the Language Encoder
        in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679)
        by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang,
        Qinghong and Wu, Ledell.1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)**
        (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778)
        by Yuan Gong, Yu-An Chung, James Glass.1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)**
        (from Tsinghua University) released with the paper [Autoformer: Decomposition
        Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)
        by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)**
        (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark)
        by Suno AI team.1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)**
        (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence
        Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
        by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
        Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)**
        (from \xC9cole polytechnique) released with the paper [BARThez: a Skilled
        Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321)
        by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)**
        (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence
        Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran,
        Duong Minh Le and Dat Quoc Nguyen.1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)**
        (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image
        Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu
        Wei.1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)**
        (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional
        Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
        by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.1. **[BERT
        For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)**
        (from Google) released with the paper [Leveraging Pre-trained Checkpoints
        for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha
        Rothe, Shashi Narayan, Aliaksei Severyn.1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)**
        (from VinAI Research) released with the paper [BERTweet: A pre-trained language
        model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by
        Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)**
        (from Google Research) released with the paper [Big Bird: Transformers for
        Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru
        Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
        Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)**
        (from Google Research) released with the paper [Big Bird: Transformers for
        Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru
        Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
        Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)**
        (from Microsoft Research AI4Science) released with the paper [BioGPT: generative
        pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9)
        by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and
        Tie-Yan Liu.1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)**
        (from Google AI) released with the paper [Big Transfer (BiT): General Visual
        Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov,
        Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil
        Houlsby.1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)**
        (from Facebook) released with the paper [Recipes for building an open-domain
        chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan,
        Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster,
        Eric M. Smith, Y-Lan Boureau, Jason Weston.1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)**
        (from Facebook) released with the paper [Recipes for building an open-domain
        chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan,
        Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster,
        Eric M. Smith, Y-Lan Boureau, Jason Weston.1. **[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)**
        (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image
        Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)
        by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)**
        (from Salesforce) released with the paper [BLIP-2: Bootstrapping Language-Image
        Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
        by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.1. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)**
        (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).1.
        **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (from
        Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499)
        by Adrian de Wynter and Daniel J. Perry.1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)**
        (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released
        with the paper [BridgeTower: Building Bridges Between Encoders in Vision-Language
        Representation Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei
        Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)**
        (from NAVER CLOVA) released with the paper [BROS: A Pre-trained Language Model
        Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539)
        by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae
        Park.1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)**
        (from Google Research) released with the paper [ByT5: Towards a token-free
        future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
        by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang,
        Mihir Kale, Adam Roberts, Colin Raffel.1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)**
        (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty
        French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*,
        Benjamin Muller*, Pedro Javier Ortiz Su\xE1rez*, Yoann Dupont, Laurent Romary,
        \xC9ric Villemonte de la Clergerie, Djam\xE9 Seddah and Beno\xEEt Sagot.1.
        **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from
        Google Research) released with the paper [CANINE: Pre-training an Efficient
        Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874)
        by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)**
        (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language
        Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu
        Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)**
        (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio
        Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687)
        by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick,
        Shlomo Dubnov.1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)**
        (from OpenAI) released with the paper [Learning Transferable Visual Models
        From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec
        Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
        Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
        Krueger, Ilya Sutskever.1. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)**
        (from University of G\xF6ttingen) released with the paper [Image Segmentation
        Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L\xFCddecke
        and Alexander Ecker.1. **[CLVP](https://huggingface.co/docs/transformers/model_doc/clvp)**
        released with the paper [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243)
        by James Betker.1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)**
        (from Salesforce) released with the paper [A Conversational Paradigm for Program
        Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki
        Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.1.
        **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)**
        (from MetaAI) released with the paper [Code Llama: Open Foundation Models
        for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)
        by Baptiste Rozi\xE8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai
        Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\xE9r\xE9my Rapin,
        Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton
        Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\xE9fossez, Jade Copet,
        Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom,
        Gabriel Synnaeve.1. **[Cohere](https://huggingface.co/docs/transformers/model_doc/cohere)**
        (from Cohere) released with the paper [Command-R: Retrieval Augmented Generation
        at Production Scale](<https://txt.cohere.com/command-r/>) by Cohere. 1. **[Conditional
        DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)**
        (from Microsoft Research Asia) released with the paper [Conditional DETR for
        Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng,
        Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong
        Wang.1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)**
        (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based
        Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao
        Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)**
        (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)
        by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
        Saining Xie.1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)**
        (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and
        Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808)
        by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In
        So Kweon, Saining Xie.1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)**
        (from Tsinghua University) released with the paper [CPM: A Large-scale Generative
        Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan
        Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng
        Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng,
        Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang,
        Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.1. **[CPM-Ant](https://huggingface.co/docs/transformers/model_doc/cpmant)**
        (from OpenBMB) released by the [OpenBMB](https://www.openbmb.org/).1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)**
        (from Salesforce) released with the paper [CTRL: A Conditional Transformer
        Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858)
        by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and
        Richard Socher.1. **[CvT](https://huggingface.co/docs/transformers/model_doc/cvt)**
        (from Microsoft) released with the paper [CvT: Introducing Convolutions to
        Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin
        Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.1. **[Data2Vec](https://huggingface.co/docs/transformers/model_doc/data2vec)**
        (from Facebook) released with the paper [Data2Vec:  A General Framework for
        Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555)
        by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael
        Auli.1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)**
        (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT
        with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng
        He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)**
        (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT
        with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng
        He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.1. **[Decision Transformer](https://huggingface.co/docs/transformers/model_doc/decision_transformer)**
        (from Berkeley/Facebook/Google) released with the paper [Decision Transformer:
        Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)
        by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
        Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)**
        (from SenseTime Research) released with the paper [Deformable DETR: Deformable
        Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159)
        by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)**
        (from Facebook) released with the paper [Training data-efficient image transformers
        & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo
        Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,
        Herv\xE9 J\xE9gou.1. **[DePlot](https://huggingface.co/docs/transformers/model_doc/deplot)**
        (from Google AI) released with the paper [DePlot: One-shot visual language
        reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505)
        by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene,
        Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin
        Altun.1. **[Depth Anything](https://huggingface.co/docs/transformers/model_doc/depth_anything)**
        (from University of Hong Kong and TikTok) released with the paper [Depth Anything:
        Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891)
        by Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang
        Zhao.1. **[DETA](https://huggingface.co/docs/transformers/model_doc/deta)**
        (from The University of Texas at Austin) released with the paper [NMS Strikes
        Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun
        Cho, Xingyi Zhou, Philipp Kr\xE4henb\xFChl.1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)**
        (from Facebook) released with the paper [End-to-End Object Detection with
        Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco
        Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.1.
        **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)**
        (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative
        Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)
        by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang
        Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)**
        (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001)
        by Ali Hassani and Humphrey Shi.1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)**
        (from Meta AI) released with the paper [DINOv2: Learning Robust Visual Features
        without Supervision](https://arxiv.org/abs/2304.07193) by Maxime Oquab, Timoth\xE9e
        Darcet, Th\xE9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre
        Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran,
        Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li,
        Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\xE9
        Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.1.
        **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)**
        (from HuggingFace), released together with the paper [DistilBERT, a distilled
        version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)
        by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied
        to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation),
        RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation),
        Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation)
        and a German version of DistilBERT.1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)**
        (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training
        for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong
        Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)**
        (from NAVER), released together with the paper [OCR-free Document Understanding
        Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong,
        Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo
        Yun, Dongyoon Han, Seunghyun Park.1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)**
        (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain
        Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin,
        Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi
        Chen, and Wen-tau Yih.1. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)**
        (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413)
        by Ren\xE9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun.1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)**
        (from Snap Research) released with the paper [EfficientFormer: Vision Transformers
        at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan,
        Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian
        Ren.1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)**
        (from Google Brain) released with the paper [EfficientNet: Rethinking Model
        Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
        by Mingxing Tan, Quoc V. Le.1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)**
        (from Google Research/Stanford University) released with the paper [ELECTRA:
        Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555)
        by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)**
        (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438)
        by Alexandre D\xE9fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)**
        (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints
        for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha
        Rothe, Shashi Narayan, Aliaksei Severyn.1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)**
        (from Baidu) released with the paper [ERNIE: Enhanced Representation through
        Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan
        Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,
        Hao Tian, Hua Wu.1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)**
        (from Baidu) released with the paper [ERNIE-M: Enhanced Multilingual Representation
        by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674)
        by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng
        Wang.1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)**
        (from Meta AI) are transformer protein language models.  **ESM-1b** was released
        with the paper [Biological structure and function emerge from scaling unsupervised
        learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118)
        by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin,
        Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus.
        **ESM-1v** was released with the paper [Language models enable zero-shot prediction
        of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648)
        by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander
        Rives. **ESM-2 and ESMFold** were released with the paper [Language models
        of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902)
        by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu,
        Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander
        Rives.1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)**
        (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli,
        Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra
        and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay,
        Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and
        Penedo, Guilherme.1. **[FastSpeech2Conformer](https://huggingface.co/docs/transformers/model_doc/fastspeech2_conformer)**
        (from ESPnet) released with the paper [Recent Developments On Espnet Toolkit
        Boosted By Conformer](https://arxiv.org/abs/2010.13956) by Pengcheng Guo,
        Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma,
        Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji
        Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang.1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)**
        (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)
        by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
        Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
        Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
        Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew
        Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
        Denny Zhou, Quoc V. Le, and Jason Wei1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)**
        (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints)
        by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
        Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
        Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
        Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew
        Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
        Denny Zhou, Quoc V. Le, and Jason Wei1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)**
        (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model
        Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Lo\xEFc
        Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre
        Allauzen, Beno\xEEt Crabb\xE9, Laurent Besacier, Didier Schwab.1. **[FLAVA](https://huggingface.co/docs/transformers/model_doc/flava)**
        (from Facebook AI) released with the paper [FLAVA: A Foundational Language
        And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet
        Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba,
        Marcus Rohrbach, and Douwe Kiela.1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)**
        (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier
        Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie,
        Ilya Eckstein, Santiago Ontanon.1. **[FocalNet](https://huggingface.co/docs/transformers/model_doc/focalnet)**
        (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926)
        by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.1. **[Funnel
        Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)**
        (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering
        out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)
        by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.1. **[Fuyu](https://huggingface.co/docs/transformers/model_doc/fuyu)**
        (from ADEPT) Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus
        Odena, Arushi Somani, Sa\u011Fnak Ta\u015F\u0131rlar. Released with the paper
        [blog post](https://www.adept.ai/blog/fuyu-8b)1. **[Gemma](https://huggingface.co/docs/transformers/model_doc/gemma)**
        (from Google) released with the paper [Gemma: Open Models Based on Gemini
        Technology and Research](https://blog.google/technology/developers/gemma-open-models/)
        by the Gemma Google team.1. **[GIT](https://huggingface.co/docs/transformers/model_doc/git)**
        (from Microsoft Research) released with the paper [GIT: A Generative Image-to-text
        Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by
        Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
        Zicheng Liu, Ce Liu, Lijuan Wang.1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)**
        (from KAIST) released with the paper [Global-Local Path Networks for Monocular
        Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436)
        by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo
        Kim.1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)**
        (from OpenAI) released with the paper [Improving Language Understanding by
        Generative Pre-Training](https://openai.com/research/language-unsupervised/)
        by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.1. **[GPT
        Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (from EleutherAI)
        released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo)
        by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.1. **[GPT
        NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from
        EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive
        Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman,
        Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor
        Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu
        Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach1. **[GPT
        NeoX Japanese](https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese)**
        (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo
        Hattori.1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)**
        (from OpenAI) released with the paper [Language Models are Unsupervised Multitask
        Learners](https://openai.com/research/better-language-models/) by Alec Radford,
        Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever.1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)**
        (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/)
        by Ben Wang and Aran Komatsuzaki.1. **[GPT-Sw3](https://huggingface.co/docs/transformers/model_doc/gpt-sw3)**
        (from AI-Sweden) released with the paper [Lessons Learned from GPT-SW3: Building
        the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf)
        by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman,
        Severine Verlinden, Joey \xD6hman, Fredrik Carlsson, Magnus Sahlgren.1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)**
        (from BigCode) released with the paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988)
        by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher
        Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu,
        Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel
        Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel
        Romero, Michael Lappert, Francesco De Toni, Bernardo Garc\xEDa del R\xEDo,
        Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo
        Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish
        Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes,
        Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.1. **[GPTSAN-japanese](https://huggingface.co/docs/transformers/model_doc/gptsan-japanese)**
        released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md)
        by Toshiyuki Sakamoto(tanreinama).1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)**
        (from Microsoft) released with the paper [Do Transformers Really Perform Bad
        for Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan
        Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,
        Tie-Yan Liu.1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)**
        (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation
        Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui
        Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong
        Wang.1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)**
        (from Allegro.pl, AGH University of Science and Technology) released with
        the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf)
        by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)**
        (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation
        Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447)
        by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
        Salakhutdinov, Abdelrahman Mohamed.1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)**
        (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321)
        by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.1.
        **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)**
        (from HuggingFace) released with the paper [OBELICS: An Open Web-Scale Filtered
        Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527)
        by Hugo Lauren\xE7on, Lucile Saulnier, L\xE9o Tronchon, Stas Bekman, Amanpreet
        Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush,
        Douwe Kiela, Matthieu Cord, Victor Sanh.1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)**
        (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/)
        by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan,
        Ilya Sutskever.1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)**
        (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released
        with the paper [Informer: Beyond Efficient Transformer for Long Sequence Time-Series
        Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang,
        Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)**
        (from Salesforce) released with the paper [InstructBLIP: Towards General-purpose
        Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500)
        by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,
        Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)**
        (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf)
        by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
        Ilya Sutskever.1. **[KOSMOS-2](https://huggingface.co/docs/transformers/model_doc/kosmos-2)**
        (from Microsoft Research Asia) released with the paper [Kosmos-2: Grounding
        Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824)
        by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma,
        Furu Wei.1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)**
        (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training
        of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318)
        by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)**
        (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal
        Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)
        by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan
        Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)**
        (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training
        for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387)
        by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)**
        (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal
        Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)
        by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio,
        Cha Zhang, Furu Wei.1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)**
        (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
        by Iz Beltagy, Matthew E. Peters, Arman Cohan.1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)**
        (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's
        Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham,
        Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\xE9 J\xE9gou,
        Matthijs Douze.1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)**
        (from South China University of Technology) released with the paper [LiLT:
        A Simple yet Effective Language-Independent Layout Transformer for Structured
        Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang,
        Lianwen Jin, Kai Ding.1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)**
        (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient
        Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron,
        Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\xE9e
        Lacroix, Baptiste Rozi\xE8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien
        Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)**
        (from The FAIR team of Meta AI) released with the paper [Llama2: Open Foundation
        and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
        by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
        Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
        Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
        Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
        Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,
        Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann,
        Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya
        Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
        Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,
        Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith,
        Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams,
        Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
        Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey
        Edunov, Thomas Scialom.1. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)**
        (from Microsoft Research & University of Wisconsin-Madison) released with
        the paper [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) by
        Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.1. **[LLaVA-NeXT](https://huggingface.co/docs/transformers/model_doc/llava_next)**
        (from Microsoft Research & University of Wisconsin-Madison) released with
        the paper [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/abs/2310.03744)
        by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)**
        (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
        by Iz Beltagy, Matthew E. Peters, Arman Cohan.1. **[LongT5](https://huggingface.co/docs/transformers/model_doc/longt5)**
        (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer
        for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua
        Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei
        Yang.1. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)**
        (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity
        Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057)
        by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.1.
        **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (from
        UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality
        Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490)
        by Hao Tan and Mohit Bansal.1. **[M-CTC-T](https://huggingface.co/docs/transformers/model_doc/mctct)**
        (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual
        Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana
        Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)**
        (from Facebook) released with the paper [Beyond English-Centric Multilingual
        Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti
        Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep
        Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom
        Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand
        Joulin.1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)**
        (from Google) released with the paper [MADLAD-400: A Multilingual And Document-Level
        Large Audited Dataset](https://arxiv.org/abs/2309.04662) by Sneha Kudugunta,
        Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine
        Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat.1.
        **[Mamba](https://huggingface.co/docs/transformers/model_doc/mamba)** (from
        Albert Gu and Tri Dao) released with the paper [Mamba: Linear-Time Sequence
        Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) by
        Albert Gu and Tri Dao.1. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)**
        Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data
        by J\xF6rg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/)
        is being developed by the Microsoft Translator Team.1. **[MarkupLM](https://huggingface.co/docs/transformers/model_doc/markuplm)**
        (from Microsoft Research Asia) released with the paper [MarkupLM: Pre-training
        of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518)
        by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.1. **[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former)**
        (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer
        for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen
        Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.1.
        **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)**
        (from Meta and UIUC) released with the paper [Per-Pixel Classification is
        Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278)
        by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.1. **[MatCha](https://huggingface.co/docs/transformers/model_doc/matcha)**
        (from Google AI) released with the paper [MatCha: Enhancing Visual Language
        Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662)
        by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee,
        Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)**
        (from Facebook) released with the paper [Multilingual Denoising Pre-training
        for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan
        Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad,
        Mike Lewis, Luke Zettlemoyer.1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)**
        (from Facebook) released with the paper [Multilingual Translation with Extensible
        Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401)
        by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary,
        Jiatao Gu, Angela Fan.1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)**
        (from Meta/USC/CMU/SJTU) released with the paper [Mega: Moving Average Equipped
        Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting
        Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and
        Luke Zettlemoyer.1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)**
        (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion
        Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
        by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared
        Casper and Bryan Catanzaro.1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)**
        (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion
        Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
        by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared
        Casper and Bryan Catanzaro.1. **[MGP-STR](https://huggingface.co/docs/transformers/model_doc/mgp-str)**
        (from Alibaba Research) released with the paper [Multi-Granularity Prediction
        for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang,
        Cheng Da, and Cong Yao.1. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)**
        (from Mistral AI) by The [Mistral AI](https://mistral.ai) team: Albert Jiang,
        Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
        Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L\xE9lio
        Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le
        Scao, Thibaut Lavril, Thomas Wang, Timoth\xE9e Lacroix, William El Sayed.1.
        **[Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral)**
        (from Mistral AI) by The [Mistral AI](https://mistral.ai) team: Albert Jiang,
        Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
        Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L\xE9lio
        Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le
        Scao, Thibaut Lavril, Thomas Wang, Timoth\xE9e Lacroix, William El Sayed.1.
        **[mLUKE](https://huggingface.co/docs/transformers/model_doc/mluke)** (from
        Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations
        in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151)
        by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)**
        (from Facebook) released with the paper [Scaling Speech Technology to 1,000+
        Languages](https://arxiv.org/abs/2305.13516) by Vineel Pratap, Andros Tjandra,
        Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng
        Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui
        Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)**
        (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic
        BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing
        Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.1.
        **[MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1)**
        (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional
        Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)
        by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
        Tobias Weyand, Marco Andreetto, Hartwig Adam.1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)**
        (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals
        and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler,
        Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)**
        (from Apple) released with the paper [MobileViT: Light-weight, General-purpose,
        and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178)
        by Sachin Mehta and Mohammad Rastegari.1. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)**
        (from Apple) released with the paper [Separable Self-attention for Mobile
        Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and
        Mohammad Rastegari.1. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)**
        (from Microsoft Research) released with the paper [MPNet: Masked and Permuted
        Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)
        by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)**
        (from MosaiML) released with the repository [llm-foundry](https://github.com/mosaicml/llm-foundry/)
        by the MosaicML NLP Team.1. **[MRA](https://huggingface.co/docs/transformers/model_doc/mra)**
        (from the University of Wisconsin - Madison) released with the paper [Multi
        Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284)
        by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.1.
        **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google
        AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text
        transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant,
        Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin
        Raffel.1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)**
        (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)
        by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve,
        Yossi Adi and Alexandre D\xE9fossez.1. **[MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody)**
        (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)
        by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve,
        Yossi Adi and Alexandre D\xE9fossez.1. **[MVP](https://huggingface.co/docs/transformers/model_doc/mvp)**
        (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training
        for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi
        Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)**
        (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143)
        by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)**
        (from Huawei Noah\u2019s Ark Lab) released with the paper [NEZHA: Neural Contextualized
        Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204)
        by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng
        Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)**
        (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered
        Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.1.
        **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)**
        (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered
        Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.1.
        **[Nougat](https://huggingface.co/docs/transformers/model_doc/nougat)** (from
        Meta AI) released with the paper [Nougat: Neural Optical Understanding for
        Academic Documents](https://arxiv.org/abs/2308.13418) by Lukas Blecher, Guillem
        Cucurull, Thomas Scialom, Robert Stojnic.1. **[Nystr\xF6mformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)**
        (from the University of Wisconsin - Madison) released with the paper [Nystr\xF6mformer:
        A Nystr\xF6m-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902)
        by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn
        Fung, Yin Li, Vikas Singh.1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)**
        (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule
        Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh
        Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.1.
        **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)**
        (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).1.
        **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from
        Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language
        Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller,
        Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)**
        (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection
        with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer,
        Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,
        Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang,
        Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)**
        (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683)
        by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)**
        (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer
        Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf)
        by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.1.
        **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)**
        (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term
        Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie,
        Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)**
        (from Google) released with the paper [PEGASUS: Pre-training with Extracted
        Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)
        by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)**
        (from Google) released with the paper [Investigating Efficiently Extending
        Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)
        by Jason Phang, Yao Zhao, and Peter J. Liu.1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)**
        (from Deepmind) released with the paper [Perceiver IO: A General Architecture
        for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew
        Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu,
        David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier
        H\xE9naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Jo\xE3o
        Carreira.1. **[Persimmon](https://huggingface.co/docs/transformers/model_doc/persimmon)**
        (from ADEPT) released in a [blog post](https://www.adept.ai/blog/persimmon-8b)
        by Erich Elsen, Augustus Odena, Maxwell Nye, Sa\u011Fnak Ta\u015F\u0131rlar,
        Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.1. **[Phi](https://huggingface.co/docs/transformers/model_doc/phi)**
        (from Microsoft) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)
        by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\xE9sar Teodoro Mendes,
        Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
        de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin
        Wang, S\xE9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and
        Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)
        by Yuanzhi Li, S\xE9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya
        Gunasekar and Yin Tat Lee.1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)**
        (from VinAI Research) released with the paper [PhoBERT: Pre-trained language
        models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/)
        by Dat Quoc Nguyen and Anh Tuan Nguyen.1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)**
        (from Google) released with the paper [Pix2Struct: Screenshot Parsing as Pretraining
        for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton
        Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos,
        Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)**
        (from UCLA NLP) released with the paper [Unified Pre-training for Program
        Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin
        Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.1. **[PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer)**
        (from Sea AI Labs) released with the paper [MetaFormer is Actually What You
        Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo,
        Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng,
        Jiashi and Yan, Shuicheng.1. **[Pop2Piano](https://huggingface.co/docs/transformers/model_doc/pop2piano)**
        released with the paper [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895)
        by Jongho Choi and Kyogu Lee.1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)**
        (from Microsoft Research) released with the paper [ProphetNet: Predicting
        Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063)
        by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen,
        Ruofei Zhang and Ming Zhou.1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)**
        (from Nanjing University, The University of Hong Kong etc.) released with
        the paper [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction
        without Convolutions](https://arxiv.org/pdf/2102.12122.pdf) by Wenhai Wang,
        Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
        Luo, Ling Shao.1. **[PVTv2](https://huggingface.co/docs/transformers/model_doc/pvt_v2)**
        (from Shanghai AI Laboratory, Nanjing University, The University of Hong Kong
        etc.) released with the paper [PVT v2: Improved Baselines with Pyramid Vision
        Transformer](https://arxiv.org/abs/2106.13797) by Wenhai Wang, Enze Xie, Xiang
        Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.1.
        **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)**
        (from NVIDIA) released with the paper [Integer Quantization for Deep Learning
        Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602)
        by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.1.
        **[Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2)** (from
        the Qwen team, Alibaba Group) released with the paper [Qwen Technical Report](https://arxiv.org/abs/2309.16609)
        by Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang
        Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
        Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
        Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng
        Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao
        Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan,
        Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren
        Zhou, Xiaohuan Zhou and Tianhang Zhu.1. **[RAG](https://huggingface.co/docs/transformers/model_doc/rag)**
        (from Facebook) released with the paper [Retrieval-Augmented Generation for
        Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick
        Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin,
        Naman Goyal, Heinrich K\xFCttler, Mike Lewis, Wen-tau Yih, Tim Rockt\xE4schel,
        Sebastian Riedel, Douwe Kiela.1. **[REALM](https://huggingface.co/docs/transformers/model_doc/realm.html)**
        (from Google Research) released with the paper [REALM: Retrieval-Augmented
        Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu,
        Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)**
        (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
        by Nikita Kitaev, \u0141ukasz Kaiser, Anselm Levskaya.1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)**
        (from META Platforms) released with the paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678)
        by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr
        Doll\xE1r.1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)**
        (from Google Research) released with the paper [Rethinking embedding coupling
        in pre-trained language models](https://arxiv.org/abs/2010.12821) by Hyung
        Won Chung, Thibault F\xE9vry, Henry Tsai, M. Johnson, Sebastian Ruder.1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)**
        (from Microsoft Research) released with the paper [Deep Residual Learning
        for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu
        Zhang, Shaoqing Ren, Jian Sun.1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)**
        (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized
        BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu,
        Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
        Lewis, Luke Zettlemoyer, Veselin Stoyanov.1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)**
        (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit
        for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey
        Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
        Michael Auli.1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)**
        (from WeChatAI) released with the paper [RoCBert: Robust Chinese Bert with
        Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf)
        by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.1. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)**
        (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced
        Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
        by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)**
        (from Bo Peng), released on [this repo](https://github.com/BlinkDL/RWKV-LM)
        by Bo Peng.1. **[SeamlessM4T](https://huggingface.co/docs/transformers/model_doc/seamless_m4t)**
        (from Meta AI) released with the paper [SeamlessM4T \u2014 Massively Multilingual
        & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)
        by the Seamless Communication team.1. **[SeamlessM4Tv2](https://huggingface.co/docs/transformers/model_doc/seamless_m4t_v2)**
        (from Meta AI) released with the paper [Seamless: Multilingual Expressive
        and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/)
        by the Seamless Communication team.1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)**
        (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design
        for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203)
        by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping
        Luo.1. **[SegGPT](https://huggingface.co/docs/transformers/model_doc/seggpt)**
        (from Beijing Academy of Artificial Intelligence (BAAI)) released with the
        paper [SegGPT: Segmenting Everything In Context](https://arxiv.org/abs/2304.03284)
        by Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang.1.
        **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)**
        (from Meta AI) released with the paper [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf)
        by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
        Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr
        Dollar, Ross Girshick.1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)**
        (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in
        Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870)
        by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav
        Artzi.1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)**
        (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in
        Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870)
        by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav
        Artzi.1. **[SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)**
        (from Google AI) released with the paper [Sigmoid Loss for Language Image
        Pre-Training](https://arxiv.org/abs/2303.15343) by Xiaohua Zhai, Basil Mustafa,
        Alexander Kolesnikov, Lucas Beyer.1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)**
        (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal
        Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205)
        by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu,
        Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)**
        (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text
        Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang,
        Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)**
        (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised
        Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan
        Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.1.
        **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)**
        (from Tel Aviv University), released together with the paper [Few-Shot Question
        Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438)
        by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.1.
        **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)**
        (from Berkeley) released with the paper [SqueezeBERT: What can computer vision
        teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316)
        by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.1.
        **[StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm)**
        (from Stability AI) released with the paper [StableLM 3B 4E1T (Technical Report)](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo)
        by Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz, Duy
        Phung, Maksym Zhuravinskyi, Nathan Cooper, Nikhil Pinnaparaju, Reshinth Adithyan,
        and James Baicoianu.1. **[Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2)**
        (from BigCode team) released with the paper [StarCoder 2 and The Stack v2:
        The Next Generation](https://arxiv.org/abs/2402.19173) by Anton Lozhkov, Raymond
        Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi,
        Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian,
        Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry
        Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li,
        Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao
        Yu, Lucas Krau\xDF, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati,
        Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher
        Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri
        Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley,
        Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane
        Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite,
        Carlos Mu\xF1oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun
        Guha, Leandro von Werra, and Harm de Vries.1. **[SuperPoint](https://huggingface.co/docs/transformers/model_doc/superpoint)**
        (from MagicLeap) released with the paper [SuperPoint: Self-Supervised Interest
        Point Detection and Description](https://arxiv.org/abs/1712.07629) by Daniel
        DeTone, Tomasz Malisiewicz and Andrew Rabinovich.1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)**
        (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention
        for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446)
        by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan
        Yang, Fahad Shahbaz Khan.1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)**
        (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision
        Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze
        Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining
        Guo.1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)**
        (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up
        Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han
        Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng
        Zhang, Li Dong, Furu Wei, Baining Guo.1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)**
        (from University of W\xFCrzburg) released with the paper [Swin2SR: SwinV2
        Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)
        by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.1. **[SwitchTransformers](https://huggingface.co/docs/transformers/model_doc/switch_transformers)**
        (from Google) released with the paper [Switch Transformers: Scaling to Trillion
        Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
        by William Fedus, Barret Zoph, Noam Shazeer.1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)**
        (from Google AI) released with the paper [Exploring the Limits of Transfer
        Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
        by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan
        Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)**
        (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)
        by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan
        Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.1. **[Table
        Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)**
        (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive
        Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061)
        by Brandon Smock, Rohith Pesala, Robin Abraham.1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)**
        (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing
        via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Pawe\u0142
        Krzysztof Nowak, Thomas M\xFCller, Francesco Piccinno and Julian Martin Eisenschlos.1.
        **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from
        Microsoft Research) released with the paper [TAPEX: Table Pre-training via
        Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian
        Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang
        Lou.1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)**
        (from HuggingFace).1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)**
        (from Facebook) released with the paper [Is Space-Time Attention All You Need
        for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius,
        Heng Wang, Lorenzo Torresani.1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)**
        (from the University of California at Berkeley) released with the paper [Offline
        Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039)
        by Michael Janner, Qiyang Li, Sergey Levine1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)**
        (from Google/CMU) released with the paper [Transformer-XL: Attentive Language
        Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by
        Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan
        Salakhutdinov.1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)**
        (from Microsoft), released together with the paper [TrOCR: Transformer-based
        Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282)
        by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,
        Zhoujun Li, Furu Wei.1. **[TVLT](https://huggingface.co/docs/transformers/model_doc/tvlt)**
        (from UNC Chapel Hill) released with the paper [TVLT: Textless Vision-Language
        Transformer](https://arxiv.org/abs/2209.14156) by Zineng Tang, Jaemin Cho,
        Yixin Nie, Mohit Bansal.1. **[TVP](https://huggingface.co/docs/transformers/model_doc/tvp)**
        (from Intel) released with the paper [Text-Visual Prompting for Efficient
        2D Temporal Video Grounding](https://arxiv.org/abs/2303.04995) by Yimeng Zhang,
        Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.1. **[UDOP](https://huggingface.co/docs/transformers/model_doc/udop)**
        (from Microsoft Research) released with the paper [Unifying Vision, Text,
        and Layout for Universal Document Processing](https://arxiv.org/abs/2212.02623)
        by Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu,
        Michael Zeng, Cha Zhang, Mohit Bansal.1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)**
        (from Google Research) released with the paper [Unifying Language Learning
        Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani,
        Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,
        Neil Houlsby, Donald Metzler1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)**
        (from Google Research) released with the paper [UniMax: Fairer and More Effective
        Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi)
        by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan
        Narang, Noah Constant.1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)**
        (from Microsoft Research) released with the paper [UniSpeech: Unified Speech
        Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597)
        by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
        Michael Zeng, Xuedong Huang.1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)**
        (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL
        SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752)
        by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu,
        Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.1. **[UnivNet](https://huggingface.co/docs/transformers/model_doc/univnet)**
        (from Kakao Corporation) released with the paper [UnivNet: A Neural Vocoder
        with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform
        Generation](https://arxiv.org/abs/2106.07889) by Won Jang, Dan Lim, Jaesam
        Yoon, Bongwan Kim, and Juntae Kim.1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)**
        (from Peking University) released with the paper [Unified Perceptual Parsing
        for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng
        Liu, Bolei Zhou, Yuning Jiang, Jian Sun.1. **[VAN](https://huggingface.co/docs/transformers/model_doc/van)**
        (from Tsinghua University and Nankai University) released with the paper [Visual
        Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze
        Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)**
        (from Multimedia Computing Group, Nanjing University) released with the paper
        [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised
        Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing
        Song, Jue Wang, Limin Wang.1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)**
        (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT:
        Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334)
        by Wonjae Kim, Bokyung Son, Ildoo Kim.1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)**
        (from University of Wisconsin\u2013Madison) released with the paper [Making
        Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784)
        by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning
        Chai, Dennis Park, Yong Jae Lee.1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)**
        (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers
        for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey
        Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
        Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
        Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)**
        (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant
        Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian
        Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)**
        (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers
        for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey
        Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
        Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
        Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)**
        (from Meta AI) released with the paper [Exploring Plain Vision Transformer
        Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao
        Li, Hanzi Mao, Ross Girshick, Kaiming He.1. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)**
        (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision
        Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining
        Xie, Yanghao Li, Piotr Doll\xE1r, Ross Girshick.1. **[ViTMatte](https://huggingface.co/docs/transformers/model_doc/vitmatte)**
        (from HUST-VL) released with the paper [ViTMatte: Boosting Image Matting with
        Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by
        Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)**
        (from Meta AI) released with the paper [Masked Siamese Networks for Label-Efficient
        Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron,
        Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin,
        Michael Rabbat, Nicolas Ballas.1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)**
        (from Kakao Enterprise) released with the paper [Conditional Variational Autoencoder
        with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103)
        by Jaehyeon Kim, Jungil Kong, Juhee Son.1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)**
        (from Google Research) released with the paper [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691)
        by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010Di\u0107,
        Cordelia Schmid.1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)**
        (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised
        Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei
        Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.1. **[Wav2Vec2-BERT](https://huggingface.co/docs/transformers/model_doc/wav2vec2-bert)**
        (from Meta AI) released with the paper [Seamless: Multilingual Expressive
        and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/)
        by the Seamless Communication team.1. **[Wav2Vec2-Conformer](https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer)**
        (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text
        Modeling with FAIRSEQ](https://arxiv.org/abs/2010.05171) by Changhan Wang,
        Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)**
        (from Facebook AI) released with the paper [Simple and Effective Zero-shot
        Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong
        Xu, Alexei Baevski, Michael Auli.1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)**
        (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised
        Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900)
        by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,
        Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou,
        Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)**
        (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale
        Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford,
        Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.1.
        **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (from
        Microsoft Research) released with the paper [Expanding Language-Image Pretrained
        Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by
        Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong
        Fu, Shiming Xiang, Haibin Ling.1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)**
        (from Meta AI) released with the paper [Lifting the Curse of Multilinguality
        by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255)
        by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel,
        Mikel Artetxe.1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)**
        (From Facebook AI) released with the paper [Few-shot Learning with Multilingual
        Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor
        Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott,
        Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer,
        Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer,
        Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)**
        (from Facebook) released together with the paper [Cross-lingual Language Model
        Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis
        Conneau.1. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlm-prophetnet)**
        (from Microsoft Research) released with the paper [ProphetNet: Predicting
        Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063)
        by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen,
        Ruofei Zhang and Ming Zhou.1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)**
        (from Facebook AI), released together with the paper [Unsupervised Cross-lingual
        Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis
        Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume
        Wenzek, Francisco Guzm\xE1n, Edouard Grave, Myle Ott, Luke Zettlemoyer and
        Veselin Stoyanov.1. **[XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl)**
        (from Facebook AI), released together with the paper [Larger-Scale Transformers
        for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572)
        by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.1.
        **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (from
        Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck
        in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472)
        by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad,
        Luke Zettlemoyer, Madian Khabsa.1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)**
        (from Google/CMU) released with the paper [XLNet: Generalized Autoregressive
        Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
        by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
        Quoc V. Le.1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)**
        (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual
        Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296)
        by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu,
        Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino,
        Alexei Baevski, Alexis Conneau, Michael Auli.1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)**
        (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation
        Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis
        Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.1.
        **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (from
        Huazhong University of Science & Technology) released with the paper [You
        Only Look at One Sequence: Rethinking Transformer in Vision through Object
        Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao,
        Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.1. **[YOSO](https://huggingface.co/docs/transformers/model_doc/yoso)**
        (from the University of Wisconsin - Madison) released with the paper [You
        Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714)
        by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung,
        Vikas Singh.1. Want to contribute a new model? We have added a **detailed
        guide and templates** to guide you in the process of adding a new model. You
        can find them in the [`templates`](./templates) folder of the repository.
        Be sure to check the [contributing guidelines](./CONTRIBUTING.md) and contact
        the maintainers or open an issue to collect feedback before starting your
        PR.To check if each model has an implementation in Flax, PyTorch or TensorFlow,
        or has an associated tokenizer backed by the \U0001F917 Tokenizers library,
        refer to [this table](https://huggingface.co/docs/transformers/index#supported-frameworks).These
        implementations have been tested on several datasets (see the example scripts)
        and should match the performance of the original implementations. You can
        find more details on performance in the Examples section of the [documentation](https://github.com/huggingface/transformers/tree/main/examples).##
        Learn more| Section | Description ||-|-|| [Documentation](https://huggingface.co/docs/transformers/)
        | Full API documentation and tutorials || [Task summary](https://huggingface.co/docs/transformers/task_summary)
        | Tasks supported by \U0001F917 Transformers || [Preprocessing tutorial](https://huggingface.co/docs/transformers/preprocessing)
        | Using the `Tokenizer` class to prepare data for the models || [Training
        and fine-tuning](https://huggingface.co/docs/transformers/training) | Using
        the models provided by \U0001F917 Transformers in a PyTorch/TensorFlow training
        loop and the `Trainer` API || [Quick tour: Fine-tuning/usage scripts](https://github.com/huggingface/transformers/tree/main/examples)
        | Example scripts for fine-tuning models on a wide range of tasks || [Model
        sharing and uploading](https://huggingface.co/docs/transformers/model_sharing)
        | Upload and share your fine-tuned models with the community |## CitationWe
        now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you
        can cite for the \U0001F917 Transformers library:```bibtex@inproceedings{wolf-etal-2020-transformers,
        \   title = \"Transformers: State-of-the-Art Natural Language Processing\",
        \   author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond
        and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and
        R\xE9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick
        von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and
        Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and
        Alexander M. Rush\",    booktitle = \"Proceedings of the 2020 Conference on
        Empirical Methods in Natural Language Processing: System Demonstrations\",
        \   month = oct,    year = \"2020\",    address = \"Online\",    publisher
        = \"Association for Computational Linguistics\",    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",
        \   pages = \"38--45\"}```"
      Package: transformers
      Source: pip
      Version: 4.38.2
      Hash: ''
      licenses:
      - Apache-2.0
      - MIT
      Title: transformers
      DownloadURL: https://github.com/huggingface/transformers/archive/refs/tags/v4.38.2.tar.gz
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/huggingface/transformers
    community_name: https://github.com/huggingface/transformers
    community_url: https://github.com/huggingface/transformers
    component_comment: ''
    component_highlevel_description: ''
    component_name: transformers
    component_platform: linux
    component_programing_language: ''
    component_version: v4.38.2
    licenses: []
    src_download_link: https://github.com/huggingface/transformers/archive/refs/tags/v4.38.2.tar.gz
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: transformers
    target_sw: linux
    vendor: pip
    version: 4.38.2
    web_url: https://github.com/huggingface/transformers
  licenses:
  - Apache-2.0
  - MIT
  name: transformers
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 4.38.2
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: 4.38.2
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: triton+2.2.0
  additional_info:
    fossa-attribution:
      Description: The fossa information is fetched from  FOSSA service. Don't Edit!
      Package: triton
      Source: pip
      Version: 2.2.0
      Hash: ''
      licenses:
      - MIT
      Title: triton
      DownloadURL: https://files.pythonhosted.org/packages/95/05/ed974ce87fe8c8843855daa2136b3409ee1c126707ab54a8b72815c08b49/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
  bazaar:
    register: 'yes'
    prim: ''
    community_link: https://github.com/openai/triton/
    community_name: https://github.com/openai/triton/
    community_url: https://github.com/openai/triton/
    component_comment: ''
    component_highlevel_description: ''
    component_name: triton
    component_platform: linux
    component_programing_language: ''
    component_version: v2.2.0
    licenses: []
    src_download_link: https://github.com/openai/triton/archive/0e7b97bd47fc4beb21ae960a516cd9a7ae9bc060.zip
    stako_decision_reason: automatic
    stako: DO_NOT_EDIT_MANUALLY
    stako_comment: ''
    bazaarurl: ''
    recode: ''
    retext: ''
    country: ''
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: triton
    target_sw: linux
    vendor: pip
    version: 2.2.0
    web_url: https://github.com/openai/triton/
  licenses:
  - MIT
  name: triton
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 2.2.0
  mimer:
    linking: MANDATORY_FOR_MIMER
    product_number: ''
    product_version_label: ''
    selected_licenses:
    - SELECT_FROM_LICENSES
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: typing-extensions+4.10.0
  additional_info:
    fossa-attribution:
      Description: "# Typing Extensions[![Chat at https://gitter.im/python/typing](https://badges.gitter.im/python/typing.svg)](https://gitter.im/python/typing)[Documentation](https://typing-extensions.readthedocs.io/en/latest/#)
        \u2013[PyPI](https://pypi.org/project/typing-extensions/)## OverviewThe `typing_extensions`
        module serves two related purposes:- Enable use of new type system features
        on older Python versions. For example,  `typing.TypeGuard` is new in Python
        3.10, but `typing_extensions` allows  users on previous Python versions to
        use it too.- Enable experimentation with new type system PEPs before they
        are accepted and  added to the `typing` module.`typing_extensions` is treated
        specially by static type checkers such asmypy and pyright. Objects defined
        in `typing_extensions` are treated the sameway as equivalent forms in `typing`.`typing_extensions`
        uses[Semantic Versioning](https://semver.org/). Themajor version will be incremented
        only for backwards-incompatible changes.Therefore, it's safe to dependon `typing_extensions`
        like this: `typing_extensions >=x.y, <(x+1)`,where `x.y` is the first version
        that includes all features you need.## Included itemsSee [the documentation](https://typing-extensions.readthedocs.io/en/latest/#)
        for acomplete listing of module contents.## ContributingSee [CONTRIBUTING.md](https://github.com/python/typing_extensions/blob/main/CONTRIBUTING.md)for
        how to contribute to `typing_extensions`."
      Package: typing-extensions
      Source: pip
      Version: 4.10.0
      Hash: ''
      licenses:
      - 0BSD
      - Python-2.0
      Title: typing-extensions
      DownloadURL: https://files.pythonhosted.org/packages/16/3a/0d26ce356c7465a19c9ea8814b960f8a36c3b0d07c323176620b7b483e44/typing_extensions-4.10.0.tar.gz
  bazaar:
    register: 'no'
    prim: 17/CTX1033609
    community_link: https://pypi.org/project/typing-extensions/4.10.0/
    community_name: https://pypi.org/project/typing-extensions/4.10.0/
    community_url: https://pypi.org/project/typing-extensions/4.10.0/
    component_comment: ''
    component_highlevel_description: 'This is a backport of the standard library typing
      module to Python versions older than 3.5.

      Typing defines a standard notation for Python function and variable type annotations.
      The notation can be used for documenting code in a concise, standard format,
      and it has been designed to also be used by static and runtime type checkers,
      static analyzers, IDEs and other tools.'
    component_name: typing-extensions
    component_platform: linux
    component_programing_language: Python
    component_version: 4.10.0
    licenses:
    - FAL1159179/20 (Python Software Foundation License 2.0 (PSF-2.0))
    src_download_link: https://files.pythonhosted.org/packages/16/3a/0d26ce356c7465a19c9ea8814b960f8a36c3b0d07c323176620b7b483e44/typing_extensions-4.10.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1080387&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: typing-extensions
    target_sw: linux
    vendor: pip
    version: 4.10.0
    web_url: https://pypi.org/project/typing-extensions/4.11.0/
  licenses:
  - 0BSD
  - Python-2.0
  name: typing-extensions
  primary:
  - optimum+1.17.1
  - pydantic+1.10.14
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 4.10.0
  mimer:
    linking: Static
    product_number: CTX1033609
    product_version_label: 4.10.0
    selected_licenses:
    - PSF-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: tzdata+2024.1
  additional_info:
    fossa-attribution:
      Description: 'tzdata: Python package providing IANA time zone data====================================================This
        is a Python package containing ``zic``-compiled binaries for the IANA timezone
        database. It is intended to be a fallback for systems that do not havesystem
        time zone data installed (or don''t have it installed in a standardlocation),
        as a part of `PEP 615 <https://www.python.org/dev/peps/pep-0615/>`_This repository
        generates a ``pip``-installable package, published on PyPI as`tzdata <https://pypi.org/project/tzdata>`_.For
        more information, see `the documentation <https://tzdata.readthedocs.io>`_.'
      Package: tzdata
      Source: pip
      Version: '2024.1'
      Hash: ''
      licenses:
      - Apache-2.0
      - public-domain
      Title: tzdata
      DownloadURL: https://files.pythonhosted.org/packages/74/5b/e025d02cb3b66b7b76093404392d4b44343c69101cc85f4d180dd5784717/tzdata-2024.1.tar.gz
  bazaar:
    register: 'no'
    prim: 35/CAX1057935
    community_link: https://github.com/python/tzdata
    community_name: https://github.com/python/tzdata
    community_url: https://github.com/python/tzdata
    component_comment: ''
    component_highlevel_description: Python package wrapping the IANA time zone database
    component_name: tzdata
    component_platform: linux
    component_programing_language: Python
    component_version: '2024.1'
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/python/tzdata/archive/2024.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1082232&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: tzdata
    target_sw: linux
    vendor: pip
    version: '2024.1'
    web_url: https://github.com/python/tzdata
  licenses:
  - Apache-2.0
  - public-domain
  name: tzdata
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - '2024.1'
  mimer:
    linking: Static
    product_number: CAX1057935
    product_version_label: '2024.1'
    selected_licenses:
    - Apache-2.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: urllib3+2.2.1
  additional_info:
    fossa-attribution:
      Description: "<h1 align=\"center\">![urllib3](https://github.com/urllib3/urllib3/raw/main/docs/_static/banner_github.svg)</h1><p
        align=\"center\">  <a href=\"https://pypi.org/project/urllib3\"><img alt=\"PyPI
        Version\" src=\"https://img.shields.io/pypi/v/urllib3.svg?maxAge=86400\" /></a>
        \ <a href=\"https://pypi.org/project/urllib3\"><img alt=\"Python Versions\"
        src=\"https://img.shields.io/pypi/pyversions/urllib3.svg?maxAge=86400\" /></a>
        \ <a href=\"https://discord.gg/urllib3\"><img alt=\"Join our Discord\" src=\"https://img.shields.io/discord/756342717725933608?color=%237289da&label=discord\"
        /></a>  <a href=\"https://github.com/urllib3/urllib3/actions?query=workflow%3ACI\"><img
        alt=\"Coverage Status\" src=\"https://img.shields.io/badge/coverage-100%25-success\"
        /></a>  <a href=\"https://github.com/urllib3/urllib3/actions?query=workflow%3ACI\"><img
        alt=\"Build Status on GitHub\" src=\"https://github.com/urllib3/urllib3/workflows/CI/badge.svg\"
        /></a>  <a href=\"https://urllib3.readthedocs.io\"><img alt=\"Documentation
        Status\" src=\"https://readthedocs.org/projects/urllib3/badge/?version=latest\"
        /></a><br>  <a href=\"https://deps.dev/pypi/urllib3\"><img alt=\"OpenSSF Scorecard\"
        src=\"https://api.securityscorecards.dev/projects/github.com/urllib3/urllib3/badge\"
        /></a>  <a href=\"https://slsa.dev\"><img alt=\"SLSA 3\" src=\"https://slsa.dev/images/gh-badge-level3.svg\"
        /></a>  <a href=\"https://bestpractices.coreinfrastructure.org/projects/6227\"><img
        alt=\"CII Best Practices\" src=\"https://bestpractices.coreinfrastructure.org/projects/6227/badge\"
        /></a></p>urllib3 is a powerful, *user-friendly* HTTP client for Python. Much
        of thePython ecosystem already uses urllib3 and you should too.urllib3 brings
        many critical features that are missing from the Pythonstandard libraries:-
        Thread safety.- Connection pooling.- Client-side SSL/TLS verification.- File
        uploads with multipart encoding.- Helpers for retrying requests and dealing
        with HTTP redirects.- Support for gzip, deflate, brotli, and zstd encoding.-
        Proxy support for HTTP and SOCKS.- 100% test coverage.urllib3 is powerful
        and easy to use:```python3>>> import urllib3>>> resp = urllib3.request(\"GET\",
        \"http://httpbin.org/robots.txt\")>>> resp.status200>>> resp.datab\"User-agent:
        *\\nDisallow: /deny\\n\"```## Installingurllib3 can be installed with [pip](https://pip.pypa.io):```bash$
        python -m pip install urllib3```Alternatively, you can grab the latest source
        code from [GitHub](https://github.com/urllib3/urllib3):```bash$ git clone
        https://github.com/urllib3/urllib3.git$ cd urllib3$ pip install .```## Documentationurllib3
        has usage and reference documentation at [urllib3.readthedocs.io](https://urllib3.readthedocs.io).##
        Communityurllib3 has a [community Discord channel](https://discord.gg/urllib3)
        for asking questions andcollaborating with other contributors. Drop by and
        say hello \U0001F44B## Contributingurllib3 happily accepts contributions.
        Please see our[contributing documentation](https://urllib3.readthedocs.io/en/latest/contributing.html)for
        some tips on getting started.## Security DisclosuresTo report a security vulnerability,
        please use the[Tidelift security contact](https://tidelift.com/security).Tidelift
        will coordinate the fix and disclosure with maintainers.## Maintainers- [@sethmlarson](https://github.com/sethmlarson)
        (Seth M. Larson)- [@pquentin](https://github.com/pquentin) (Quentin Pradet)-
        [@illia-v](https://github.com/illia-v) (Illia Volochii)- [@theacodes](https://github.com/theacodes)
        (Thea Flowers)- [@haikuginger](https://github.com/haikuginger) (Jess Shapiro)-
        [@lukasa](https://github.com/lukasa) (Cory Benfield)- [@sigmavirus24](https://github.com/sigmavirus24)
        (Ian Stapleton Cordasco)- [@shazow](https://github.com/shazow) (Andrey Petrov)\U0001F44B##
        SponsorshipIf your company benefits from this library, please consider [sponsoring
        itsdevelopment](https://urllib3.readthedocs.io/en/latest/sponsors.html).##
        For EnterpriseProfessional support for urllib3 is available as part of the
        [TideliftSubscription][1].  Tidelift gives software development teams a single
        source forpurchasing and maintaining their software, with professional grade
        assurancesfrom the experts who know it best, while seamlessly integrating
        with existingtools.[1]: https://tidelift.com/subscription/pkg/pypi-urllib3?utm_source=pypi-urllib3&utm_medium=referral&utm_campaign=readme"
      Package: urllib3
      Source: pip
      Version: 2.2.1
      Hash: ''
      licenses:
      - MIT
      - MPL-2.0
      - PSF-2.0
      Title: urllib3
      DownloadURL: https://files.pythonhosted.org/packages/7a/50/7fd50a27caa0652cd4caf224aa87741ea41d3265ad13f010886167cfcc79/urllib3-2.2.1.tar.gz
  bazaar:
    register: 'no'
    prim: 53/CAX1057288
    community_link: https://pypi.org/project/urllib3/
    community_name: https://pypi.org/project/urllib3/
    community_url: https://pypi.org/project/urllib3/
    component_comment: ''
    component_highlevel_description: Python HTTP library with thread-safe connection
      pooling, file post support, sanity friendly, and more.
    component_name: urllib3
    component_platform: linux
    component_programing_language: Python
    component_version: 2.2.1
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://files.pythonhosted.org/packages/7a/50/7fd50a27caa0652cd4caf224aa87741ea41d3265ad13f010886167cfcc79/urllib3-2.2.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1079620&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Canada
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: urllib3
    target_sw: linux
    vendor: pip
    version: 2.2.1
    web_url: https://pypi.org/project/urllib3/2.2.1/
  licenses:
  - MIT
  - MPL-2.0
  - PSF-2.0
  name: urllib3
  primary:
  - optimum+1.17.1
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 2.2.1
  mimer:
    linking: Static
    product_number: CAX1057288
    product_version_label: 2.2.1
    selected_licenses:
    - MIT
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: Werkzeug+3.0.1
  additional_info:
    fossa-attribution:
      Description: 'Werkzeug========*werkzeug* German noun: "tool". Etymology: *werk*
        ("work"), *zeug* ("stuff")Werkzeug is a comprehensive `WSGI`_ web application
        library. It began asa simple collection of various utilities for WSGI applications
        and hasbecome one of the most advanced WSGI utility libraries.It includes:-   An
        interactive debugger that allows inspecting stack traces and    source code
        in the browser with an interactive interpreter for any    frame in the stack.-   A
        full-featured request object with objects to interact with    headers, query
        args, form data, files, and cookies.-   A response object that can wrap other
        WSGI applications and handle    streaming data.-   A routing system for matching
        URLs to endpoints and generating URLs    for endpoints, with an extensible
        system for capturing variables    from URLs.-   HTTP utilities to handle entity
        tags, cache control, dates, user    agents, cookies, files, and more.-   A
        threaded WSGI server for use while developing applications    locally.-   A
        test client for simulating HTTP requests during testing without    requiring
        running a server.Werkzeug doesn''t enforce any dependencies. It is up to the
        developer tochoose a template engine, database adapter, and even how to handlerequests.
        It can be used to build all sorts of end user applicationssuch as blogs, wikis,
        or bulletin boards.`Flask`_ wraps Werkzeug, using it to handle the details
        of WSGI whileproviding more structure and patterns for defining powerfulapplications...
        _WSGI: https://wsgi.readthedocs.io/en/latest/.. _Flask: https://www.palletsprojects.com/p/flask/Installing----------Install
        and update using `pip`_:.. code-block:: text    pip install -U Werkzeug..
        _pip: https://pip.pypa.io/en/stable/getting-started/A Simple Example----------------..
        code-block:: python    from werkzeug.wrappers import Request, Response    @Request.application    def
        application(request):        return Response(''Hello, World!'')    if __name__
        == ''__main__'':        from werkzeug.serving import run_simple        run_simple(''localhost'',
        4000, application)Donate------The Pallets organization develops and supports
        Werkzeug and otherpopular packages. In order to grow the community of contributors
        andusers, and allow the maintainers to devote more time to the projects,`please
        donate today`_... _please donate today: https://palletsprojects.com/donateLinks------   Documentation:
        https://werkzeug.palletsprojects.com/-   Changes: https://werkzeug.palletsprojects.com/changes/-   PyPI
        Releases: https://pypi.org/project/Werkzeug/-   Source Code: https://github.com/pallets/werkzeug/-   Issue
        Tracker: https://github.com/pallets/werkzeug/issues/-   Chat: https://discord.gg/pallets'
      Package: Werkzeug
      Source: pip
      Version: 3.0.1
      Hash: ''
      licenses:
      - BSD-3-Clause
      Title: Werkzeug
      DownloadURL: https://files.pythonhosted.org/packages/0d/cc/ff1904eb5eb4b455e442834dabf9427331ac0fa02853bf83db817a7dd53d/werkzeug-3.0.1.tar.gz
  bazaar:
    register: 'no'
    prim: 29/CAX1057065
    community_link: https://pypi.org/project/Werkzeug/3.0.1/
    community_name: https://pypi.org/project/Werkzeug/3.0.1/
    community_url: https://pypi.org/project/Werkzeug/3.0.1/
    component_comment: ''
    component_highlevel_description: 'Werkzeug started as simple collection of various
      utilities for WSGI applications

      and has become one of the most advanced WSGI utility modules.'
    component_name: werkzeug
    component_platform: linux
    component_programing_language: Python
    component_version: 3.0.1
    licenses:
    - FAL1159003/2 (BSD 3-Clause (BSD-3-Clause))
    src_download_link: https://files.pythonhosted.org/packages/0d/cc/ff1904eb5eb4b455e442834dabf9427331ac0fa02853bf83db817a7dd53d/werkzeug-3.0.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1066442&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Austria
    crypto: No Encryption
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: Werkzeug
    target_sw: linux
    vendor: pip
    version: 3.0.1
    web_url: https://pypi.org/project/Werkzeug/3.0.1/
  licenses:
  - BSD-3-Clause
  name: Werkzeug
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 3.0.1
  mimer:
    linking: Static
    product_number: CAX1057065
    product_version_label: 3.0.1
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: wheel+0.42.0
  additional_info:
    fossa-attribution:
      Description: 'wheel=====This library is the reference implementation of the
        Python wheel packagingstandard, as defined in `PEP 427`_.It has two different
        roles:#. A setuptools_ extension for building wheels that provides the   ``bdist_wheel``
        setuptools command#. A command line tool for working with wheel filesIt should
        be noted that wheel is **not** intended to be used as a library, andas such
        there is no stable, public API... _PEP 427: https://www.python.org/dev/peps/pep-0427/..
        _setuptools: https://pypi.org/project/setuptools/Documentation-------------The
        documentation_ can be found on Read The Docs... _documentation: https://wheel.readthedocs.io/Code
        of Conduct---------------Everyone interacting in the wheel project''s codebases,
        issue trackers, chatrooms, and mailing lists is expected to follow the `PSF
        Code of Conduct`_... _PSF Code of Conduct: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md'
      Package: wheel
      Source: pip
      Version: 0.42.0
      Hash: ''
      licenses:
      - Apache-2.0
      - BSD-3-Clause
      - MIT
      Title: wheel
      DownloadURL: https://files.pythonhosted.org/packages/b0/b4/bc2baae3970c282fae6c2cb8e0f179923dceb7eaffb0e76170628f9af97b/wheel-0.42.0.tar.gz
  bazaar:
    register: 'no'
    prim: 21/CAX1058858
    community_link: https://github.com/pypa/wheel
    community_name: https://github.com/pypa/wheel
    community_url: https://github.com/pypa/wheel
    component_comment: ''
    component_highlevel_description: The official binary distribution format for Python
    component_name: wheel
    component_platform: linux
    component_programing_language: Python
    component_version: 0.42.0
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://github.com/pypa/wheel/archive/0.42.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1072241&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Finland
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: wheel
    target_sw: linux
    vendor: pip
    version: 0.42.0
    web_url: https://pypi.org/project/wheel/0.43.0/
  licenses:
  - Apache-2.0
  - BSD-3-Clause
  - MIT
  name: wheel
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 0.42.0
  mimer:
    linking: Static
    product_number: CAX1058858
    product_version_label: 0.42.0
    selected_licenses:
    - MIT
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: wrapt+1.14.1
  additional_info:
    fossa-attribution:
      Description: 'wrapt=====|Actions| |PyPI|The aim of the **wrapt** module is to
        provide a transparent object proxyfor Python, which can be used as the basis
        for the construction of functionwrappers and decorator functions.The **wrapt**
        module focuses very much on correctness. It therefore goesway beyond existing
        mechanisms such as ``functools.wraps()`` to ensure thatdecorators preserve
        introspectability, signatures, type checking abilitiesetc. The decorators
        that can be constructed using this module will work infar more scenarios than
        typical decorators and provide more predictable andconsistent behaviour.To
        ensure that the overhead is as minimal as possible, a C extension moduleis
        used for performance critical components. An automatic fallback to apure Python
        implementation is also provided where a target system does nothave a compiler
        to allow the C extension to be compiled.Documentation-------------For further
        information on the **wrapt** module see:* http://wrapt.readthedocs.org/Quick
        Start-----------To implement your decorator you need to first define a wrapper
        function.This will be called each time a decorated function is called. The
        wrapperfunction needs to take four positional arguments:* ``wrapped`` - The
        wrapped function which in turns needs to be called by your wrapper function.*
        ``instance`` - The object to which the wrapped function was bound when it
        was called.* ``args`` - The list of positional arguments supplied when the
        decorated function was called.* ``kwargs`` - The dictionary of keyword arguments
        supplied when the decorated function was called.The wrapper function would
        do whatever it needs to, but would usually inturn call the wrapped function
        that is passed in via the ``wrapped``argument.The decorator ``@wrapt.decorator``
        then needs to be applied to the wrapperfunction to convert it into a decorator
        which can in turn be applied toother functions... code-block:: python    import
        wrapt        @wrapt.decorator    def pass_through(wrapped, instance, args,
        kwargs):        return wrapped(*args, **kwargs)    @pass_through    def function():        passIf
        you wish to implement a decorator which accepts arguments, then wrap thedefinition
        of the decorator in a function closure. Any arguments suppliedto the outer
        function when the decorator is applied, will be available tothe inner wrapper
        when the wrapped function is called... code-block:: python    import wrapt    def
        with_arguments(myarg1, myarg2):        @wrapt.decorator        def wrapper(wrapped,
        instance, args, kwargs):            return wrapped(*args, **kwargs)        return
        wrapper    @with_arguments(1, 2)    def function():        passWhen applied
        to a normal function or static method, the wrapper functionwhen called will
        be passed ``None`` as the ``instance`` argument.When applied to an instance
        method, the wrapper function when called willbe passed the instance of the
        class the method is being called on as the``instance`` argument. This will
        be the case even when the instance methodwas called explicitly via the class
        and the instance passed as the firstargument. That is, the instance will never
        be passed as part of ``args``.When applied to a class method, the wrapper
        function when called will bepassed the class type as the ``instance`` argument.When
        applied to a class, the wrapper function when called will be passed``None``
        as the ``instance`` argument. The ``wrapped`` argument in thiscase will be
        the class.The above rules can be summarised with the following example...
        code-block:: python    import inspect        @wrapt.decorator    def universal(wrapped,
        instance, args, kwargs):        if instance is None:            if inspect.isclass(wrapped):                #
        Decorator was applied to a class.                return wrapped(*args, **kwargs)            else:                #
        Decorator was applied to a function or staticmethod.                return
        wrapped(*args, **kwargs)        else:            if inspect.isclass(instance):                #
        Decorator was applied to a classmethod.                return wrapped(*args,
        **kwargs)            else:                # Decorator was applied to an instancemethod.                return
        wrapped(*args, **kwargs)Using these checks it is therefore possible to create
        a universal decoratorthat can be applied in all situations. It is no longer
        necessary to createdifferent variants of decorators for normal functions and
        instance methods,or use additional wrappers to convert a function decorator
        into one thatwill work for instance methods.In all cases, the wrapped function
        passed to the wrapper function is calledin the same way, with ``args`` and
        ``kwargs`` being passed. The``instance`` argument doesn''t need to be used
        in calling the wrappedfunction.Repository----------Full source code for the
        **wrapt** module, including documentation filesand unit tests, can be obtained
        from github.* https://github.com/GrahamDumpleton/wrapt.. |Actions| image::
        https://img.shields.io/github/workflow/status/GrahamDumpleton/wrapt/Test/develop?logo=github&cacheSeconds=600   :target:
        https://github.com/GrahamDumpleton/wrapt/actions.. |PyPI| image:: https://img.shields.io/pypi/v/wrapt.svg?logo=python&cacheSeconds=3600   :target:
        https://pypi.python.org/pypi/wrapt'
      Package: wrapt
      Source: pip
      Version: 1.14.1
      Hash: ''
      licenses:
      - BSD-2-Clause
      Title: wrapt
      DownloadURL: https://files.pythonhosted.org/packages/11/eb/e06e77394d6cf09977d92bff310cb0392930c08a338f99af6066a5a98f92/wrapt-1.14.1.tar.gz
  bazaar:
    register: 'no'
    prim: 14/CAX1058303
    community_link: https://github.com/GrahamDumpleton/wrapt
    community_name: https://github.com/GrahamDumpleton/wrapt
    community_url: https://github.com/GrahamDumpleton/wrapt
    component_comment: ''
    component_highlevel_description: A Python module for decorators, wrappers and
      monkey patching. The aim of the wrapt module is to provide a transparent object
      proxy for Python, which can be used as the basis for the construction of function
      wrappers and decorator functions.
    component_name: wrapt, Python
    component_platform: linux
    component_programing_language: Python
    component_version: 1.14.1
    licenses:
    - FAL1159003/1 (BSD 2-Clause "Simplified" License (BSD-2-Clause))
    src_download_link: https://github.com/GrahamDumpleton/wrapt/archive/refs/tags/1.14.1.zip
    stako_decision_reason: allowed
    stako: ESW3
    stako_comment: Product version is older than 18 months
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1029427&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Australia
    crypto: 'NO'
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: wrapt
    target_sw: linux
    vendor: pip
    version: 1.14.1
    web_url: https://github.com/GrahamDumpleton/wrapt
  licenses:
  - BSD-2-Clause
  name: wrapt
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 1.14.1
  mimer:
    linking: Static
    product_number: CAX1058303
    product_version_label: 1.14.1
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: xxhash+3.4.1
  additional_info:
    fossa-attribution:
      Description: "python-xxhash=============.. image:: https://github.com/ifduyue/python-xxhash/actions/workflows/test.yml/badge.svg
        \   :target: https://github.com/ifduyue/python-xxhash/actions/workflows/test.yml
        \   :alt: Github Actions Status.. image:: https://img.shields.io/pypi/v/xxhash.svg
        \   :target: https://pypi.org/project/xxhash/    :alt: Latest Version.. image::
        https://img.shields.io/pypi/pyversions/xxhash.svg    :target: https://pypi.org/project/xxhash/
        \   :alt: Supported Python versions.. image:: https://img.shields.io/pypi/l/xxhash.svg
        \   :target: https://pypi.org/project/xxhash/    :alt: License.. _HMAC: http://en.wikipedia.org/wiki/Hash-based_message_authentication_code..
        _xxHash: https://github.com/Cyan4973/xxHash.. _Cyan4973: https://github.com/Cyan4973xxhash
        is a Python binding for the xxHash_ library by `Yann Collet`__.__ Cyan4973_Installation------------..
        code-block:: bash   $ pip install xxhash   You can also install using conda:..
        code-block:: bash   $ conda install -c conda-forge python-xxhashInstalling
        From Source~~~~~~~~~~~~~~~~~~~~~~~.. code-block:: bash   $ pip install --no-binary
        xxhash xxhashPrerequisites++++++++++++++On Debian/Ubuntu:.. code-block:: bash
        \  $ apt-get install python-dev gccOn CentOS/Fedora:.. code-block:: bash   $
        yum install python-devel gcc redhat-rpm-configLinking to libxxhash.so~~~~~~~~~~~~~~~~~~~~~~~~By
        default python-xxhash will use bundled xxHash,we can change this by specifying
        ENV var ``XXHASH_LINK_SO``:.. code-block:: bash   $ XXHASH_LINK_SO=1 pip install
        --no-binary xxhash xxhashUsage--------Module version and its backend xxHash
        library version can be retrieved usingthe module properties ``VERSION`` AND
        ``XXHASH_VERSION`` respectively... code-block:: python    >>> import xxhash
        \   >>> xxhash.VERSION    '2.0.0'    >>> xxhash.XXHASH_VERSION    '0.8.0'This
        module is hashlib-compliant, which means you can use it in the same way as
        ``hashlib.md5``.    | update() -- update the current digest with an additional
        string    | digest() -- return the current digest value    | hexdigest() --
        return the current digest as a string of hexadecimal digits    | intdigest()
        -- return the current digest as an integer    | copy() -- return a copy of
        the current xxhash object    | reset() -- reset statemd5 digest returns bytes,
        but the original xxh32 and xxh64 C APIs return integers.While this module
        is made hashlib-compliant, ``intdigest()`` is also provided toget the integer
        digest.Constructors for hash algorithms provided by this module are ``xxh32()``
        and ``xxh64()``.For example, to obtain the digest of the byte string ``b'Nobody
        inspects the spammish repetition'``:.. code-block:: python    >>> import xxhash
        \   >>> x = xxhash.xxh32()    >>> x.update(b'Nobody inspects')    >>> x.update(b'
        the spammish repetition')    >>> x.digest()    b'\\xe2);/'    >>> x.digest_size
        \   4    >>> x.block_size    16More condensed:.. code-block:: python    >>>
        xxhash.xxh32(b'Nobody inspects the spammish repetition').hexdigest()    'e2293b2f'
        \   >>> xxhash.xxh32(b'Nobody inspects the spammish repetition').digest()
        == x.digest()    TrueAn optional seed (default is 0) can be used to alter
        the result predictably:.. code-block:: python    >>> import xxhash    >>>
        xxhash.xxh64('xxhash').hexdigest()    '32dd38952c4bc720'    >>> xxhash.xxh64('xxhash',
        seed=20141025).hexdigest()    'b559b98d844e0635'    >>> x = xxhash.xxh64(seed=20141025)
        \   >>> x.update('xxhash')    >>> x.hexdigest()    'b559b98d844e0635'    >>>
        x.intdigest()    13067679811253438005Be careful that xxh32 takes an unsigned
        32-bit integer as seed, while xxh64takes an unsigned 64-bit integer. Although
        unsigned integer overflow isdefined behavior, it's better not to make it happen:..
        code-block:: python    >>> xxhash.xxh32('I want an unsigned 32-bit seed!',
        seed=0).hexdigest()    'f7a35af8'    >>> xxhash.xxh32('I want an unsigned
        32-bit seed!', seed=2**32).hexdigest()    'f7a35af8'    >>> xxhash.xxh32('I
        want an unsigned 32-bit seed!', seed=1).hexdigest()    'd8d4b4ba'    >>> xxhash.xxh32('I
        want an unsigned 32-bit seed!', seed=2**32+1).hexdigest()    'd8d4b4ba'    >>>
        \   >>> xxhash.xxh64('I want an unsigned 64-bit seed!', seed=0).hexdigest()
        \   'd4cb0a70a2b8c7c1'    >>> xxhash.xxh64('I want an unsigned 64-bit seed!',
        seed=2**64).hexdigest()    'd4cb0a70a2b8c7c1'    >>> xxhash.xxh64('I want
        an unsigned 64-bit seed!', seed=1).hexdigest()    'ce5087f12470d961'    >>>
        xxhash.xxh64('I want an unsigned 64-bit seed!', seed=2**64+1).hexdigest()
        \   'ce5087f12470d961'``digest()`` returns bytes of the **big-endian** representation
        of the integerdigest:.. code-block:: python    >>> import xxhash    >>> h
        = xxhash.xxh64()    >>> h.digest()    b'\\xefF\\xdb7Q\\xd8\\xe9\\x99'    >>>
        h.intdigest().to_bytes(8, 'big')    b'\\xefF\\xdb7Q\\xd8\\xe9\\x99'    >>>
        h.hexdigest()    'ef46db3751d8e999'    >>> format(h.intdigest(), '016x')    'ef46db3751d8e999'
        \   >>> h.intdigest()    17241709254077376921    >>> int(h.hexdigest(), 16)
        \   17241709254077376921Besides xxh32/xxh64 mentioned above, oneshot functions
        are also provided,so we can avoid allocating XXH32/64 state on heap:    |
        xxh32_digest(bytes, seed=0)    | xxh32_intdigest(bytes, seed=0)    | xxh32_hexdigest(bytes,
        seed=0)    | xxh64_digest(bytes, seed=0)    | xxh64_intdigest(bytes, seed=0)
        \   | xxh64_hexdigest(bytes, seed=0).. code-block:: python    >>> import xxhash
        \   >>> xxhash.xxh64('a').digest() == xxhash.xxh64_digest('a')    True    >>>
        xxhash.xxh64('a').intdigest() == xxhash.xxh64_intdigest('a')    True    >>>
        xxhash.xxh64('a').hexdigest() == xxhash.xxh64_hexdigest('a')    True    >>>
        xxhash.xxh64_hexdigest('xxhash', seed=20141025)    'b559b98d844e0635'    >>>
        xxhash.xxh64_intdigest('xxhash', seed=20141025)    13067679811253438005L    >>>
        xxhash.xxh64_digest('xxhash', seed=20141025)    '\\xb5Y\\xb9\\x8d\\x84N\\x065'..
        code-block:: python    In [1]: import xxhash    In [2]: %timeit xxhash.xxh64_hexdigest('xxhash')
        \   268 ns \xB1 24.1 ns per loop (mean \xB1 std. dev. of 7 runs, 1000000 loops
        each)    In [3]: %timeit xxhash.xxh64('xxhash').hexdigest()    416 ns \xB1
        17.3 ns per loop (mean \xB1 std. dev. of 7 runs, 1000000 loops each)XXH3 hashes
        are available since v2.0.0 (xxHash v0.8.0), they are:Streaming classes:    |
        xxh3_64    | xxh3_128Oneshot functions:    | xxh3_64_digest(bytes, seed=0)
        \   | xxh3_64_intdigest(bytes, seed=0)    | xxh3_64_hexdigest(bytes, seed=0)
        \   | xxh3_128_digest(bytes, seed=0)    | xxh3_128_intdigest(bytes, seed=0)
        \   | xxh3_128_hexdigest(bytes, seed=0)And aliases:    | xxh128 = xxh3_128
        \   | xxh128_digest = xxh3_128_digest    | xxh128_intdigest = xxh3_128_intdigest
        \   | xxh128_hexdigest = xxh3_128_hexdigestCaveats-------SEED OVERFLOW~~~~~~~~~~~~~~xxh32
        takes an unsigned 32-bit integer as seed, and xxh64 takesan unsigned 64-bit
        integer as seed. Make sure that the seed is greater thanor equal to ``0``.ENDIANNESS~~~~~~~~~~~As
        of python-xxhash 0.3.0, ``digest()`` returns bytes of the**big-endian** representation
        of the integer digest. It usedto be little-endian.DONT USE XXHASH IN HMAC~~~~~~~~~~~~~~~~~~~~~~~Though
        you can use xxhash as an HMAC_ hash function, but it'shighly recommended not
        to.xxhash is **NOT** a cryptographic hash function, it is anon-cryptographic
        hash algorithm aimed at speed and quality.Do not put xxhash in any position
        where cryptographic hashfunctions are required.Copyright and License---------------------Copyright
        (c) 2014-2020 Yue Du - https://github.com/ifduyueLicensed under `BSD 2-Clause
        License <http://opensource.org/licenses/BSD-2-Clause>`_CHANGELOG-----------v3.4.1
        2023-10-05~~~~~~~~~~~~~~~~~- Remove setuptools_scmv3.4.0 2023-10-05~~~~~~~~~~~~~~~~~-
        Build wheels for Python 3.12v3.3.0 2023-07-29~~~~~~~~~~~~~~~~~- Upgrade xxHash
        to v0.8.2- Drop support for Python 3.6v3.2.0 2022-12-28~~~~~~~~~~~~~~~~~This
        is the last version to support Python 3.6- Build Python 3.11 wheels.- Remove
        setup.py test_suites, call unittest directlyv3.1.0 2022-10-19~~~~~~~~~~~~~~~~~-
        Type annotations.- Enabled muslinux wheels building.v3.0.0 2022-02-25~~~~~~~~~~~~~~~~~-
        New set `algorithms_available` lists all implemented algorithms in `xxhash`
        \ package.- Upgrade xxHash to v0.8.1.- Drop support for EOL Python versions,
        require python >= 3.6 from now on.- Migrate to github actions and build arm64
        wheels for macOS.- Always release GIL.v2.0.2 2021-04-15~~~~~~~~~~~~~~~~~-
        Fix Travis CI OSX dpl python2.7 get-pip.py errorv2.0.1 2021-04-15~~~~~~~~~~~~~~~~~-
        Only to trigger Python 3.9 wheels building.v2.0.0 2020-08-03~~~~~~~~~~~~~~~~~-
        **Require xxHash version >= v0.8.0**- Upgrade xxHash to v0.8.0- XXH3 hashes:
        `xxh3_64`, `xxh3_128`, and their oneshot functionsv1.4.4 2020-06-20~~~~~~~~~~~~~~~~~-
        Upgrade xxHash to v0.7.3- Stop using PEP393 deprecated APIs- Use XXH(32|64)_canonicalFromHash
        to replace u2bytes and ull2bytesv1.4.3 2019-11-12~~~~~~~~~~~~~~~~~- Upgrade
        xxHash to v0.7.2- Python 3.8 wheelsv1.4.2 2019-10-13~~~~~~~~~~~~~~~~~- Fixed:
        setup.py fails when reading README.rst and the default encoding is not UTF-8v1.4.1
        2019-08-27~~~~~~~~~~~~~~~~~- Fixed: xxh3.h in missing from source tarballv1.4.0
        2019-08-25~~~~~~~~~~~~~~~~~- Upgrade xxHash to v0.7.1v1.3.0 2018-10-21~~~~~~~~~~~~~~~~~-
        Wheels are now built automatically- Split CFFI variant into a separate package
        `ifduyue/python-xxhash-cffi <https://github.com/ifduyue/python-xxhash-cffi>`_v1.2.0
        2018-07-13~~~~~~~~~~~~~~~~~- Add oneshot functions xxh{32,64}_{,int,hex}digestv1.1.0
        2018-07-05~~~~~~~~~~~~~~~~~- Allow input larger than 2GB- Release the GIL
        on sufficiently large input- Drop support for Python 3.2v1.0.1 2017-03-02~~~~~~~~~~~~~~~~~~-
        Free state actively, instead of delegating it to ffi.gcv1.0.0 2017-02-10~~~~~~~~~~~~~~~~~~-
        Fixed copy() segfault- Added CFFI variantv0.6.3 2017-02-10~~~~~~~~~~~~~~~~~~-
        Fixed copy() segfaultv0.6.2 2017-02-10~~~~~~~~~~~~~~~~~~- Upgrade xxHash to
        v0.6.2v0.6.1 2016-06-26~~~~~~~~~~~~~~~~~~- Upgrade xxHash to v0.6.1v0.5.0
        2016-03-02~~~~~~~~~~~~~~~~~~- Upgrade xxHash to v0.5.0v0.4.3 2015-08-21~~~~~~~~~~~~~~~~~~-
        Upgrade xxHash to r42v0.4.1 2015-08-16~~~~~~~~~~~~~~~~~~- Upgrade xxHash to
        r41v0.4.0 2015-08-05~~~~~~~~~~~~~~~~~~- Added method reset- Upgrade xxHash
        to r40v0.3.2 2015-01-27~~~~~~~~~~~~~~~~~~- Fixed some typos in docstringsv0.3.1
        2015-01-24~~~~~~~~~~~~~~~~~~- Upgrade xxHash to r39v0.3.0 2014-11-11~~~~~~~~~~~~~~~~~~-
        Change digest() from little-endian representation to big-endian representation
        of the integer digest.  This change breaks compatibility (digest() results
        are different).v0.2.0 2014-10-25~~~~~~~~~~~~~~~~~~- Make this package hashlib-compliantv0.1.3
        2014-10-23~~~~~~~~~~~~~~~~~~- Update xxHash to r37v0.1.2 2014-10-19~~~~~~~~~~~~~~~~~~-
        Improve: Check XXHnn_init() return value.- Update xxHash to r36v0.1.1 2014-08-07~~~~~~~~~~~~~~~~~~-
        Improve: Can now be built with Visual C++ Compiler.v0.1.0 2014-08-05~~~~~~~~~~~~~~~~~~-
        New: XXH32 and XXH64 type, which support partially update.- Fix: build under
        Python 3.4v0.0.2 2014-08-03~~~~~~~~~~~~~~~~~~- NEW: Support Python 3v0.0.1
        2014-07-30~~~~~~~~~~~~~~~~~~- NEW: xxh32 and xxh64"
      Package: xxhash
      Source: pip
      Version: 3.4.1
      Hash: ''
      licenses:
      - BSD-2-Clause
      - BSD-3-Clause
      Title: xxhash
      DownloadURL: https://files.pythonhosted.org/packages/04/ef/1a95dc97a71b128a7c5fd531e42574b274629a4ad1354a694087e2305467/xxhash-3.4.1.tar.gz
  bazaar:
    register: 'no'
    prim: 1/CTX1040104
    community_link: https://pypi.org/project/xxhash/
    community_name: https://pypi.org/project/xxhash/
    community_url: https://pypi.org/project/xxhash/
    component_comment: ''
    component_highlevel_description: Python Binding for xxHash
    component_name: xxhash
    component_platform: linux
    component_programing_language: C#
    component_version: 3.4.1
    licenses:
    - FAL1159003/1 (BSD 2-Clause "Simplified" License (BSD-2-Clause))
    src_download_link: https://files.pythonhosted.org/packages/04/ef/1a95dc97a71b128a7c5fd531e42574b274629a4ad1354a694087e2305467/xxhash-3.4.1.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1078662&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Undefined
    crypto: ''
    programming_language: C#
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: xxhash
    target_sw: linux
    vendor: pip
    version: 3.4.1
    web_url: https://github.com/ifduyue/python-xxhash
  licenses:
  - BSD-2-Clause
  - BSD-3-Clause
  name: xxhash
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 3.4.1
  mimer:
    linking: Static
    product_number: CTX1040104
    product_version_label: 3.4.1
    selected_licenses:
    - BSD-2-Clause
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: yarl+1.9.4
  additional_info:
    fossa-attribution:
      Description: "yarl====The module provides handy URL class for URL parsing and
        changing... image:: https://github.com/aio-libs/yarl/workflows/CI/badge.svg
        \ :target: https://github.com/aio-libs/yarl/actions?query=workflow%3ACI  :align:
        right.. image:: https://codecov.io/gh/aio-libs/yarl/branch/master/graph/badge.svg
        \ :target: https://codecov.io/gh/aio-libs/yarl.. image:: https://badge.fury.io/py/yarl.svg
        \   :target: https://badge.fury.io/py/yarl.. image:: https://readthedocs.org/projects/yarl/badge/?version=latest
        \   :target: https://yarl.aio-libs.org.. image:: https://img.shields.io/pypi/pyversions/yarl.svg
        \   :target: https://pypi.python.org/pypi/yarl.. image:: https://img.shields.io/matrix/aio-libs:matrix.org?label=Discuss%20on%20Matrix%20at%20%23aio-libs%3Amatrix.org&logo=matrix&server_fqdn=matrix.org&style=flat
        \  :target: https://matrix.to/#/%23aio-libs:matrix.org   :alt: Matrix Room
        \u2014 #aio-libs:matrix.org.. image:: https://img.shields.io/matrix/aio-libs-space:matrix.org?label=Discuss%20on%20Matrix%20at%20%23aio-libs-space%3Amatrix.org&logo=matrix&server_fqdn=matrix.org&style=flat
        \  :target: https://matrix.to/#/%23aio-libs-space:matrix.org   :alt: Matrix
        Space \u2014 #aio-libs-space:matrix.orgIntroduction------------Url is constructed
        from ``str``:.. code-block:: pycon   >>> from yarl import URL   >>> url =
        URL('https://www.python.org/~guido?arg=1#frag')   >>> url   URL('https://www.python.org/~guido?arg=1#frag')All
        url parts: *scheme*, *user*, *password*, *host*, *port*, *path*,*query* and
        *fragment* are accessible by properties:.. code-block:: pycon   >>> url.scheme
        \  'https'   >>> url.host   'www.python.org'   >>> url.path   '/~guido'   >>>
        url.query_string   'arg=1'   >>> url.query   <MultiDictProxy('arg': '1')>
        \  >>> url.fragment   'frag'All url manipulations produce a new url object:..
        code-block:: pycon   >>> url = URL('https://www.python.org')   >>> url / 'foo'
        / 'bar'   URL('https://www.python.org/foo/bar')   >>> url / 'foo' % {'bar':
        'baz'}   URL('https://www.python.org/foo?bar=baz')Strings passed to constructor
        and modification methods areautomatically encoded giving canonical representation
        as result:.. code-block:: pycon   >>> url = URL('https://www.python.org/\u0448\u043B\u044F\u0445')
        \  >>> url   URL('https://www.python.org/%D1%88%D0%BB%D1%8F%D1%85')Regular
        properties are *percent-decoded*, use ``raw_`` versions forgetting *encoded*
        strings:.. code-block:: pycon   >>> url.path   '/\u0448\u043B\u044F\u0445'
        \  >>> url.raw_path   '/%D1%88%D0%BB%D1%8F%D1%85'Human readable representation
        of URL is available as ``.human_repr()``:.. code-block:: pycon   >>> url.human_repr()
        \  'https://www.python.org/\u0448\u043B\u044F\u0445'For full documentation
        please read https://yarl.aio-libs.org.Installation------------::   $ pip install
        yarlThe library is Python 3 only!PyPI contains binary wheels for Linux, Windows
        and MacOS.  If you want to install``yarl`` on another operating system (like
        *Alpine Linux*, which is notmanylinux-compliant because of the missing glibc
        and therefore, cannot beused with our wheels) the the tarball will be used
        to compile the library fromthe source code. It requires a C compiler and and
        Python headers installed.To skip the compilation you must explicitly opt-in
        by using a PEP 517configuration setting ``pure-python``, or setting the ``YARL_NO_EXTENSIONS``environment
        variable to a non-empty value, e.g.:.. code-block:: console   $ pip install
        yarl --config-settings=pure-python=falsePlease note that the pure-Python (uncompiled)
        version is much slower. However,PyPy always uses a pure-Python implementation,
        and, as such, it is unaffectedby this variable.Dependencies------------YARL
        requires multidict_ library.API documentation------------------The documentation
        is located at https://yarl.aio-libs.org.Why isn't boolean supported by the
        URL query API?-------------------------------------------------There is no
        standard for boolean representation of boolean values.Some systems prefer
        ``true``/``false``, others like ``yes``/``no``, ``on``/``off``,``Y``/``N``,
        ``1``/``0``, etc.``yarl`` cannot make an unambiguous decision on how to serialize
        ``bool`` values becauseit is specific to how the end-user's application is
        built and would be different fordifferent apps.  The library doesn't accept
        booleans in the API; a user should convertbools into strings using own preferred
        translation protocol.Comparison with other URL libraries------------------------------------*
        furl (https://pypi.python.org/pypi/furl)  The library has rich functionality
        but the ``furl`` object is mutable.  I'm afraid to pass this object into foreign
        code: who knows if the  code will modify my url in a terrible way while I
        just want to send URL  with handy helpers for accessing URL properties.  ``furl``
        has other non-obvious tricky things but the main objection  is mutability.*
        URLObject (https://pypi.python.org/pypi/URLObject)  URLObject is immutable,
        that's pretty good.  Every URL change generates a new URL object.  But the
        library doesn't do any decode/encode transformations leaving the  end user
        to cope with these gory details.Source code-----------The project is hosted
        on GitHub_Please file an issue on the `bug tracker<https://github.com/aio-libs/yarl/issues>`_
        if you have found a bugor have some suggestion in order to improve the library.The
        library uses `Azure Pipelines <https://dev.azure.com/aio-libs/yarl>`_ forContinuous
        Integration.Discussion list---------------*aio-libs* google group: https://groups.google.com/forum/#!forum/aio-libsFeel
        free to post your questions and ideas here.Authors and License-------------------The
        ``yarl`` package is written by Andrew Svetlov.It's *Apache 2* licensed and
        freely available... _GitHub: https://github.com/aio-libs/yarl.. _multidict:
        https://github.com/aio-libs/multidict..    You should *NOT* be adding new
        change log entries to this file, this    file is managed by towncrier. You
        *may* edit previous change logs to    fix problems like typo corrections or
        such.    To add a new change log entry, please see    https://pip.pypa.io/en/latest/development/#adding-a-news-entry
        \   we named the news folder \"changes\".    WARNING: Don't drop the next
        directive!.. towncrier release notes start1.9.4 (2023-12-06)==================Bug
        fixes---------- Started raising ``TypeError`` when a string value is passed
        into  ``yarl.URL.build()`` as the ``port`` argument  -- by `@commonism <https://github.com/sponsors/commonism>`__.
        \ Previously the empty string as port would create malformed URLs when rendered
        as string representations. (`#883 <https://github.com/aio-libs/yarl/issues/883>`__)Packaging
        updates and notes for downstreams--------------------------------------------
        The leading ``--`` has been dropped from the `PEP 517 <https://peps.python.org/pep-517>`__
        in-tree build  backend config setting names. ``--pure-python`` is now just
        ``pure-python``  -- by `@webknjaz <https://github.com/sponsors/webknjaz>`__.
        \ The usage now looks as follows:  .. code-block:: console      $ python -m
        build \\          --config-setting=pure-python=true \\          --config-setting=with-cython-tracing=true
        \ (`#963 <https://github.com/aio-libs/yarl/issues/963>`__)Contributor-facing
        changes--------------------------- A step-by-step ``Release Guide`` guide
        has  been added, describing how to release *yarl* -- by `@webknjaz <https://github.com/sponsors/webknjaz>`__.
        \ This is primarily targeting maintainers. (`#960 <https://github.com/aio-libs/yarl/issues/960>`__)-
        Coverage collection has been implemented for the Cython modules  -- by `@webknjaz
        <https://github.com/sponsors/webknjaz>`__.  It will also be reported to Codecov
        from any non-release CI jobs.  To measure coverage in a development environment,
        *yarl* can be  installed in editable mode, which requires an environment variable
        \ ``YARL_CYTHON_TRACING=1`` to be set:  .. code-block:: console      $ YARL_CYTHON_TRACING=1
        python -Im pip install -e .  Editable install produces C-files required for
        the Cython coverage  plugin to map the measurements back to the PYX-files.
        (`#961 <https://github.com/aio-libs/yarl/issues/961>`__)- It is now possible
        to request line tracing in Cython builds using the  ``with-cython-tracing``
        `PEP 517 <https://peps.python.org/pep-517>`__ config setting  -- `@webknjaz
        <https://github.com/sponsors/webknjaz>`__.  This can be used in CI and development
        environment to measure coverage  on Cython modules, but is not normally useful
        to the end-users or  downstream packagers.  Here's a usage example:  .. code-block::
        console      $ python -Im pip install . --config-settings=with-cython-tracing=true
        \ For editable installs, this setting is on by default. Otherwise, it's  off
        unless requested explicitly. (`#962 <https://github.com/aio-libs/yarl/issues/962>`__)1.9.3
        (2023-11-20)==================Bug fixes---------- Stopped dropping trailing
        slashes in ``yarl.URL.joinpath()`` -- by `@gmacon <https://github.com/sponsors/gmacon>`__.
        (`#862 <https://github.com/aio-libs/yarl/issues/862>`__, `#866 <https://github.com/aio-libs/yarl/issues/866>`__)-
        Started accepting string subclasses in ``__truediv__()`` operations (``URL
        / segment``) -- by `@mjpieters <https://github.com/sponsors/mjpieters>`__.
        (`#871 <https://github.com/aio-libs/yarl/issues/871>`__, `#884 <https://github.com/aio-libs/yarl/issues/884>`__)-
        Fixed the human representation of URLs with square brackets in usernames and
        passwords -- by `@mjpieters <https://github.com/sponsors/mjpieters>`__. (`#876
        <https://github.com/aio-libs/yarl/issues/876>`__, `#882 <https://github.com/aio-libs/yarl/issues/882>`__)-
        Updated type hints to include ``URL.missing_port()``, ``URL.__bytes__()``
        \ and the ``encoding`` argument to ``yarl.URL.joinpath()``  -- by `@mjpieters
        <https://github.com/sponsors/mjpieters>`__. (`#891 <https://github.com/aio-libs/yarl/issues/891>`__)Packaging
        updates and notes for downstreams--------------------------------------------
        Integrated Cython 3 to enable building *yarl* under Python 3.12 -- by `@mjpieters
        <https://github.com/sponsors/mjpieters>`__. (`#829 <https://github.com/aio-libs/yarl/issues/829>`__,
        `#881 <https://github.com/aio-libs/yarl/issues/881>`__)- Declared modern ``setuptools.build_meta``
        as the `PEP 517 <https://peps.python.org/pep-517>`__ build  backend in ``pyproject.toml``
        explicitly -- by `@webknjaz <https://github.com/sponsors/webknjaz>`__. (`#886
        <https://github.com/aio-libs/yarl/issues/886>`__)- Converted most of the packaging
        setup into a declarative ``setup.cfg``  config -- by `@webknjaz <https://github.com/sponsors/webknjaz>`__.
        (`#890 <https://github.com/aio-libs/yarl/issues/890>`__)- The packaging is
        replaced from an old-fashioned ``setup.py`` to an  in-tree `PEP 517 <https://peps.python.org/pep-517>`__
        build backend -- by `@webknjaz <https://github.com/sponsors/webknjaz>`__.
        \ Whenever the end-users or downstream packagers need to build ``yarl`` from
        \ source (a Git checkout or an sdist), they may pass a ``config_settings``
        \ flag ``--pure-python``. If this flag is not set, a C-extension will be built
        \ and included into the distribution.  Here is how this can be done with ``pip``:
        \ .. code-block:: console      $ python -m pip install . --config-settings=--pure-python=false
        \ This will also work with ``-e | --editable``.  The same can be achieved
        via ``pypa/build``:  .. code-block:: console      $ python -m build --config-setting=--pure-python=false
        \ Adding ``-w | --wheel`` can force ``pypa/build`` produce a wheel from source
        \ directly, as opposed to building an ``sdist`` and then building from it.
        (`#893 <https://github.com/aio-libs/yarl/issues/893>`__)  .. attention::     v1.9.3
        was the only version using the ``--pure-python`` setting name.     Later versions
        dropped the ``--`` prefix, making it just ``pure-python``.- Declared Python
        3.12 supported officially in the distribution package metadata  -- by `@edgarrmondragon
        <https://github.com/sponsors/edgarrmondragon>`__. (`#942 <https://github.com/aio-libs/yarl/issues/942>`__)Contributor-facing
        changes--------------------------- A regression test for no-host URLs was
        added per `#821 <https://github.com/aio-libs/yarl/issues/821>`__  and ``3986``
        -- by `@kenballus <https://github.com/sponsors/kenballus>`__. (`#821 <https://github.com/aio-libs/yarl/issues/821>`__,
        `#822 <https://github.com/aio-libs/yarl/issues/822>`__)- Started testing *yarl*
        against Python 3.12 in CI -- by `@mjpieters <https://github.com/sponsors/mjpieters>`__.
        (`#881 <https://github.com/aio-libs/yarl/issues/881>`__)- All Python 3.12
        jobs are now marked as required to pass in CI  -- by `@edgarrmondragon <https://github.com/sponsors/edgarrmondragon>`__.
        (`#942 <https://github.com/aio-libs/yarl/issues/942>`__)- MyST is now integrated
        in Sphinx -- by `@webknjaz <https://github.com/sponsors/webknjaz>`__.  This
        allows the contributors to author new documents in Markdown  when they have
        difficulties with going straight RST. (`#953 <https://github.com/aio-libs/yarl/issues/953>`__)1.9.2
        (2023-04-25)==================Bugfixes--------- Fix regression with ``__truediv__``
        and absolute URLs with empty paths causing the raw path to lack the leading
        ``/``.  (`#854 <https://github.com/aio-libs/yarl/issues/854>`_)1.9.1 (2023-04-21)==================Bugfixes---------
        Marked tests that fail on older Python patch releases (< 3.7.10, < 3.8.8 and
        < 3.9.2) as expected to fail due to missing a security fix for CVE-2021-23336.
        (`#850 <https://github.com/aio-libs/yarl/issues/850>`_)1.9.0 (2023-04-19)==================This
        release was never published to PyPI, due to issues with the build process.Features---------
        Added ``URL.joinpath(*elements)``, to create a new URL appending multiple
        path elements. (`#704 <https://github.com/aio-libs/yarl/issues/704>`_)- Made
        ``URL.__truediv__()`` return ``NotImplemented`` if called with an  unsupported
        type \u2014 by `@michaeljpeters <https://github.com/sponsors/michaeljpeters>`__.
        \ (`#832 <https://github.com/aio-libs/yarl/issues/832>`_)Bugfixes---------
        Path normalization for absolute URLs no longer raises a ValueError exception
        \ when ``..`` segments would otherwise go beyond the URL path root.  (`#536
        <https://github.com/aio-libs/yarl/issues/536>`_)- Fixed an issue with update_query()
        not getting rid of the query when argument is None. (`#792 <https://github.com/aio-libs/yarl/issues/792>`_)-
        Added some input restrictions on with_port() function to prevent invalid boolean
        inputs or out of valid port inputs; handled incorrect 0 port representation.
        (`#793 <https://github.com/aio-libs/yarl/issues/793>`_)- Made ``yarl.URL.build()``
        raise a ``TypeError`` if the ``host`` argument is ``None`` \u2014 by `@paulpapacz
        <https://github.com/sponsors/paulpapacz>`__. (`#808 <https://github.com/aio-libs/yarl/issues/808>`_)-
        Fixed an issue with ``update_query()`` getting rid of the query when the argument
        \ is empty but not ``None``. (`#845 <https://github.com/aio-libs/yarl/issues/845>`_)Misc-----
        `#220 <https://github.com/aio-libs/yarl/issues/220>`_1.8.2 (2022-12-03)==================This
        is the first release that started shipping wheels for Python 3.11.1.8.1 (2022-08-01)==================Misc-----
        `#694 <https://github.com/aio-libs/yarl/issues/694>`_, `#699 <https://github.com/aio-libs/yarl/issues/699>`_,
        `#700 <https://github.com/aio-libs/yarl/issues/700>`_, `#701 <https://github.com/aio-libs/yarl/issues/701>`_,
        `#702 <https://github.com/aio-libs/yarl/issues/702>`_, `#703 <https://github.com/aio-libs/yarl/issues/703>`_,
        `#739 <https://github.com/aio-libs/yarl/issues/739>`_1.8.0 (2022-08-01)==================Features---------
        Added ``URL.raw_suffix``, ``URL.suffix``, ``URL.raw_suffixes``, ``URL.suffixes``,
        ``URL.with_suffix``. (`#613 <https://github.com/aio-libs/yarl/issues/613>`_)Improved
        Documentation----------------------- Fixed broken internal references to ``yarl.URL.human_repr()``.
        \ (`#665 <https://github.com/aio-libs/yarl/issues/665>`_)- Fixed broken external
        references to ``multidict:index`` docs. (`#665 <https://github.com/aio-libs/yarl/issues/665>`_)Deprecations
        and Removals-------------------------- Dropped Python 3.6 support. (`#672
        <https://github.com/aio-libs/yarl/issues/672>`_)Misc----- `#646 <https://github.com/aio-libs/yarl/issues/646>`_,
        `#699 <https://github.com/aio-libs/yarl/issues/699>`_, `#701 <https://github.com/aio-libs/yarl/issues/701>`_1.7.2
        (2021-11-01)==================Bugfixes--------- Changed call in ``with_port()``
        to stop reencoding parts of the URL that were already encoded. (`#623 <https://github.com/aio-libs/yarl/issues/623>`_)1.7.1
        (2021-10-07)==================Bugfixes--------- Fix 1.7.0 build error1.7.0
        (2021-10-06)==================Features--------- Add ``__bytes__()`` magic
        method so that ``bytes(url)`` will work and use optimal ASCII encoding.  (`#582
        <https://github.com/aio-libs/yarl/issues/582>`_)- Started shipping platform-specific
        arm64 wheels for Apple Silicon. (`#622 <https://github.com/aio-libs/yarl/issues/622>`_)-
        Started shipping platform-specific wheels with the ``musl`` tag targeting
        typical Alpine Linux runtimes. (`#622 <https://github.com/aio-libs/yarl/issues/622>`_)-
        Added support for Python 3.10. (`#622 <https://github.com/aio-libs/yarl/issues/622>`_)1.6.3
        (2020-11-14)==================Bugfixes--------- No longer loose characters
        when decoding incorrect percent-sequences (like ``%e2%82%f8``). All non-decodable
        percent-sequences are now preserved.  `#517 <https://github.com/aio-libs/yarl/issues/517>`_-
        Provide x86 Windows wheels.  `#535 <https://github.com/aio-libs/yarl/issues/535>`_----1.6.2
        (2020-10-12)==================Bugfixes--------- Provide generated ``.c`` files
        in TarBall distribution.  `#530  <https://github.com/aio-libs/multidict/issues/530>`_1.6.1
        (2020-10-12)==================Features--------- Provide wheels for ``aarch64``,
        ``i686``, ``ppc64le``, ``s390x`` architectures on  Linux as well as ``x86_64``.
        \ `#507  <https://github.com/aio-libs/yarl/issues/507>`_- Provide wheels for
        Python 3.9.  `#526 <https://github.com/aio-libs/yarl/issues/526>`_Bugfixes---------
        ``human_repr()`` now always produces valid representation equivalent to the
        original URL (if the original URL is valid).  `#511 <https://github.com/aio-libs/yarl/issues/511>`_-
        Fixed  requoting a single percent followed by a percent-encoded character
        in the Cython implementation.  `#514 <https://github.com/aio-libs/yarl/issues/514>`_-
        Fix ValueError when decoding ``%`` which is not followed by two hexadecimal
        digits.  `#516 <https://github.com/aio-libs/yarl/issues/516>`_- Fix decoding
        ``%`` followed by a space and hexadecimal digit.  `#520 <https://github.com/aio-libs/yarl/issues/520>`_-
        Fix annotation of ``with_query()``/``update_query()`` methods for ``key=[val1,
        val2]`` case.  `#528 <https://github.com/aio-libs/yarl/issues/528>`_Removal--------
        Drop Python 3.5 support; Python 3.6 is the minimal supported Python version.----1.6.0
        (2020-09-23)==================Features--------- Allow for int and float subclasses
        in query, while still denying bool.  `#492 <https://github.com/aio-libs/yarl/issues/492>`_Bugfixes---------
        Do not requote arguments in ``URL.build()``, ``with_xxx()`` and in ``/`` operator.
        \ `#502 <https://github.com/aio-libs/yarl/issues/502>`_- Keep IPv6 brackets
        in ``origin()``.  `#504 <https://github.com/aio-libs/yarl/issues/504>`_----1.5.1
        (2020-08-01)==================Bugfixes--------- Fix including relocated internal
        ``yarl._quoting_c`` C-extension into published PyPI dists.  `#485 <https://github.com/aio-libs/yarl/issues/485>`_Misc-----
        `#484 <https://github.com/aio-libs/yarl/issues/484>`_----1.5.0 (2020-07-26)==================Features---------
        Convert host to lowercase on URL building.  `#386 <https://github.com/aio-libs/yarl/issues/386>`_-
        Allow using ``mod`` operator (``%``) for updating query string (an alias for
        ``update_query()`` method).  `#435 <https://github.com/aio-libs/yarl/issues/435>`_-
        Allow use of sequences such as ``list`` and ``tuple`` in the values  of a
        mapping such as ``dict`` to represent that a key has many values::      url
        = URL(\"http://example.com\")      assert url.with_query({\"a\": [1, 2]})
        == URL(\"http://example.com/?a=1&a=2\")  `#443 <https://github.com/aio-libs/yarl/issues/443>`_-
        Support ``URL.build()`` with scheme and path (creates a relative URL).  `#464
        <https://github.com/aio-libs/yarl/issues/464>`_- Cache slow IDNA encode/decode
        calls.  `#476 <https://github.com/aio-libs/yarl/issues/476>`_- Add ``@final``
        / ``Final`` type hints  `#477 <https://github.com/aio-libs/yarl/issues/477>`_-
        Support URL authority/raw_authority properties and authority argument of ``URL.build()``
        method.  `#478 <https://github.com/aio-libs/yarl/issues/478>`_- Hide the library
        implementation details, make the exposed public list very clean.  `#483 <https://github.com/aio-libs/yarl/issues/483>`_Bugfixes---------
        Fix tests with newer Python (3.7.6, 3.8.1 and 3.9.0+).  `#409 <https://github.com/aio-libs/yarl/issues/409>`_-
        Fix a bug where query component, passed in a form of mapping or sequence,
        is unquoted in unexpected way.  `#426 <https://github.com/aio-libs/yarl/issues/426>`_-
        Hide ``Query`` and ``QueryVariable`` type aliases in ``__init__.pyi``, now
        they are prefixed with underscore.  `#431 <https://github.com/aio-libs/yarl/issues/431>`_-
        Keep IPv6 brackets after updating port/user/password.  `#451 <https://github.com/aio-libs/yarl/issues/451>`_----1.4.2
        (2019-12-05)==================Features--------- Workaround for missing ``str.isascii()``
        in Python 3.6  `#389 <https://github.com/aio-libs/yarl/issues/389>`_----1.4.1
        (2019-11-29)==================* Fix regression, make the library work on Python
        3.5 and 3.6 again.1.4.0 (2019-11-29)==================* Distinguish an empty
        password in URL from a password not provided at all (#262)* Fixed annotations
        for optional parameters of ``URL.build`` (#309)* Use None as default value
        of ``user`` parameter of ``URL.build`` (#309)* Enforce building C Accelerated
        modules when installing from source tarball, use  ``YARL_NO_EXTENSIONS`` environment
        variable for falling back to (slower) Pure Python  implementation (#329)*
        Drop Python 3.5 support* Fix quoting of plus in path by pure python version
        (#339)* Don't create a new URL if fragment is unchanged (#292)* Included in
        error message the path that produces starting slash forbidden error (#376)*
        Skip slow IDNA encoding for ASCII-only strings (#387)1.3.0 (2018-12-11)==================*
        Fix annotations for ``query`` parameter (#207)* An incoming query sequence
        can have int variables (the same as for  Mapping type) (#208)* Add ``URL.explicit_port``
        property (#218)* Give a friendlier error when port can't be converted to int
        (#168)* ``bool(URL())`` now returns ``False`` (#272)1.2.6 (2018-06-14)==================*
        Drop Python 3.4 trove classifier (#205)1.2.5 (2018-05-23)==================*
        Fix annotations for ``build`` (#199)1.2.4 (2018-05-08)==================*
        Fix annotations for ``cached_property`` (#195)1.2.3 (2018-05-03)==================*
        Accept ``str`` subclasses in ``URL`` constructor (#190)1.2.2 (2018-05-01)==================*
        Fix build1.2.1 (2018-04-30)==================* Pin minimal required Python
        to 3.5.3 (#189)1.2.0 (2018-04-30)==================* Forbid inheritance, replace
        ``__init__`` with ``__new__`` (#171)* Support PEP-561 (provide type hinting
        marker) (#182)1.1.1 (2018-02-17)==================* Fix performance regression:
        don't encode empty ``netloc`` (#170)1.1.0 (2018-01-21)==================*
        Make pure Python quoter consistent with Cython version (#162)1.0.0 (2018-01-15)==================*
        Use fast path if quoted string does not need requoting (#154)* Speed up quoting/unquoting
        by ``_Quoter`` and ``_Unquoter`` classes (#155)* Drop ``yarl.quote`` and ``yarl.unquote``
        public functions (#155)* Add custom string writer, reuse static buffer if
        available (#157)  Code is 50-80 times faster than Pure Python version (was
        4-5 times faster)* Don't recode IP zone (#144)* Support ``encoded=True`` in
        ``yarl.URL.build()`` (#158)* Fix updating query with multiple keys (#160)0.18.0
        (2018-01-10)===================* Fallback to IDNA 2003 if domain name is not
        IDNA 2008 compatible (#152)0.17.0 (2017-12-30)===================* Use IDNA
        2008 for domain name processing (#149)0.16.0 (2017-12-07)===================*
        Fix raising ``TypeError`` by ``url.query_string()`` after  ``url.with_query({})``
        (empty mapping) (#141)0.15.0 (2017-11-23)===================* Add ``raw_path_qs``
        attribute (#137)0.14.2 (2017-11-14)===================* Restore ``strict``
        parameter as no-op in ``quote`` / ``unquote``0.14.1 (2017-11-13)===================*
        Restore ``strict`` parameter as no-op for sake of compatibility with  aiohttp
        2.20.14.0 (2017-11-11)===================* Drop strict mode (#123)* Fix ``\"ValueError:
        Unallowed PCT %\"`` when there's a ``\"%\"`` in the URL (#124)0.13.0 (2017-10-01)===================*
        Document ``encoded`` parameter (#102)* Support relative URLs like ``'?key=value'``
        (#100)* Unsafe encoding for QS fixed. Encode ``;`` character in value parameter
        (#104)* Process passwords without user names (#95)0.12.0 (2017-06-26)===================*
        Properly support paths without leading slash in ``URL.with_path()`` (#90)*
        Enable type annotation checks0.11.0 (2017-06-26)===================* Normalize
        path (#86)* Clear query and fragment parts in ``.with_path()`` (#85)0.10.3
        (2017-06-13)===================* Prevent double URL arguments unquoting (#83)0.10.2
        (2017-05-05)===================* Unexpected hash behavior (#75)0.10.1 (2017-05-03)===================*
        Unexpected compare behavior (#73)* Do not quote or unquote + if not a query
        string. (#74)0.10.0 (2017-03-14)===================* Added ``URL.build`` class
        method (#58)* Added ``path_qs`` attribute (#42)0.9.8 (2017-02-16)==================*
        Do not quote ``:`` in path0.9.7 (2017-02-16)==================* Load from
        pickle without _cache (#56)* Percent-encoded pluses in path variables become
        spaces (#59)0.9.6 (2017-02-15)==================* Revert backward incompatible
        change (BaseURL)0.9.5 (2017-02-14)==================* Fix BaseURL rich comparison
        support0.9.4 (2017-02-14)==================* Use BaseURL0.9.3 (2017-02-14)==================*
        Added BaseURL0.9.2 (2017-02-08)==================* Remove debug print0.9.1
        (2017-02-07)==================* Do not lose tail chars (#45)0.9.0 (2017-02-07)==================*
        Allow to quote ``%`` in non strict mode (#21)* Incorrect parsing of query
        parameters with %3B (;) inside (#34)* Fix core dumps (#41)* ``tmpbuf`` - compiling
        error (#43)* Added ``URL.update_path()`` method* Added ``URL.update_query()``
        method (#47)0.8.1 (2016-12-03)==================* Fix broken aiohttp: revert
        back ``quote`` / ``unquote``.0.8.0 (2016-12-03)==================* Support
        more verbose error messages in ``.with_query()`` (#24)* Don't percent-encode
        ``@`` and ``:`` in path (#32)* Don't expose ``yarl.quote`` and ``yarl.unquote``,
        these functions are  part of private API0.7.1 (2016-11-18)==================*
        Accept not only ``str`` but all classes inherited from ``str`` also (#25)0.7.0
        (2016-11-07)==================* Accept ``int`` as value for ``.with_query()``0.6.0
        (2016-11-07)==================* Explicitly use UTF8 encoding in ``setup.py``
        (#20)* Properly unquote non-UTF8 strings (#19)0.5.3 (2016-11-02)==================*
        Don't use ``typing.NamedTuple`` fields but indexes on URL construction0.5.2
        (2016-11-02)==================* Inline ``_encode`` class method0.5.1 (2016-11-02)==================*
        Make URL construction faster by removing extra classmethod calls0.5.0 (2016-11-02)==================*
        Add Cython optimization for quoting/unquoting* Provide binary wheels0.4.3
        (2016-09-29)==================* Fix typing stubs0.4.2 (2016-09-29)==================*
        Expose ``quote()`` and ``unquote()`` as public API0.4.1 (2016-09-28)==================*
        Support empty values in query (``'/path?arg'``)0.4.0 (2016-09-27)==================*
        Introduce ``relative()`` (#16)0.3.2 (2016-09-27)==================* Typo fixes
        #150.3.1 (2016-09-26)==================* Support sequence of pairs as ``with_query()``
        parameter0.3.0 (2016-09-26)==================* Introduce ``is_default_port()``0.2.1
        (2016-09-26)==================* Raise ValueError for URLs like 'http://:8080/'0.2.0
        (2016-09-18)==================* Avoid doubling slashes when joining paths
        (#13)* Appending path starting from slash is forbidden (#12)0.1.4 (2016-09-09)==================*
        Add ``kwargs`` support for ``with_query()`` (#10)0.1.3 (2016-09-07)==================*
        Document ``with_query()``, ``with_fragment()`` and ``origin()``* Allow ``None``
        for ``with_query()`` and ``with_fragment()``0.1.2 (2016-09-07)==================*
        Fix links, tune docs theme.0.1.1 (2016-09-06)==================* Update README,
        old version used obsolete API0.1.0 (2016-09-06)==================* The library
        was deeply refactored, bytes are gone away but all  accepted strings are encoded
        if needed.0.0.1 (2016-08-30)==================* The first release."
      Package: yarl
      Source: pip
      Version: 1.9.4
      Hash: ''
      licenses:
      - Apache-2.0
      Title: yarl
      DownloadURL: https://files.pythonhosted.org/packages/e0/ad/bedcdccbcbf91363fd425a948994f3340924145c2bc8ccb296f4a1e52c28/yarl-1.9.4.tar.gz
  bazaar:
    register: 'no'
    prim: 9/CTX1028736
    community_link: https://github.com/aio-libs/yarl
    community_name: https://github.com/aio-libs/yarl
    community_url: https://github.com/aio-libs/yarl
    component_comment: ''
    component_highlevel_description: Yet another URL library
    component_name: yarl
    component_platform: linux
    component_programing_language: Python
    component_version: V1.9.4
    licenses:
    - FAL1159004/20 (Apache License 2.0 (Apache-2.0))
    src_download_link: https://github.com/aio-libs/yarl/archive/v1.9.4.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1073383&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: Ukraine
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: yarl
    target_sw: linux
    vendor: pip
    version: 1.9.4
    web_url: https://github.com/aio-libs/yarl
  licenses:
  - Apache-2.0
  name: yarl
  primary:
  - optimum+1.17.1
  subcomponent: false
  type: FOSS
  versions:
  - 1.9.4
  mimer:
    linking: Static
    product_number: CTX1028736
    product_version_label: V1.9.4
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
- ID: zipp+3.17.0
  additional_info:
    fossa-attribution:
      Description: '.. image:: https://img.shields.io/pypi/v/zipp.svg   :target: https://pypi.org/project/zipp..
        image:: https://img.shields.io/pypi/pyversions/zipp.svg.. image:: https://github.com/jaraco/zipp/actions/workflows/main.yml/badge.svg   :target:
        https://github.com/jaraco/zipp/actions?query=workflow%3A%22tests%22   :alt:
        tests.. image:: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json    :target:
        https://github.com/astral-sh/ruff    :alt: Ruff.. .. image:: https://readthedocs.org/projects/PROJECT_RTD/badge/?version=latest..    :target:
        https://PROJECT_RTD.readthedocs.io/en/latest/?badge=latest.. image:: https://img.shields.io/badge/skeleton-2024-informational   :target:
        https://blog.jaraco.com/skeleton.. image:: https://tidelift.com/badges/package/pypi/zipp   :target:
        https://tidelift.com/subscription/pkg/pypi-zipp?utm_source=pypi-zipp&utm_medium=readmeA
        pathlib-compatible Zipfile object wrapper. Official backport of the standard
        library`Path object <https://docs.python.org/3.8/library/zipfile.html#path-objects>`_.Compatibility=============New
        features are introduced in this third-party library and later mergedinto CPython.
        The following table indicates which versions of this librarywere contributed
        to different versions in the standard library:.. list-table::   :header-rows:
        1   * - zipp     - stdlib   * - 3.15     - 3.12   * - 3.5     - 3.11   * -
        3.2     - 3.10   * - 3.3 ??     - 3.9   * - 1.0     - 3.8Usage=====Use ``zipp.Path``
        in place of ``zipfile.Path`` on any Python.For Enterprise==============Available
        as part of the Tidelift Subscription.This project and the maintainers of thousands
        of other packages are working with Tidelift to deliver one enterprise subscription
        that covers all of the open source you use.`Learn more <https://tidelift.com/subscription/pkg/pypi-zipp?utm_source=pypi-zipp&utm_medium=referral&utm_campaign=github>`_.'
      Package: zipp
      Source: pip
      Version: 3.17.0
      Hash: ''
      licenses:
      - MIT
      Title: zipp
      DownloadURL: https://files.pythonhosted.org/packages/58/03/dd5ccf4e06dec9537ecba8fcc67bbd4ea48a2791773e469e73f94c3ba9a6/zipp-3.17.0.tar.gz
  bazaar:
    register: 'no'
    prim: 23/CTX1026317
    community_link: https://github.com/jaraco/zipp
    community_name: https://github.com/jaraco/zipp
    community_url: https://github.com/jaraco/zipp
    component_comment: ''
    component_highlevel_description: 'A pathlib-compatible Zipfile object wrapper.
      A backport of the Path object.

      functionality.'
    component_name: zipp
    component_platform: linux
    component_programing_language: Python
    component_version: V3.17.0
    licenses:
    - FAL1159008 (MIT License (MIT))
    src_download_link: https://github.com/jaraco/zipp/archive/v3.17.0.tar.gz
    stako_decision_reason: automatic
    stako: ESW2
    stako_comment: ''
    bazaarurl: https://scas.internal.ericsson.com/search3pp?id=1065059&id-filter=equals&rowDisplay=Card-Simple
    recode: ''
    retext: ''
    country: United States
    crypto: ''
    programming_language: Python
  encryptions:
    used:
    - ''
  evms:
    register: 'yes'
    product_name: zipp
    target_sw: linux
    vendor: pip
    version: 3.17.0
    web_url: https://github.com/jaraco/zipp
  licenses:
  - MIT
  name: zipp
  primary:
  - tensorflow+2.15.0
  subcomponent: false
  type: FOSS
  versions:
  - 3.17.0
  mimer:
    linking: Static
    product_number: CTX1026317
    product_version_label: V3.17.0
    obligation: Including the full license text in a prominent place with the software
      when the FOSS is distributed
    usage: Use as is
    primary: 'False'
